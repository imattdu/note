# 4.hbase进阶



## 4.1详细架构图



![](https://raw.githubusercontent.com/imattdu/img/main/img/20210829104019.png)



1.StoreFile 
保存实际数据的物理文件，StoreFile 以 HFile 的形式存储在 HDFS 上。每个 Store 会有
一个或多个 StoreFile（HFile），数据在每个 StoreFile 中都是有序的。 
2.MemStore 
写缓存，由于 HFile 中的数据要求是有序的，所以数据是先存储在 MemStore 中，排好序后，等到达刷写时机才会刷写到 HFile，每次刷写都会形成一个新的 HFile。 
3.WAL 
由于数据要经 MemStore 排序后才能刷写到 HFile，但把数据保存在内存中会有很高的
概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做 Write-Ahead logfile 的文件
中，然后再写入 MemStore 中。所以在系统出现故障的时候，数据可以通过这个日志文件重
建。





## 4.2写数据



### 原理图



![](https://raw.githubusercontent.com/imattdu/img/main/img/20210829104701.png)





写流程： 
1.Client 先访问 zookeeper，获取 hbase:meta 表位于哪个 Region Server。 
2.访问对应的 Region Server，获取 hbase:meta 表，根据读请求的 namespace:table/rowkey，查询出目标数据位于哪个 Region Server 中的哪个 Region 中。并将该 table 的 region 信息以及 meta 表的位置信息缓存在客户端的 meta cache，方便下次访问。 
3.与目标 Region Server 进行通讯； 
4.将数据顺序写入（追加）到 WAL； 
5.将数据写入对应的 MemStore，数据会在 MemStore 进行排序； 
6.向客户端发送 ack； 
7.等达到 MemStore 的刷写时机后，将数据刷写到 HFile。





获取锁

添加时间戳

构建wal,并没有写入磁盘

内存中添加wal



写入内存

释放锁

同步wal(判断是否出错，出错则回滚)



判断同步是否失败





4.老版本，现在不提供给用户

当 WAL 文件的数量超过 hbase.regionserver.max.logs，region 会按照时间顺序依次进
行刷写，直到 WAL 文件数量减小到 hbase.regionserver.max.log 以下（该属性名已经废弃，
现无需手动设置，最大值为 32）。









文件数小于三个小合并相当于大合并



compact:小合并



compactmajor:大合并

















写入日志



写入缓存



日志写入文件







不同列ddf

0.4:阻塞



0.4*0.95:不阻塞





最后一条到达1h就会写入磁盘中





每次flush都会生成一个hfile文件



flush:都在内存中则会删除

大合并





R:region数





官网不推荐使用多列组合



如果有n1, n2, n3

n2, n3列组数据少，每次flush n2, n3 就会有很多小文件

​    
