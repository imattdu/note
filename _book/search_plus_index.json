{"./":{"url":"./","title":"快速开始","keywords":"","body":"Introduction Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-26 16:56:40 "},"09tool/docker/docker.html":{"url":"09tool/docker/docker.html","title":"docker","keywords":"","body":" 概述 是什么 一次封装， 到处运行 通过镜像构建一个容器，这个容器会提前安装一些软件 安装卸载 安装 该教程适用centos7 参考手册 https://docs.docker.com/install/linux/docker-ce/centos/ 查看版本号 cat /etc/redhat-release 确保网络正常，安装gcc yum -y install gcc 安装gcc-c++ yum -y install gcc-c++ 卸载旧的版本 yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 安装需要的软件包 yum install -y yum-utils 设置stable镜像仓库 推荐使用这个 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 更新yum软件包索引 yum makecache fast 安装DOCKER CE yum install docker-ce docker-ce-cli containerd.io 启动docker systemctl start docker docker version 测试 docker run hello-world 配置阿里云镜像 mkdir -p /etc/docker vim /etc/docker/daemon.json 阿里云 { \"registry-mirrors\": [\"https://l0s7i35d.mirror.aliyuncs.com\"] } systemctl daemon-reload systemctl restart docker 卸载 systemctl stop docker yum -y remove docker-ce docker-ce-cli containerd.io rm -rf /var/lib/docker rm -rf /var/lib/containerd 错误 可以参考linux 目录下Linux安装和基本配置.md中的磁盘扩容，更好的方式是安装的时候推荐50g空间 空间不足 基本组成 镜像：Docker 镜像（Image）就是一个只读的模板。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。 容器：Docker 利用容器（Container）独立运行的一个或一组应用。容器是用镜像创建的运行实例。 仓库：仓库（Repository）是集中存放镜像文件的场所。 原理 如何工作 Docker是一个Client-Server结构的系统，Docker守护进程运行在主机上， 然后通过Socket连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。 容器，是一个运行时环境，就是我们前面说到的集装箱。 为什么比vm快 (1)docker有着比虚拟机更少的抽象层。由亍docker不需要Hypervisor实现硬件资源虚拟化,运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。 (2)docker利用的是宿主机的内核,而不需要Guest OS。因此,当新建一个容器时,docker不需要和虚拟机一样重新加载一个操作系统内核。仍而避免引寻、加载操作系统内核返个比较费时费资源的过程,当新建一个虚拟机时,虚拟机软件需要加载Guest OS,返个新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统,则省略了返个过程,因此新建一个docker容器只需要几秒钟。 常用命令 帮助命令 docker version docker info docker --help 镜像命令 docker images列出本地主机上的镜像 docker images REPOSITORY：表示镜像的仓库源 TAG：镜像的标签 IMAGE ID：镜像ID CREATED：镜像创建时间 SIZE：镜像大小 [root@iz2zeiw2bqogm8ir3ugpvqz ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE hello-world latest d1165f221234 5 months ago 13.3kB 同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。 如果你不指定一个镜像的版本标签，例如你只使用 ubuntu，docker 将默认使用 ubuntu:latest 镜像 -a :列出本地所有的镜像（含中间映像层） -q :只显示镜像ID。 --digests :显示镜像的摘要信息 --no-trunc :显示完整的镜像信息 docker images --digests docker search docker search:从docker hub中搜，下载从阿里云镜像 docker search [OPTIONS] 镜像名字 https://hub.docker.com docker search tomcat --no-trunc : 显示完整的镜像描述 docker pull docker pull 镜像名字[:TAG] docker pull tomcat docker rmi docker rmi 某个XXX镜像名字ID 删除单个 docker rmi -f 镜像ID 删除多个 docker rmi -f 镜像名1:TAG 镜像名2:TAG 删除全部 docker rmi -f $(docker images -qa) 容器命令 新建并启动容器 docker run [OPTIONS] IMAGE [COMMAND] [ARG...] OPTIONS说明（常用）：有些是一个减号，有些是两个减号 --name=\"容器新名字\": 为容器指定一个名称； -d: 后台运行容器，并返回容器ID，也即启动守护式容器； -i：以交互模式运行容器，通常与 -t 同时使用； -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用； -P: 随机端口映射； -p: 指定端口映射，有以下四种格式 ip:hostPort:containerPort ip::containerPort hostPort:containerPort containerPort docker run -it centos /bin/bash 退出容器 容器不停止 ctrl+p+q 容器停止 exit 列出当前运行的容器 docker ps [OPTIONS] docker ps OPTIONS说明（常用）： -a :列出当前所有正在运行的容器+历史上运行过的 -l :显示最近创建的容器。 -n：显示最近n个创建的容器。 -q :静默模式，只显示容器编号。 --no-trunc :不截断输出。 常用 docker ps -qa docker ps -a 启动容器 //docker start 容器ID或者容器名 docker start centos:centos7 重启容器 docker restart 容器ID或者容器名 停止容器 docker stop 容器ID或者容器名 强制停止容器 docker kill 容器ID或者容器名 删除已停止的容器 docker rm 容器ID 一次删除多个 docker rm -f $(docker ps -a -q) docker ps -a -q | xargs docker rm 进入容器 docker exec -it 容器id /bin/baah docker attach 容器i 重要命令 以后台方式启动 docker run -d 容器名 docker run -d tomcat 查看容器日志 docker logs -f -t --tail 容器ID -t 是加入时间戳 -f 跟随最新的日志打印 --tail 数字 显示最后多少条 查看容器内运行的进程 docker top 容器ID 查看容器内部细节 docker inspect 容器ID 进入正在运行的容器并以命令行交互 docker exec -it 容器ID bashShell docker exec -it xxccc /bin/bash docker attach 容器ID 区别 attach 直接进入容器启动命令的终端，不会启动新的进程 exec 是在容器中打开新的终端，并且可以启动新的进程 从容器内拷贝文件到主机上 docker cp 容器ID:容器内路径 目的主机路径 attach Attach to a running container # 当前 shell 下 attach 连接指定运行镜像 build Build an image from a Dockerfile # 通过 Dockerfile 定制镜像 commit Create a new image from a container changes # 提交当前容器为新的镜像 cp Copy files/folders from the containers filesystem to the host path #从容器中拷贝指定文件或者目录到宿主机中 create Create a new container # 创建一个新的容器，同 run，但不启动容器 diff Inspect changes on a container's filesystem # 查看 docker 容器变化 events Get real time events from the server # 从 docker 服务获取容器实时事件 exec Run a command in an existing container # 在已存在的容器上运行命令 export Stream the contents of a container as a tar archive # 导出容器的内容流作为一个 tar 归档文件[对应 import ] history Show the history of an image # 展示一个镜像形成历史 images List images # 列出系统当前镜像 import Create a new filesystem image from the contents of a tarball # 从tar包中的内容创建一个新的文件系统映像[对应export] info Display system-wide information # 显示系统相关信息 inspect Return low-level information on a container # 查看容器详细信息 kill Kill a running container # kill 指定 docker 容器 load Load an image from a tar archive # 从一个 tar 包中加载一个镜像[对应 save] login Register or Login to the docker registry server # 注册或者登陆一个 docker 源服务器 logout Log out from a Docker registry server # 从当前 Docker registry 退出 logs Fetch the logs of a container # 输出当前容器日志信息 port Lookup the public-facing port which is NAT-ed to PRIVATE_PORT # 查看映射端口对应的容器内部源端口 pause Pause all processes within a container # 暂停容器 ps List containers # 列出容器列表 pull Pull an image or a repository from the docker registry server # 从docker镜像源服务器拉取指定镜像或者库镜像 push Push an image or a repository to the docker registry server # 推送指定镜像或者库镜像至docker源服务器 restart Restart a running container # 重启运行的容器 rm Remove one or more containers # 移除一个或者多个容器 rmi Remove one or more images # 移除一个或多个镜像[无容器使用该镜像才可删除，否则需删除相关容器才可继续或 -f 强制删除] run Run a command in a new container # 创建一个新的容器并运行一个命令 save Save an image to a tar archive # 保存一个镜像为一个 tar 包[对应 load] search Search for an image on the Docker Hub # 在 docker hub 中搜索镜像 start Start a stopped containers # 启动容器 stop Stop a running containers # 停止容器 tag Tag an image into a repository # 给源中镜像打标签 top Lookup the running processes of a container # 查看容器中运行的进程信息 unpause Unpause a paused container # 取消暂停容器 version Show the docker version information # 查看 docker 版本号 wait Block until a container stops, then print its exit code # 截取容器停止时的退出状态值 镜像 是什么 一种可以创建运行环境的软件 特点 Docker镜像都是只读的 当容器启动时，一个新的可写层被加载到镜像的顶部。 这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。 docker commit docker commit提交容器副本使之成为一个新的镜像 docker commit -m=“提交的描述信息” -a=“作者” 容器ID 要创建的目标镜像名:[标签名] docker run -it -p 8080:8080 tomcat cd /usr/local/tomcat/webapps touch a.txt docker commit -m=\"my centos\" -a=\"matt\" xxcc mcentos:1.0 容器数据卷 可以干什么 容器的持久化 容器建继承+共享数据 数据卷 命令 docker run -it -v /宿主机绝对路径目录:/容器内目录 镜像名 # 查看容器是否挂载成功 docker inspect 容器ID # 只读 docker run -it -v /宿主机绝对路径目录:/容器内目录:ro 镜像名 容器停止，对宿主机的修改仍可以对容器发生改变 docker run -it -v /home/matt/dataVolumeContainer:/dataVolumeContainer centos /bin/bash Dockerfile # volume test FROM centos VOLUME [\"/dataVolumeContainer1\",\"/dataVolumeContainer2\"] CMD echo \"finished,--------success1\" CMD /bin/bash docker build -f /home/matt/docker/Dockerfile -t matt/centos . 镜像是千层饼，一个镜像可能多层组成 !> 一段重要的内容，随机端口号 docker run -dit -P 8080 tomcat 宿主机端口随机分配 问题 没遇到 Docker挂载主机目录Docker访问出现cannot open directory .: Permission denied 解决办法：在挂载目录后多加一个--privileged=true参数即可 数据卷容器 是什么 命名的容器挂载数据卷，其它容器通过挂载这个(父容器)实现数据共享，挂载数据卷的容器，称之为数据卷容器 c1 docker run -it --name c1 matt/centos c2继承c1 docker run -it --name c2 --volumes-from c1 matt/centos docker run -it --name c3 --volumes-from c1 matt/centos c2，c3数据就可以共享，即使删除c1，仍然可以共享 容器之间配置信息的传递，数据卷的生命周期一直持续到没有容器使用它为止 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-12 11:54:57 "},"09tool/docker/dockerfile.html":{"url":"09tool/docker/dockerfile.html","title":"dockerfile","keywords":"","body":"dockerfile 是什么 Dockerfile是用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本。 构建过程解析 Dockerfile内容基础知识 每条保留字指令都必须为大写字母且后面要跟随至少一个参数 指令按照从上到下，顺序执行 表示注释 每条指令都会创建一个新的镜像层，并对镜像进行提交 Dockerfile是软件的原材料 Docker镜像是软件的交付品 Docker容器则可以认为是软件的运行态。 大致流程 （1）docker从基础镜像运行一个容器 （2）执行一条指令并对容器作出修改 （3）执行类似docker commit的操作提交一个新的镜像层 （4）docker再基于刚提交的镜像运行一个新容器 （5）执行dockerfile中的下一条指令直到所有指令都执行完成 关键字 关键字 描述 FROM 基础镜像，当前新镜像是基于哪个镜像的 MAINTAINER 镜像维护者的姓名和邮箱地址 RUN 容器构建时需要运行的命令 EXPOSE 当前容器对外暴露出的端口 WORKDIR 指定在创建容器后，终端默认登陆的进来工作目录，一个落脚点 ENV 用来在构建镜像过程中设置环境变量 ADD 将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar压缩包 COPY 类似ADD，拷贝文件和目录到镜像中。将从构建上下文目录中 的文件/目录复制到新的一层的镜像内的 位置 VOLUME 容器数据卷，用于数据保存和持久化工作 CMD 指定一个容器启动时要运行的命令（Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换） ENTRYPOINT 指定一个容器启动时要运行的命令（ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数） ONBUILD 当构建一个被继承的Dockerfile时运行命令，父镜像在被子继承后父镜像的onbuild被触发 ENV MY_PATH /usr/mytest 这个环境变量可以在后续的任何RUN指令中使用，这就如同在命令前面指定了环境变量前缀一样； 也可以在其它指令中直接使用这些环境变量， 比如：WORKDIR $MY_PATH ONBUILD 案例 dockerfile FROM centos:centos7 MAINTAINER matt #把宿主机当前上下文的c.txt拷贝到容器/usr/local/路径下 COPY c.txt /usr/local/cincontainer.txt #把java与tomcat添加到容器中 ADD jdk-8u171-linux-x64.tar.gz /usr/local/ ADD apache-tomcat-9.0.8.tar.gz /usr/local/ #安装vim编辑器 RUN yum -y install vim #设置工作访问时候的WORKDIR路径，登录落脚点 ENV MYPATH /usr/local WORKDIR $MYPATH #配置java与tomcat环境变量 ENV JAVA_HOME /usr/local/jdk1.8.0_171 ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar ENV CATALINA_HOME /usr/local/apache-tomcat-9.0.8 ENV CATALINA_BASE /usr/local/apache-tomcat-9.0.8 ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin #容器运行时监听的端口 EXPOSE 8080 #启动时运行tomcat # ENTRYPOINT [\"/usr/local/apache-tomcat-9.0.8/bin/startup.sh\" ] # CMD [\"/usr/local/apache-tomcat-9.0.8/bin/catalina.sh\",\"run\"] CMD /usr/local/apache-tomcat-9.0.8/bin/startup.sh && tail -F /usr/local/apache-tomcat-9.0.8/bin/logs/catalina.out 构建 docker build -t mcentos:1 # 默认会使用该文件夹Dockerfile 也可以通过-f指定其他文件夹 docker build -f Dockerfile -t mcentos:1 常用软件安装 mysql docker pull mysql:5.6 docker run -p 12345:3306 --name mysql -v /zzyyuse/mysql/conf:/etc/mysql/conf.d -v /zzyyuse/mysql/logs:/logs -v /zzyyuse/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 docker run -p 12345:3306 --name mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 redis docker pull redis:3.2 # --appendonly yes 开启持久化 docker run -p 6379:6379 -v /home/matt/myredis/data:/data -v /home/matt/myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf -d redis:3.2 redis-server /usr/local/etc/redis/redis.conf --appendonly yes aliyun https://dev.aliyun.com/search.html Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-05 13:30:56 "},"09tool/jetBrains.html":{"url":"09tool/jetBrains.html","title":"jetbrain","keywords":"","body":"安装 下一步下一步安装即可，安装时选择要安装的组件，注意存储位置 配置 IDEA配置 常规配置 视图显示 显示菜单字体 控制台字体 设置鼠标滚轮修改字体大小 设置自动导包 忽略大小写 Git配置 maven配置 注释 类的注释 /** @author matt @create ${YEAR}-${MONTH}-${DAY} ${TIME} */ 方法的注释 ** * 功能： * @author matt * @date $date$ $param$ * @return $return$ */ groovyScript(\"def result=''; def params=\\\"${_1}\\\".replaceAll('[\\\\\\\\[|\\\\\\\\]|\\\\\\\\s]', '').split(',').toList(); for(i = 0; i 注释顶格 取消自动更新 终端配置 插件 关闭拼写检查 Spelling spring 复制到idea中的文件需要重构项目否则无法访问404 junit无法从控制台输入 help ->Edit Custom VM Options -Deditable.java.test.console=true properties文件乱码 如果还是不起作用删除文件重新创建即可 pycharm配置 pycharm取消波浪线提示 进入波浪线设置界面看看到上方有三个设置项None、Syntax、Inspections，可以拖动箭头设置。 1.None表示没有波浪线； 2.Syntax表示只有语法错误显示波浪线； 3.Inspections表示语法错误和不符合PEP8规范显示波浪线。 使用 IDEA 打开一个项目 对于maven项目我们打开选择pom.xml即可 常规使用 如何重启 pycharm如何创建一个项目 1.设置新项目名称和存储路径（untitled可以修改）； 2.Project Interpreter设置新建项目所依赖的python环境； ​ 2.1 New environment using 设置新的依赖环境。在项目中新建一个venv（virtualenv）目录，用于存放虚拟的python环境，这里所有的类库依赖都可以直接脱离系统安装的python独立运行； ​ 2.1.1 勾选上Inherit global site-packages则可以使用base interpreter（基础解释器）中安装的第三方库（即本地Python的site-packages目录中的类库）；不选将和外界完全隔离（会在base interpreter的基础上创建一个新的虚拟解释器）； ​ 2.1.2 勾选上Make available to all projects则可以将此项目的虚拟环境提供给其他项目使用； ​ 2.2 Existing Interpreter关联已经存在的python解释器，可以使用该解释器所安装的Python库； 建议选择 New environment using 可以在Base Interpreter选择系统中安装的Python解释器，这样可以让项目独立部署运行，也可以避免一台服务器部署多个项目之间存在类库的版本依赖问。 压缩包引入到IDEA，help->reset auto -> reset goland 配置 使用goland创建项目 clion 如何 多个main函数 CMakeLists.txt cmake_minimum_required(VERSION 3.19) project(demo) set(CMAKE_CXX_STANDARD 14) # 遍历项目根目录下所有的 .cpp 文件 file (GLOB_RECURSE files *.cpp) foreach (file ${files}) string(REGEX REPLACE \".+/(.+)/(.+)\\\\..*\" \"\\\\1-\\\\2\" exe ${file}) add_executable (${exe} ${file}) message (\\ \\ \\ \\ --\\ src/${exe}.cpp\\ will\\ be\\ compiled\\ to\\ bin/${exe}) endforeach () clion 常用配置 导入bits/stdc++.h出错 brew install gcc /opt/homebrew/Cellar/gcc/11.2.0_3/bin/gcc-11 /opt/homebrew/Cellar/gcc/11.2.0_3/bin/g++-11 -D CMAKE_CXX_COMPILER=/opt/homebrew/Cellar/gcc/11.2.0_3/bin/g++-11 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-26 19:44:16 "},"10algorithm/base/基础算法.html":{"url":"10algorithm/base/基础算法.html","title":"基础算法","keywords":"","body":"123434 快排 模板 algorithm x j, j+1 l j,j+1 r i-1,i void quick_sort(int q[], int l, int r) { if (l >= r) return; int i = l - 1, j = r + 1, x = q[l + r >> 1]; while (i x); if (i 快速排序 给定你一个长度为 n 的整数数列。 请你使用快速排序对这个数列按照从小到大进行排序。 并将排好序的数列按顺序输出。 输入格式 输入共两行，第一行包含整数 n。 第二行包含 n 个整数（所有整数均在 1∼1091∼109 范围内），表示整个数列。 输出格式 输出共一行，包含 nn 个整数，表示排好序的数列。 数据范围 1≤n≤100000 输入样例： 5 3 1 2 4 5 输出样例： 1 2 3 4 5 code #include #include using namespace std; const int N = 1e5+10; void quick_sort(int q[], int l, int r) { if (l >= r) return; int i = l - 1, j = r + 1, x = q[l + r >> 1]; while (i x); if (i 第k个数 给定一个长度为 nn 的整数数列，以及一个整数 kk，请用快速选择算法求出数列从小到大排序后的第 kk 个数。 输入格式 第一行包含两个整数 nn 和 kk。 第二行包含 n个整数（所有整数均在 1∼1e9 范围内），表示整数数列。 输出格式 输出一个整数，表示数列的第 k 小数。 数据范围 1≤n≤100000, 1≤k≤n 输入样例： 5 3 2 4 1 5 3 输出样例： 3 code #include #include using namespace std; const int N = 1e5+10; int q[N]; int quick_sort(int l, int r, int k) { if (l == r) return q[l]; int i = l - 1, j = r + 1, x = q[l + r >> 1]; while (i x); if (i Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-12 13:54:38 "},"10algorithm/base/数据结构.html":{"url":"10algorithm/base/数据结构.html","title":"数据结构","keywords":"","body":"链表 单链表 实现一个单链表，链表初始为空，支持三种操作： 向链表头插入一个数； 删除第 kk 个插入的数后面的数； 在第 kk 个插入的数后插入一个数。 现在要对该链表进行 MM 次操作，进行完所有操作后，从头到尾输出整个链表。 注意:题目中第 k 个插入的数并不是指当前链表的第 k 个数。例如操作过程中一共插入了 n 个数，则按照插入的时间顺序，这 n 个数依次为：第 11 个插入的数，第 22 个插入的数，…第 nn 个插入的数。 输入格式 第一行包含整数 M，表示操作次数。 接下来 M 行，每行包含一个操作命令，操作命令可能为以下几种： H x，表示向链表头插入一个数 xx。 D k，表示删除第 kk 个插入的数后面的数（当 kk 为 00 时，表示删除头结点）。 I k x，表示在第 kk 个插入的数后面插入一个数 xx（此操作中 kk 均大于 00）。 输出格式 共一行，将整个链表从头到尾输出。 数据范围 1≤M≤100000 所有操作保证合法。 输入样例： 10 H 9 I 1 1 D 1 D 0 H 6 I 3 6 I 4 5 I 4 5 I 3 4 D 6 输出样例： 6 4 6 5 code #include using namespace std; const int N = 1e5 + 10; int head, e[N], ne[N], idx; void init() { head = -1; } void add_to_head(int x) { e[idx] = x; ne[idx] = head; head = idx++; } void add(int k, int x) { e[idx] = x; ne[idx] = ne[k]; ne[k] = idx++; } void remove(int k) { ne[k] = ne[ne[k]]; } int main() { init(); int m; cin >> m; while (m--) { char ch; int k, x; cin >> ch; if (ch == 'H') { cin >> x; add_to_head(x); } else if (ch == 'I') { cin >> k >> x; add(k - 1, x); } else if(ch == 'D') { cin >> k; if (k == 0) head = ne[head]; else remove(k - 1); } } int t = head; while (t != -1) { cout 双链表 实现一个双链表，双链表初始为空，支持 55 种操作： 在最左侧插入一个数； 在最右侧插入一个数； 将第 kk 个插入的数删除； 在第 kk 个插入的数左侧插入一个数； 在第 kk 个插入的数右侧插入一个数 现在要对该链表进行 MM 次操作，进行完所有操作后，从左到右输出整个链表。 注意:题目中第 kk 个插入的数并不是指当前链表的第 kk 个数。例如操作过程中一共插入了 nn 个数，则按照插入的时间顺序，这 nn 个数依次为：第 11 个插入的数，第 22 个插入的数，…第 nn 个插入的数。 输入格式 第一行包含整数 MM，表示操作次数。 接下来 MM 行，每行包含一个操作命令，操作命令可能为以下几种： L x，表示在链表的最左端插入数 xx。 R x，表示在链表的最右端插入数 xx。 D k，表示将第 kk 个插入的数删除。 IL k x，表示在第 kk 个插入的数左侧插入一个数。 IR k x，表示在第 kk 个插入的数右侧插入一个数。 输出格式 共一行，将整个链表从左到右输出。 数据范围 1≤M≤1000001≤M≤100000 所有操作保证合法。 输入样例： 10 R 7 D 1 L 3 IL 2 10 D 3 IL 2 7 L 8 R 9 IL 4 7 IR 2 2 输出样例： 8 7 7 3 2 9 左 -> 右 code #include using namespace std; const int N = 1e5 + 10; int e[N], l[N], r[N], idx; void init() { r[0] = 1; l[1] = 0; idx = 2; } void add(int k, int x) { e[idx] = x; l[idx] = k; r[idx] = r[k]; l[r[k]] = idx; r[k] = idx++; } void remove(int k) { r[l[k]] = r[k]; l[r[k]] = l[k]; } int main() { init(); int m; cin >> m; while (m--) { string s; int k, x; cin >> s; if (s == \"L\") { cin >> x; add(0, x); } else if (s == \"R\") { cin >> x; add(l[1], x); } else if (s == \"D\") { cin >> k; remove(k + 1); } else if (s == \"IL\") { cin >> k >> x; add(l[k + 1], x); } else { cin >> k >> x; add(k + 1, x); } } int t = r[0]; while (t != 1) { cout 栈 栈 实现一个栈，栈初始为空，支持四种操作： push x – 向栈顶插入一个数 xx； pop – 从栈顶弹出一个数； empty – 判断栈是否为空； query – 查询栈顶元素。 现在要对栈进行 MM 个操作，其中的每个操作 33 和操作 44 都要输出相应的结果。 输入格式 第一行包含整数 MM，表示操作次数。 接下来 MM 行，每行包含一个操作命令，操作命令为 push x，pop，empty，query 中的一种。 输出格式 对于每个 empty 和 query 操作都要输出一个查询结果，每个结果占一行。 其中，empty 操作的查询结果为 YES 或 NO，query 操作的查询结果为一个整数，表示栈顶元素的值。 数据范围 1≤M≤1000001≤M≤100000, 1≤x≤1091≤x≤109 所有操作保证合法。 输入样例： 10 push 5 query push 6 pop query pop empty push 4 query empty 输出样例： 5 5 YES 4 NO code #include #include using namespace std; const int N = 1e5 + 10; int stk[N], tt; int main() { int m; cin >> m; string op; int x; while (m--) { cin >> op; if (op == \"push\") { scanf(\"%d\", &x); stk[++tt] = x; } else if (op == \"pop\") tt--; else if (op == \"query\") printf(\"%d\\n\", stk[tt]); else if (op == \"empty\") { if (tt) cout 表达式求值 给定一个表达式，其中运算符仅包含 +,-,*,/（加 减 乘 整除），可能包含括号，请你求出表达式的最终值。 注意： 数据保证给定的表达式合法。 题目保证符号 - 只作为减号出现，不会作为负号出现，例如，-1+2,(2+2)*(-(1+1)+2) 之类表达式均不会出现。 题目保证表达式中所有数字均为正整数。 题目保证表达式在中间计算过程以及结果中，均不超过 231−1231−1。 题目中的整除是指向 00 取整，也就是说对于大于 00 的结果向下取整，例如 5/3=15/3=1，对于小于 00 的结果向上取整，例如 5/(1−4)=−15/(1−4)=−1。 C++和Java中的整除默认是向零取整；Python中的整除//默认向下取整，因此Python的eval()函数中的整除也是向下取整，在本题中不能直接使用。 输入格式 共一行，为给定表达式。 输出格式 共一行，为表达式的结果。 数据范围 表达式的长度不超过 105105。 输入样例： (2+2)*(1+1) 输出样例： 8 code #include #include #include using namespace std; stack op; stack num; unordered_map pr{ {'+', 1}, {'-', 1}, {'*', 2}, {'/', 2} }; void eval() { int b = num.top(); num.pop(); int a = num.top(); num.pop(); int x = 0; char c = op.top(); op.pop(); if (c == '+') x = a + b; else if (c == '-') x = a - b; else if (c == '*') x = a * b; else x = a / b; num.push(x); } int main() { string str; cin >> str; for (int i = 0; i = pr[str[i]]) eval(); op.push(str[i]); } } while (op.size()) eval(); cout 队列 模拟队列 实现一个队列，队列初始为空，支持四种操作： push x – 向队尾插入一个数 xx； pop – 从队头弹出一个数； empty – 判断队列是否为空； query – 查询队头元素。 现在要对队列进行 MM 个操作，其中的每个操作 33 和操作 44 都要输出相应的结果。 输入格式 第一行包含整数 MM，表示操作次数。 接下来 MM 行，每行包含一个操作命令，操作命令为 push x，pop，empty，query 中的一种。 输出格式 对于每个 empty 和 query 操作都要输出一个查询结果，每个结果占一行。 其中，empty 操作的查询结果为 YES 或 NO，query 操作的查询结果为一个整数，表示队头元素的值。 数据范围 1≤M≤1000001≤M≤100000, 1≤x≤1091≤x≤109, 所有操作保证合法。 输入样例： 10 push 6 empty query pop empty push 3 push 4 pop query push 6 输出样例： NO 6 YES 4 code #include #include using namespace std; const int N = 1e5 + 10; int q[N], hh, tt = -1; int main() { int n; cin >> n; string op; int x; while (n--) { cin >> op; if (op == \"push\") { cin >> x; q[++tt] = x; } else if (op == \"pop\") hh++; else if (op == \"query\") printf(\"%d\\n\", q[hh]); else { if (hh 循环队列 // hh 表示队头，tt表示队尾的后一个位置 int q[N], hh = 0, tt = 0; // 向队尾插入一个数 q[tt ++ ] = x; if (tt == N) tt = 0; // 从队头弹出一个数 hh ++ ; if (hh == N) hh = 0; // 队头的值 q[hh]; // 判断队列是否为空 if (hh != tt) { } 单调栈 给定一个长度为 N 的整数数列，输出每个数左边第一个比它小的数，如果不存在则输出 −1。 输入格式 第一行包含整数 N，表示数列长度。 第二行包含 N 个整数，表示整数数列。 输出格式 共一行，包含 N 个整数，其中第 i 个数表示第 i 个数的左边第一个比它小的数，如果不存在则输出 −1。 数据范围 1≤N≤105 1≤数列中元素≤109 输入样例： 5 3 4 2 7 5 输出样例： -1 3 -1 2 2 #include using namespace std; const int N = 1e5 + 10; int a[N], stk[N], tt; int main() { int n; scanf(\"%d\", &n); for (int i = 0; i = a[i]) tt--; if (tt) printf(\"%d \", stk[tt]); else printf(\"-1 \"); stk[++tt] = a[i]; } } 单调队列 滑动窗口 给定一个大小为 n≤10^6 的数组。 有一个大小为 k 的滑动窗口，它从数组的最左边移动到最右边。 你只能在窗口中看到 k 个数字。 每次滑动窗口向右移动一个位置。 以下是一个例子： 该数组为 [1 3 -1 -3 5 3 6 7]，k 为 3。 窗口位置 最小值 最大值 [1 3 -1] -3 5 3 6 7 -1 3 1 [3 -1 -3] 5 3 6 7 -3 3 1 3 [-1 -3 5] 3 6 7 -3 5 1 3 -1 [-3 5 3] 6 7 -3 5 1 3 -1 -3 [5 3 6] 7 3 6 1 3 -1 -3 5 [3 6 7] 3 7 你的任务是确定滑动窗口位于每个位置时，窗口中的最大值和最小值。 输入格式 输入包含两行。 第一行包含两个整数 n 和 k，分别代表数组长度和滑动窗口的长度。 第二行有 n 个整数，代表数组的具体数值。 同行数据之间用空格隔开。 输出格式 输出包含两个。 第一行输出，从左至右，每个位置滑动窗口中的最小值。 第二行输出，从左至右，每个位置滑动窗口中的最大值。 输入样例： 8 3 1 3 -1 -3 5 3 6 7 输出样例： -1 -3 -3 -3 3 3 3 3 5 5 6 7 code #include using namespace std; const int N = 1e6 + 10; int a[N], q[N], hh = 0, tt = -1; int main() { int n, k; scanf(\"%d %d\", &n, &k); for (int i = 0; i q[hh]) hh++; while (hh = a[i]) tt--; q[++tt] = i; if (i - k + 1 >= 0) printf(\"%d \", a[q[hh]]); } puts(\"\"); hh = 0, tt = -1; for (int i = 0; i q[hh]) hh++; while (hh = 0) printf(\"%d \", a[q[hh]]); } puts(\"\"); return 0; } var gitalk = new Gitalk({ \"clientID\": \"8ab96766a9f6ec805ff4\", \"clientSecret\": \"df4b41cc098c4e9e2949fb5b72f98cefadb4f29f\", \"repo\": \"iweiwan.github.io\", \"owner\": \"iweiwan\", \"admin\": [\"iweiwan\"], \"id\": decodeURI(location.pathname), \"distractionFreeMode\": false }); gitalk.render(\"gitalk-container\"); Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-12 10:17:53 "},"10algorithm/base/搜索与图论.html":{"url":"10algorithm/base/搜索与图论.html","title":"搜索与图论","keywords":"","body":"搜索与图论 dfs 排列数字 给定一个整数 n，将数字 1∼n排成一排，将会有很多种排列方法。 现在，请你按照字典序将所有的排列方法输出。 输入格式 共一行，包含一个整数 n。 输出格式 按字典序输出所有排列方案，每个方案占一行。 数据范围 1≤n≤7 输入样例： 3 输出样例： 1 2 3 1 3 2 2 1 3 2 3 1 3 1 2 3 2 1 code #include using namespace std; const int N = 10; int path[N]; bool st[N]; int n; void dfs(int u) { if (u == n) { for (int i = 0; i > n; dfs(0); return 0; } n-皇后问题 n−n−皇后问题是指将 n 个皇后放在 n×n 的国际象棋棋盘上，使得皇后不能相互攻击到，即任意两个皇后都不能处于同一行、同一列或同一斜线上。 现在给定整数 nn，请你输出所有的满足条件的棋子摆法。 输入格式 共一行，包含整数 n。 输出格式 每个解决方案占 n 行，每行输出一个长度为 n 的字符串，用来表示完整的棋盘状态。 其中 . 表示某一个位置的方格状态为空，Q 表示某一个位置的方格上摆着皇后。 每个方案输出完成后，输出一个空行。 注意：行末不能有多余空格。 输出方案的顺序任意，只要不重复且没有遗漏即可。 数据范围 1≤n≤9 输入样例： 4 输出样例： .Q.. ...Q Q... ..Q. ..Q. Q... ...Q .Q.. code 2^(n^2) 每个点都需要访问俩次 #include using namespace std; const int N = 13; char g[N][N]; int n; bool row[N], col[N], dg[2 * N], udg[2 * N]; void dfs(int x, int y, int s) { if (y == n) { x++, y = 0; } if (x == n) { if (s == n) { for (int i = 0; i > n; for (int i = 0; i o(n^2) #include using namespace std; const int N = 13; char g[N][N]; bool col[N], dg[2 * N], udg[2 * N]; int n; void dfs(int u) { if (u == n) { for (int i = 0; i > n; for (int i = 0; i bfs 走迷宫 给定一个 n×m 的二维整数数组，用来表示一个迷宫，数组中只包含 0 或 1，其中 0 表示可以走的路，1 表示不可通过的墙壁。 最初，有一个人位于左上角 (1,1) 处，已知该人每次可以向上、下、左、右任意一个方向移动一个位置。 请问，该人从左上角移动至右下角 (n,m) 处，至少需要移动多少次。 数据保证 (1,1) 处和 (n,m) 处的数字为 0，且一定至少存在一条通路。 输入格式 第一行包含两个整数 n 和 m。 接下来 n 行，每行包含 m 个整数（0 或 1），表示完整的二维数组迷宫。 输出格式 输出一个整数，表示从左上角移动至右下角的最少移动次数。 数据范围 1≤n,m≤100 输入样例： 5 5 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 输出样例： 8 code #include #include #include using namespace std; typedef pair PII; const int N = 110; int g[N][N], d[N][N]; int n, m; void bfs() { queue q; memset(d, -1, sizeof d); d[0][0] = 0; q.push({0, 0}); int dx[4] = {-1, 0, 1, 0}, dy[4] = {0, 1, 0, -1}; while (!q.empty()) { auto t = q.front(); q.pop(); for (int i = 0; i = 0 && x = 0 && y > n >> m; for (int i = 0; i > g[i][j]; } bfs(); return 0; } 八数码 在一个 3×33×3 的网格中，1∼81∼8 这 88 个数字和一个 x 恰好不重不漏地分布在这 3×33×3 的网格中。 例如： 1 2 3 x 4 6 7 5 8 在游戏过程中，可以把 x 与其上、下、左、右四个方向之一的数字交换（如果存在）。 我们的目的是通过交换，使得网格变为如下排列（称为正确排列）： 1 2 3 4 5 6 7 8 x 例如，示例中图形就可以通过让 x 先后与右、下、右三个方向的数字交换成功得到正确排列。 交换过程如下： 1 2 3 1 2 3 1 2 3 1 2 3 x 4 6 4 x 6 4 5 6 4 5 6 7 5 8 7 5 8 7 x 8 7 8 x 现在，给你一个初始网格，请你求出得到正确排列至少需要进行多少次交换。 输入格式 输入占一行，将 3×33×3 的初始网格描绘出来。 例如，如果初始网格如下所示： 1 2 3 x 4 6 7 5 8 则输入为：1 2 3 x 4 6 7 5 8 输出格式 输出占一行，包含一个整数，表示最少交换次数。 如果不存在解决方案，则输出 −1−1。 输入样例： 2 3 4 1 5 x 7 6 8 输出样例 19 code #include #include #include #include using namespace std; int bfs(string start) { int dx[4] = {-1, 0, 1, 0}, dy[4] = {0, 1, 0 , -1}; string end = \"12345678x\"; queue q; q.push(start); unordered_map dist; dist[start] = 0; while (!q.empty()) { auto t = q.front(); q.pop(); int distance = dist[t]; if (t == end) return distance; int k = t.find('x'); int x = k / 3, y = k % 3; for (int i = 0; i = 0 && a = 0 && b > c; start += c; } cout 图的深度优先遍历 树的重心 给定一颗树，树中包含 n 个结点（编号 1∼n）和 n−1 条无向边。 请你找到树的重心，并输出将重心删除后，剩余各个连通块中点数的最大值。 重心定义：重心是指树中的一个结点，如果将这个点删除后，剩余各个连通块中点数的最大值最小，那么这个节点被称为树的重心。 输入格式 第一行包含整数 n，表示树的结点数。 接下来 n−1 行，每行包含两个整数 a 和 b，表示点 a 和点 b 之间存在一条边。 输出格式 输出一个整数 m，表示将重心删除后，剩余各个连通块中点数的最大值。 数据范围 1≤n≤10^5 输入样例 9 1 2 1 7 1 4 2 8 2 5 4 3 3 9 4 6 输出样例： 4 #include #include #include using namespace std; const int N = 1e5 + 10, M = 2 * N; int h[N], e[M], ne[M], idx; bool st[N]; int ans = N, n; void add(int a, int b) { e[idx] = b, ne[idx] = h[a], h[a] = idx++; } int dfs(int u) { int sum = 1, res = 0; st[u] = true; for (int i = h[u]; i != -1; i = ne[i]) { int j = e[i]; if (!st[j]) { int s = dfs(j); res = max(res, s); sum += s; } } res = max(res, n - sum); ans = min(ans, res); return sum; } int main() { cin >> n; memset(h, -1, sizeof h); for (int i = 0; i 图的广度优先遍历 图中点的层次 给定一个 n 个点 m 条边的有向图，图中可能存在重边和自环。 所有边的长度都是 1，点的编号为 1∼n。 请你求出 1 号点到 n 号点的最短距离，如果从 1 号点无法走到 n号点，输出 −1。 输入格式 第一行包含两个整数 n 和 m。 接下来 mm 行，每行包含两个整数 a 和 b，表示存在一条从 a 走到 b 的长度为 1 的边。 输出格式 输出一个整数，表示 1 号点到 n 号点的最短距离。 数据范围 1≤n,m≤10^5 输入样例： 4 5 1 2 2 3 3 4 1 3 1 4 输出样例： 1 #include #include #include using namespace std; const int N = 1e5 + 10; int h[N], e[N], ne[N], idx; int d[N], hh, tt, q[N], n, m; void add(int a, int b) { e[idx] = b, ne[idx] = h[a], h[a] = idx++; } int bfs() { memset(d, -1, sizeof d); q[0] = 1; d[1] = 0; while (hh > n >> m; memset(h, -1, sizeof h); while (m--) { int a, b; scanf(\"%d %d\", &a, &b); add(a, b); } cout n皇后 截距 1111 图： 1.邻间矩阵 二维数组 # a -> b g[a][b] 2.邻间表 很多个单链表 1 -> 3 -> 4 2 -> 3 -> 4 5 -> 4 -> 3 e :值 h 头 ne next idx 边 ans = answer res = result vis = visited dx, dy 方向向量 stk = stack q = queue dummy 链表虚拟头节点 st state 有向图才有拓扑序列 有向无环图：拓扑图 1->2 1->3 2->3 1 2 3 起点在终点前面 1 入度 有几点边指向他 0 出度 有几点边指出去 2 最短路 1单源最短路 一个点到其他所有点的最短路 1.1 n:点的数量 m 边的数量 所有边权都是正数（朴素Dijkstra算法 n*n 稠密图、堆优化版的Dijkstra算法 mlogn ） 1.2 存在负权边 （Bellman-Ford nm, SPFA m 最坏 nm） 2多源汇最短路： 起点 终点 不确定 Floyd n^3 稠密图 领接矩阵 有向图的拓扑序列 给定一个 n 个点 m 条边的有向图，点的编号是 1 到 n，图中可能存在重边和自环。 请输出任意一个该有向图的拓扑序列，如果拓扑序列不存在，则输出 −1。 若一个由图中所有点构成的序列 A 满足：对于图中的每条边 (x,y)，x 在 A 中都出现在 y 之前，则称 A 是该图的一个拓扑序列。 输入格式 第一行包含两个整数 n 和 m。 接下来 m 行，每行包含两个整数 x 和 y，表示存在一条从点 x 到点 y 的有向边 (x,y)。 输出格式 共一行，如果存在拓扑序列，则输出任意一个合法的拓扑序列即可。 否则输出 −1。 数据范围 1≤n,m≤10^5 输入样例： 3 3 1 2 2 3 1 3 输出样例： 1 2 3 #include #include #include using namespace std; const int N = 1e5 + 10; int h[N], e[N], ne[N], idx; int q[N], hh, tt = -1, d[N], n, m; void add(int a, int b) { e[idx] = b, ne[idx] = h[a], h[a] = idx++; } bool tup_sort() { for (int i = 1; i > n >> m; memset(h, -1, sizeof h); while (m--) { int a, b; scanf(\"%d %d\", &a, &b); add(a, b), d[b]++; } if (tup_sort()) { for (int i = 0; i 最短路 朴素dijkstra-Dijkstra求最短路 I 给定一个 n 个点 m 条边的有向图，图中可能存在重边和自环，所有边权均为正值。 请你求出 1 号点到 n 号点的最短距离，如果无法从 1 号点走到 n 号点，则输出 −1。 输入格式 第一行包含整数 n 和 m。 接下来 m 行每行包含三个整数 x,y,z 表示存在一条从点 x 到点 y 的有向边，边长为 z。 输出格式 输出一个整数，表示 1 号点到 n 号点的最短距离。 如果路径不存在，则输出 −1−1。 数据范围 1≤n≤500, 1≤m≤10^5, 图中涉及边长均不超过10000。 输入样例： 3 3 1 2 2 2 3 1 1 3 4 输出样例： 3 o(n^2 + m) #include #include #include using namespace std; const int N = 510, INF = 0x3f3f3f3f; int g[N][N], d[N], n, m; bool st[N]; int dijkstra() { memset(d, 0x3f, sizeof d); d[1] = 0; for (int i = 0; i > n >> m; int a, b, w; memset(g, 0x3f, sizeof g); while (m--) { scanf(\"%d%d%d\", &a, &b, &w); g[a][b] = min(g[a][b], w); } cout 堆优化-Dijkstra求最短路 II 给定一个 nn 个点 mm 条边的有向图，图中可能存在重边和自环，所有边权均为非负值。 请你求出 11 号点到 nn 号点的最短距离，如果无法从 11 号点走到 nn 号点，则输出 −1−1。 输入格式 第一行包含整数 nn 和 mm。 接下来 mm 行每行包含三个整数 x,y,zx,y,z，表示存在一条从点 xx 到点 yy 的有向边，边长为 zz。 输出格式 输出一个整数，表示 11 号点到 nn 号点的最短距离。 如果路径不存在，则输出 −1−1。 数据范围 1≤n,m≤1.5×1051≤n,m≤1.5×105, 图中涉及边长均不小于 00，且不超过 1000010000。 数据保证：如果最短路存在，则最短路的长度不超过 109109。 输入样例： 3 3 1 2 2 2 3 1 1 3 4 输出样例： 3 mlogn #include using namespace std; const int N = 150010, INF = 0x3f3f3f3f; typedef pair PII; int h[N], e[N], ne[N], w[N], idx, d[N], n, m; bool st[N]; void add(int a, int b, int c) { w[idx] = c, e[idx] = b, ne[idx] = h[a], h[a] = idx++; } int dijkstra() { memset(d, 0x3f, sizeof d); priority_queue, greater> q; q.push({0, 1}); d[1] = 0; while (!q.empty()) { auto t = q.top(); q.pop(); int distance = t.first, ver = t.second; if (st[ver]) continue; st[ver] = true; for (int i = h[ver]; i != -1; i = ne[i]) { int j = e[i]; if (distance + w[i] > n >> m; int a, b, c; memset(h, -1, sizeof h); while (m--) { scanf(\"%d%d%d\", &a, &b, &c); add(a, b, c); } cout Bellman-ford 算法-有边数限制的最短路 给定一个 n 个点 m 条边的有向图，图中可能存在重边和自环， 边权可能为负数。 请你求出从 1 号点到 n 号点的最多经过 k 条边的最短距离，如果无法从 1 号点走到 n 号点，输出 impossible。 注意：图中可能 存在负权回路 。 输入格式 第一行包含三个整数 n,m,kn,m,k。 接下来 mm 行，每行包含三个整数 x,y,zx,y,z，表示存在一条从点 xx 到点 yy 的有向边，边长为 zz。 输出格式 输出一个整数，表示从 11 号点到 nn 号点的最多经过 kk 条边的最短距离。 如果不存在满足条件的路径，则输出 impossible。 数据范围 1≤n,k≤5001≤n,k≤500, 1≤m≤100001≤m≤10000, 任意边长的绝对值不超过 1000010000。 输入样例： 3 3 1 1 2 1 2 3 1 1 3 3 输出样例： 3 bellman_ford o(nm) 允许有负权边 经过1条边最短 经过2条边最短 .... #include #include #include using namespace std; const int N = 510, M = 1e5 + 10; struct Edge { int a, b, w; } e[M]; int dist[N], back[N]; int n, m, k; void bellman_ford() { memset(dist, 0x3f, sizeof dist); dist[1] = 0; for (int i = 0; i 0x3f3f3f / 2) puts(\"impossible\"); else printf(\"%d\", dist[n]); } int main() { scanf(\"%d%d%d\", &n, &m, &k); for (int i = 0; i dijkstra 不能负边 为了避免如下的串联情况， 在边数限制为一条的情况下，节点3的距离应该是3，但是由于串联情况，利用本轮更新的节点2更新了节点3的距离，所以现在节点3的距离是2。 1 2 1 2 3 1 1 3 3 Bellman-Ford：不能有负环 3f3f3f3f 最小生成树 1普利姆算法prim 稠密 朴素 n^2 稀疏 堆优化 mlogn 2克鲁斯卡尔算法 Kruskal 稀疏 mlogm 二分图 染色法 n + m 匈牙利算法 最坏 mn 一般远小于o(mn) 最小生成树：n个城市建公路 最短 二分图：当且仅当图中不奇数环（环有奇数条边） 二分图：集合集合之间有边 集合内部没有边 算法导论：算法证明 prim算法：更新每个值 先更新后面就不会更新了 最小生成树 一个连通图可能有多个生成树。当图中的边具有权值时，总会有一个生成树的边的权值之和小于或者等于其它生成树的边的权值之和 Prim算法求最小生成树 给定一个 n 个点 m 条边的无向图，图中可能存在重边和自环，边权可能为负数。 求最小生成树的树边权重之和，如果最小生成树不存在则输出 impossible。 给定一张边带权的无向图 G=(V,E)，其中 V 表示图中点的集合，E 表示图中边的集合，n=|V|，m=|E|。 由 V 中的全部 n 个顶点和 E 中 n−1 条边构成的无向连通子图被称为 G 的一棵生成树，其中边的权值之和最小的生成树被称为无向图 G 的最小生成树。 输入格式 第一行包含两个整数 n 和 m。 接下来 m 行，每行包含三个整数 u,v,w表示点 u和点 v 之间存在一条权值为 w 的边。 输出格式 共一行，若存在最小生成树，则输出一个整数，表示最小生成树的树边权重之和，如果最小生成树不存在则输出 impossible。 数据范围 1≤n≤500, 1≤m≤10^5, 图中涉及边的边权的绝对值均不超过 10000。 输入样例： 4 5 1 2 1 1 3 2 1 4 3 2 3 2 3 4 4 输出样例： 6 code o(n*n + m) 稠密图 #include #include using namespace std; const int N = 510, INF = 0x3f3f3f3f; int g[N][N], dist[N], n, m, a, b, w; bool st[N]; int prim() { int res = 0; memset(dist, 0x3f, sizeof dist); for (int i = 0; i b -> c c是b的基础来的， 所以需要更新值 for (int j = 1; j > n >> m; for (int i = 1; i > a >> b >> w; g[a][b] = g[b][a] = min(g[a][b], w); } int res = prim(); if (res == INF) puts(\"impossible\"); else cout Kruskal算法求最小生成树 给定一个 nn 个点 mm 条边的无向图，图中可能存在重边和自环，边权可能为负数。 求最小生成树的树边权重之和，如果最小生成树不存在则输出 impossible。 给定一张边带权的无向图 G=(V,E)G=(V,E)，其中 VV 表示图中点的集合，EE 表示图中边的集合，n=|V|n=|V|，m=|E|m=|E|。 由 VV 中的全部 nn 个顶点和 EE 中 n−1n−1 条边构成的无向连通子图被称为 GG 的一棵生成树，其中边的权值之和最小的生成树被称为无向图 GG 的最小生成树。 输入格式 第一行包含两个整数 nn 和 mm。 接下来 mm 行，每行包含三个整数 u,v,wu,v,w，表示点 uu 和点 vv 之间存在一条权值为 ww 的边。 输出格式 共一行，若存在最小生成树，则输出一个整数，表示最小生成树的树边权重之和，如果最小生成树不存在则输出 impossible。 数据范围 1≤n≤1051≤n≤105, 1≤m≤2∗1051≤m≤2∗105, 图中涉及边的边权的绝对值均不超过 10001000。 输入样例： 4 5 1 2 1 1 3 2 1 4 3 2 3 2 3 4 4 输出样例： 6 code mlogm 稀疏图 #include #include using namespace std; const int N = 1e5 + 10, M = 2e5 + 10, INF = 0x3f3f3f3f; int p[N], n, m, a, b, w; struct Edge { int a, b, w; bool operator> n >> m; for (int i = 0; i > a >> b >> w; e[i] = {a, b, w}; } sort(e, e + m); int res = kruskal(); if (res == INF) puts(\"impossible\"); else cout 二分图 染色法判定二分图 给定一个 n 个点 m 条边的无向图，图中可能存在重边和自环。 请你判断这个图是否是二分图。 输入格式 第一行包含两个整数 n 和 m。 接下来 m 行，每行包含两个整数 u 和 v，表示点 u 和点 v 之间存在一条边。 输出格式 如果给定图是二分图，则输出 Yes，否则输出 No。 数据范围 1≤n,m≤10^5 输入样例： 4 4 1 3 1 4 2 3 2 4 输出样例： Yes code i - j i j属于不同的集合 o(n+m) #include #include using namespace std; const int N = 1e5 + 10, M = 2 * N; int h[N], e[M], ne[M], idx; int color[N], n, m; void add(int a, int b) { e[idx] = b, ne[idx] = h[a], h[a] = idx++; } bool dfs(int u, int c) { color[u] = c; for (int i = h[u]; i != -1; i = ne[i]) { int j = e[i]; if (!color[j] && !dfs(j, 3 - c)) return false; else if (color[j] == c) return false; } return true; } int main() { cin >> n >> m; memset(h, -1, sizeof h); while (m--) { int a, b; cin >> a >> b; add(a, b), add(b, a); } bool flag = true; for (int i = 1; i 二分图最大匹配 给定一个二分图，其中左半部包含 n1 个点（编号 1∼n1），右半部包含 n2 个点（编号 1∼n2），二分图共包含 m 条边。 数据保证任意一条边的两个端点都不可能在同一部分中。 请你求出二分图的最大匹配数。 二分图的匹配：给定一个二分图 G，在 G 的一个子图 M 中，M 的边集 {E}中的任意两条边都不依附于同一个顶点，则称 M 是一个匹配。 二分图的最大匹配：所有匹配中包含边数最多的一组匹配被称为二分图的最大匹配，其边数即为最大匹配数。 输入格式 第一行包含三个整数 n1、 n2 和 m。 接下来 m 行，每行包含两个整数 u 和 v，表示左半部点集中的点 u 和右半部点集中的点 v 之间存在一条边。 输出格式 输出一个整数，表示二分图的最大匹配数。 数据范围 1≤n1,n2≤500, 1≤u≤n1, 1≤v≤n2, 1≤m≤10^5 输入样例： 2 2 4 1 1 1 2 2 1 2 2 输出样例： 2 code o(nm) #include #include #include using namespace std; const int N = 510, M = 1e5 + 10; int h[N], e[M], ne[M], idx; int match[N]; bool st[N]; int n1, n2, m; void add(int a, int b) { e[idx] = b, ne[idx] = h[a], h[a] = idx++; } bool find(int u) { for (int i = h[u]; i != -1; i = ne[i]) { int j = e[i]; if (!st[j]) { st[j] = true; if (match[j] == 0 || find(match[j])) { match[j] = u; return true; } } } return false; } int main() { cin >> n1 >> n2 >> m; memset(h, -1, sizeof h); while (m--) { int a, b; scanf(\"%d%d\", &a, &b); add(a, b); } int res = 0; for (int i = 1; i test Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-26 23:34:40 "},"10algorithm/base/4数学.html":{"url":"10algorithm/base/4数学.html","title":"数学","keywords":"","body":"质数： 大于 1 的整数中 ，如果只包含1和本身这俩个约数 质数的判定1.试除法 分解质因数 试除法 任何数都可以表示成质数的乘积。 N只会被 最小质因子 i%pj == 0 pj 是 I的最小质因子 也是pj * i的最小质因子 i%pj != 0 pj 是 pj * i的最小质因子 对于一个合数x,一定存在最小质因子 假设pj 是x的最小质因子， 当i 美剧导 x/pj 每个数都有一个最小质因子 i是 合数 最小质因子就会停下来 i是质数 pj == i 也会停下来 n + n/2 + n/3 + ... 1 1的倍数 2的倍数 3的倍数 1的约数 2的约束 nlogn 1是所有倍数的约数 int 范围内某个数1500个约数字 222 欧拉函数 1-n 中和n互质的个数 容质原理： 等价 n更好i 400w - 500w 34 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-27 21:05:39 "},"31bigdata/flink/0概述.html":{"url":"31bigdata/flink/0概述.html","title":"概述","keywords":"","body":"基本概念 是什么 Apache Flink 是一个框架和分布式处理引擎，用于对无界和有界数据流进行状态计算 特点 支持事件时间(event-time)和处理时间(processing-time) 语义 精确一次(exactly-once)的状态一致性保证 低延迟，每秒处理数百万个事件，毫秒级延迟 与众多常用存储系统的连接 高可用，动态扩展，实现7*24小时全天候运行 Flink VS Spark Streaming 数据模型 spark 采用 RDD 模型，spark streaming 的 DStream 实际上也就是一组组小批 数据 RDD 的集合 flink 基本数据模型是数据流，以及事件(Event)序列 运行时架构 spark 是批计算，将 DAG 划分为不同的 stage，一个完成后才可以计算下一个 flink 是标准的流执行模式，一个事件在一个节点处理完后可以直接发往下一个节点进行处理 迟到的数据放入侧边流 8-9 数据产生8-9 处理的时候9:01 85763421 俩个通可以同时存在 [1,5] 1 2 4 3 5 [6,10] 6 7 8 t=3 8 来了关【1-5】的窗 wm传递：最小的值 在并行度不为1的时候： 下以上游分区woater mark最小值 状态保存在本地内存中， 检查点 taskmanager state.backend: filesystem 检查点 a b 时间还没到检查点a, 正在处理1 1 2会被丢弃 黄1 蓝1 1 2 3 4 保存1 5 6 // 前一次保存结束 到下一次保存开始 setMinPauseBetweenCheckpoints 可能检查点到了但是没超过这个数字 也不保存 端到端：输入源头 flink 幂等写入：最终一致性 5 10 15 5 10 15 5保存 预写日志(WAL):往外部系统写入并不能保证事务 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-19 17:36:35 "},"31bigdata/flink/1快速上手.html":{"url":"31bigdata/flink/1快速上手.html","title":"快速上手","keywords":"","body":"快速开始 2.1 pom.xml 4.0.0 com.matt study-flink 1.0-SNAPSHOT 8 8 org.apache.flink flink-java 1.10.1 org.apache.flink flink-streaming-java_2.12 1.10.1 org.apache.flink flink-connector-kafka-0.11_2.12 1.10.1 org.apache.bahir flink-connector-redis_2.11 1.0 org.apache.flink flink-connector-elasticsearch6_2.12 1.10.1 mysql mysql-connector-java 5.1.44 org.apache.flink flink-statebackend-rocksdb_2.12 1.10.1 2.2批处理wordcount com.matt.wc.WordCount 输出结构 (are,1) (tks,1) (how,1) (fink,1) (spark,1) (you,1) (matt,1) (hello,3) 2.3 流处理 com.matt.wc.StreamWordCount 部署 mac-Standalone 模式 修改文件所有者 # 查看当前用户 whoami # 查看用户组 id matt # 用户名：组名 chown -R matt:staff /opt/software chown -R matt:staff /opt/module 查看更多用户相关信息 https://blog.csdn.net/qq_26129413/article/details/109675386 解压 tar -zxvf flink-1.10.1-bin-scala_2.12.tgz -C /opt/module 配置 修改 flink/conf/flink-conf.yaml 文件 如果是单机安装默认即可，集群安装配置某台主机 jobmanager.rpc.address: localhost 修改 /conf/slaves 文件 vim slaves 从机机器列表 localhost 如果是集群安装可以配置为 matt06 matt07 如果是集群安装需要把flink-1.10.1同步到其他机器 启动 start-cluster.sh stop-cluster.sh 提交任务 可以通过命令行提交也可以ui进行提交 ./flink run -c com.matt.wc.StreamWordCount –p 2 FlinkTutorial-1.0-SNAPSHOT-jar-with-dependencies.jar --host lcoalhost –port 777 http://localhost:8081/#/overview TODO yarn/k8s部署 运行架构 Flink运行时的组件 作业管理器(JobManager） 控制一个应用程序执行的主进程，也就是说，每个应用程序都会被一个不同的 JobManager 所控制执行。 JobManager 会先接收到要执行的应用程序，这个应用程序会包括:作业图 (JobGraph)、逻辑数据流图(logical dataflow graph)和打包了所有的类、 库和其它资源的JAR包。 JobManager 会把JobGraph转换成一个物理层面的数据流图，这个图被叫做 “执行图”(ExecutionGraph)，包含了所有可以并发执行的任务。 JobManager 会向资源管理器(ResourceManager)请求执行任务必要的资源， 也就是任务管理器(TaskManager)上的插槽(slot)。一旦它获取到了足够的 资源，就会将执行图分发到真正运行它们的TaskManager上。而在运行过程中， JobManager会负责所有需要中央协调的操作，比如说检查点(checkpoints) 的协调。 任务管理器(TaskManager） Flink中的工作进程。通常在Flink中会有多个TaskManager运行，每一 个TaskManager都包含了一定数量的插槽（slots）。插槽的数量限制 了TaskManager能够执行的任务数量。 启动之后，TaskManager会向资源管理器注册它的插槽；收到资源管理 器的指令后，TaskManager就会将一个或者多个插槽提供给 JobManager调用。JobManager就可以向插槽分配任务（tasks）来 执行了。 在执行过程中，一个TaskManager可以跟其它运行同一应用程序的 TaskManager交换数据。 资源管理器（ResourceManager） ​ 主要负责管理任务管理器（TaskManager）的插槽（slot），TaskManger 插槽是 Flink 中 定义的处理资源单元。Flink 为不同的环境和资源管理工具提供了不同资源管理器，比如 YARN、Mesos、K8s，以及 standalone 部署。当 JobManager 申请插槽资源时，ResourceManager 会将有空闲插槽的 TaskManager 分配给 JobManager。如果 ResourceManager 没有足够的插槽 来满足 JobManager 的请求，它还可以向资源提供平台发起会话，以提供启动 TaskManager 进程的容器。另外，ResourceManager 还负责终止空闲的 TaskManager，释放计算资源。 分发器（Dispatcher） 可以跨作业运行，它为应用提交提供了REST接口。 当一个应用被提交执行时，分发器就会启动并将应用移交给一个 JobManager。 Dispatcher也会启动一个Web UI，用来方便地展示和监控作业 执行的信息。 Dispatcher在架构中可能并不是必需的，这取决于应用提交运行 的方式。 任务提交流程 任务提交流程（yarn）-TODO 任务调度原理 ​ 客户端不是运行时和程序执行 的一部分，但它用于准备并发送 dataflow(JobGraph)给 Master(JobManager)，然后，客户端断开连接或者维持连接以 等待接收计算结果。 ​ 当 Flink 集 群 启 动 后 ， 首 先 会 启 动 一 个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。 TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。 ​ Client 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境 连通即可）。提交 Job 后，Client 可以结束进程（Streaming 的任务），也可以不 结束并等待结果返回。 ​ JobManager 主 要 负 责 调 度 Job 并 协 调 Task 做 checkpoint， 职 责 上 很 像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的 执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。 ​ TaskManager 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自 己的上游建立 Netty 连接，接收数据并处理。 并行度 一个特定算子的 子任务（subtask）的个数被称之为其并行度（parallelism）。 一般情况下，一个 stream 的并行度，可以认为就是其所有算子中最大的并行度。 TaskManager 和 Slots ​ Flink 中每一个 worker(TaskManager)都是一个 JVM 进程，它可能会在独立的线 程上执行一个或多个 subtask。为了控制一个 worker 能接收多少个 task，worker 通 过 task slot 来进行控制（一个 worker 至少有一个 task slot）。 ​ 每个 task slot 表示 TaskManager 拥有资源的一个固定大小的子集。假如一个 TaskManager 有三个 slot，那么它会将其管理的内存分成三份给各个 slot。资源 slot 化意味着一个 subtask 将不需要跟来自其他 job 的 subtask 竞争被管理的内存，取而 代之的是它将拥有一定数量的内存储备。需要注意的是，这里不会涉及到 CPU 的隔 离，slot 目前仅仅用来隔离 task 的受管理的内存。 ​ ​ 通过调整 task slot 的数量，允许用户定义 subtask 之间如何互相隔离。如果一个 TaskManager 一个 slot，那将意味着每个 task group 运行在独立的 JVM 中（该 JVM 可能是通过一个特定的容器启动的），而一个 TaskManager 多个 slot 意味着更多的 subtask 可以共享同一个 JVM。而在同一个 JVM 进程中的 task 将共享 TCP 连接（基 于多路复用）和心跳消息。它们也可能共享数据集和数据结构，因此这减少了每个 task 的负载。 Flink 中每一个 TaskManager 都是一个JVM进程，它可能会在独立的线程上执 行一个或多个子任务 为了控制一个 TaskManager 能接收多少个 task， TaskManager 通过 task slot 来进行控制（一个 TaskManager 至少有一个 slot） ​ 默认情况下，Flink 允许子任务共享 slot，即使它们是不同任务的子任务（前提 是它们来自同一个 job）。 这样的结果是，一个 slot 可以保存作业的整个管道。 ​ Task Slot 是静态的概念，是指 TaskManager 具有的并发执行能力，可以通过 参数 taskmanager.numberOfTaskSlots 进行配置；而并行度 parallelism 是动态概念， 即 TaskManager 运行程序时实际使用的并发能力，可以通过参数 parallelism.default 进行配置。 ​ 也就是说，假设一共有 3 个 TaskManager，每一个 TaskManager 中的分配 3 个 TaskSlot，也就是每个 TaskManager 可以接收 3 个 task，一共 9 个 TaskSlot，如果我 们设置 parallelism.default=1，即运行程序默认的并行度为 1，9 个 TaskSlot 只用了 1 个，有 8 个空闲，因此，设置合适的并行度才能提高效率。 并行子任务的分配 程序与数据流（DataFlow） 所有的Flink程序都是由三部分组成的： Source 、Transformation 和 Sink。Source 负责读取数据源，Transformation 利用各种算子进行处理加工，Sink负责输出 在运行时，Flink上运行的程序会被映射成“逻辑数据流”（dataflows），它包 含了这三部分 每一个dataflow以一个或多个sources开始以一个或多个sinks结束。dataflow 类似于任意的有向无环图（DAG） 在大部分情况下，程序中的转换运算（transformations）跟dataflow中的算子 （operator）是一一对应的关系 执行图（ExecutionGraph） Flink 中的执行图可以分成四层：StreamGraph -> JobGraph -> ExecutionGraph -> 物理执行图 ➢ StreamGraph：是根据用户通过 Stream API 编写的代码生成的最初的图。用来 表示程序的拓扑结构。 ➢ JobGraph：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点 ➢ ExecutionGraph：JobManager 根据 JobGraph 生成ExecutionGraph。 ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。 ➢ 物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个 TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。 数据传输形式 一个程序中，不同的算子可能具有不同的并行度 算子之间传输数据的形式可以是 one-to-one (forwarding) 的模式也可以是 redistributing 的模式，具体是哪一种形式，取决于算子的种类 ➢ One-to-one：stream维护着分区以及元素的顺序（比如source和map之间）。 这意味着map 算子的子任务看到的元素的个数以及顺序跟 source 算子的子任务 生产的元素的个数、顺序相同。map、fliter、flatMap等算子都是one-to-one 的对应关系。 ➢ Redistributing：stream的分区会发生改变。每一个算子的子任务依据所选择的 transformation发送数据到不同的目标任务。例如，keyBy 基于 hashCode 重 分区、而 broadcast 和 rebalance 会随机重新分区，这些算子都会引起 redistribute过程，而 redistribute 过程就类似于 Spark 中的 shuffle 过程。 任务链（Operator Chains） Flink 采用了一种称为任务链的优化技术，可以在特定条件下减少本地通信的开销。为了满足任务链的要求，必须将两个或多个算子设为相同 的并行度，并通过本地转发（local forward）的方式进行连接 相同并行度的 one-to-one 操作，Flink 这样相连的算子链接在一起形 成一个 task，原来的算子成为里面的 subtask 并行度相同、并且是 one-to-one 操作，两个条件缺一不可 好处 它能减少线 程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量。链接的行为可以在编程 API 中进行指定。 //基于数据流进行转换计算 DataStream> resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1).setParallelism(2).startNewChain(); // 和前后都不合并任务 // .disableChaining(); // 开始一个新的任务链合并 前面断开后面不断开 // .startNewChain() Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-19 17:36:42 "},"31bigdata/flink/2API.html":{"url":"31bigdata/flink/2API.html","title":"api","keywords":"","body":"API Environment getExecutionEnvironment 这个比较常用 ​ 创建一个执行环境，表示当前执行程序的上下文。 如果程序是独立调用的，则 此方法返回本地执行环境；如果从命令行客户端调用程序以提交到集群，则此方法 返回此集群的执行环境，也就是说，getExecutionEnvironment 会根据查询运行的方 式决定返回什么样的运行环境，是最常用的一种创建执行环境的方式。 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); createLocalEnvironment 返回本地执行环境，需要在调用时指定默认的并行度。 LocalStreamEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(1); createRemoteEnvironment 返回集群执行环境，将 Jar 提交到远程服务器。需要在调用时指定 JobManager 的 IP 和端口号，并指定要在集群中运行的 Jar 包。 StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(\"jobmanage-hostname\", 6123, \"YOURPATH//WordCount.jar\"); source 从集合读取数据 com.matt.apitest.source package com.matt.apitest.source; import com.matt.apitest.beans.SensorReading; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import java.util.Arrays; /** * @author matt * @create 2022-01-16 14:36 */ public class SourceTest1_Collection { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); DataStream dataStream = env.fromCollection(Arrays.asList( new SensorReading(\"sensor_1\", 1547718199L, 35.8), new SensorReading(\"sensor_6\", 1547718201L, 15.4), new SensorReading(\"sensor_7\", 1547718202L, 6.7), new SensorReading(\"sensor_10\", 1547718205L, 38.1) )); dataStream.print(\"collection\"); // job name env.execute(\"my\"); } } 从文件读取数据 package com.matt.apitest.source; import com.matt.apitest.beans.SensorReading; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import java.util.Arrays; /** * @author matt * @create 2022-01-16 14:54 */ public class SourceTest2_File { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //env.setParallelism(1); // /Users/matt/workspace/java/bigdata DataStream dataStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); dataStream.print(\"file\"); // job name env.execute(\"my\"); } } 以 kafka 消息队列的数据作为来源 org.apache.flink flink-connector-kafka-0.11_2.12 1.10.1 package com.matt.apitest.source; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.serialization.SimpleStringSchema; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011; import java.util.Arrays; import java.util.Properties; /** * @author matt * @create 2022-01-16 15:05 */ public class SourceTest3_Kafka { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // Properties properties = new Properties(); properties.setProperty(\"bootstrap.servers\", \"matt05:9092\"); properties.setProperty(\"group.id\", \"consumer-group\"); properties.setProperty(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); properties.setProperty(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); properties.setProperty(\"auto.offset.reset\", \"latest\"); DataStream dataStream = env.addSource( new FlinkKafkaConsumer011(\"sensor\", new SimpleStringSchema(), properties)); dataStream.print(\"kafka\"); // job name env.execute(\"kafak_job\"); } } 自定义 Source 实现SourceFunction接口 package com.matt.apitest.source; import com.matt.apitest.beans.SensorReading; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import java.util.HashMap; import java.util.Properties; import java.util.Random; /** * @author matt * @create 2022-01-16 15:50 */ public class SourceTest4_UDF { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); DataStream dataStream = env.addSource(new MySensorSouce()); dataStream.print(\"kafka\"); // job name env.execute(\"kafak_job\"); } public static class MySensorSouce implements SourceFunction { // 标志位 控制数据的产生 private boolean running = true; /** * 功能： * * @param ctx * @return void * @author matt * @date 2022/1/16 */ @Override public void run(SourceContext ctx) throws Exception { // 随机数发生器 Random random = new Random(); HashMap sensorTempMap = new HashMap(); for (int i = 0; i Transform map flatmap filter package com.matt.apitest.transform; import org.apache.flink.api.common.functions.FilterFunction; import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.util.Collector; import java.io.File; /** * @author matt * @create 2022-01-16 17:16 */ public class TransFormTest1_Base { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); // 1 map string -> len(string) // 方-》园 DataStream mapStream = inputStream.map(new MapFunction() { @Override public Integer map(String s) throws Exception { return s.length(); } }); // mapStream.print(\"map\"); // flatMap 按逗号切分字端 DataStream flatMapStream = inputStream.flatMap(new FlatMapFunction() { @Override public void flatMap(String s, Collector collector) throws Exception { String[] fields = s.split(\",\"); for (String field : fields) { collector.collect(field); } } }); // 3.filter 过滤 筛选某个数据 DataStream filterStream = inputStream.filter(new FilterFunction() { @Override public boolean filter(String s) throws Exception { // true 要 false 不要这个数据 return s.startsWith(\"sensor_1\"); } }); mapStream.print(\"map\"); flatMapStream.print(\"flatMap\"); filterStream.print(\"filter\"); // job name env.execute(\"trans-form\"); } } keyedBy DataStream → KeyedStream：逻辑地将一个流拆分成不相交的分区，每个分 区包含具有相同 key 的元素，在内部以 hash 的形式实现的。 package com.matt.apitest.transform; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.functions.FilterFunction; import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.KeyedStream; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.util.Collector; /** * @author matt * @create 2022-01-17 22:14 */ public class TransFormTest1_RollingAggregation { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); // SensorReading //DataStream dataStream = inputStream.map(new MapFunction() { // // @Override // public SensorReading map(String value) throws Exception { // String[] fields = value.split(\",\"); // return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); // } //}); DataStream dataStream = inputStream.map( s -> { String[] fields = s.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); //分组 KeyedStream keyedStream = dataStream.keyBy(\"id\"); keyedStream.print(\"keyed\"); // job name env.execute(\"trans-form\"); } } 滚动聚合算子（Rolling Aggregation） 这些算子可以针对 KeyedStream 的每一个支流做聚合。 sum() min() max() minBy() maxBy() max只更新统计的字段 maxBy:非max字段使用最大的值字段那条记录的字段 package com.matt.apitest.transform; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.functions.FilterFunction; import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.KeyedStream; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.util.Collector; /** * @author matt * @create 2022-01-17 22:14 */ public class TransFormTest1_RollingAggregation { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream dataStream = inputStream.map( s -> { String[] fields = s.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); //分组 KeyedStream keyedStream = dataStream.keyBy(\"id\"); // KeyedStream keyedStream1 = dataStream.keyBy(d -> d.getId()); // 滚动聚合 // max 当前的字段 maxBy // maxBy 最大的那条记录 没有她大 非max字段也要改 // max 最大值那条字段 // DataStream resultStream = keyedStream.max(\"temperatrue\"); //resultStream.print(); keyedStream.print(\"keyed\"); // job name env.execute(\"trans-form\"); } } reduce KeyedStream → DataStream：一个分组数据流的聚合操作，合并当前的元素 和上次聚合的结果，产生一个新的值，返回的流中包含每一次聚合的结果，而不是 只返回最后一次聚合的最终结果 package com.matt.apitest.transform; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.functions.ReduceFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.KeyedStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; /** * @author matt * @create 2022-01-17 22:48 */ public class TransFormTest3 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream dataStream = inputStream.map(s -> { String[] fields = s.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); //分组 KeyedStream keyedStream = dataStream.keyBy(\"id\"); DataStream resultStream = keyedStream.reduce(new ReduceFunction() { @Override public SensorReading reduce(SensorReading v1, SensorReading v2) throws Exception { return new SensorReading(v1.getId(), v2.getTimestamp(), Math.max(v1.getTemperatrue(), v2.getTemperatrue())); } }); resultStream.print(); // job name env.execute(\"trans-form\"); } } split select DataStream → SplitStream：根据某些特征把一个 DataStream 拆分成两个或者 多个 DataStream。 SplitStream→DataStream：从一个 SplitStream 中获取一个或者多个 DataStream。 需求：传感器数据按照温度高低（以 30 度为界），拆分成两个流。 拆分 选择 package com.matt.apitest.transform; import com.matt.apitest.beans.SensorReading; import org.apache.commons.math3.geometry.partitioning.SubHyperplane; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.common.functions.ReduceFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.streaming.api.collector.selector.OutputSelector; import org.apache.flink.streaming.api.datastream.*; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.co.CoMapFunction; import java.util.Collections; /** * @author matt * @create 2022-01-17 23:30 */ public class TransFromTest4_MultiplStreams { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream inputStream = env.readTextFile(\"D:\\\\matt\\\\workspace\\\\idea\\\\hadoop\\\\study-flink\\\\src\\\\main\\\\resources\\\\sensor.txt\"); DataStream dataStream = inputStream.map(s -> { String[] fields = s.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); //1. 分流 SplitStream splitStream = dataStream.split(new OutputSelector() { @Override public Iterable select(SensorReading sensorReading) { return (sensorReading.getTemperatrue() > 30) ? Collections.singletonList(\"high\") : Collections.singletonList(\"low\") ; } }); DataStream highTempStream = splitStream.select(\"high\"); DataStream lowTempStream = splitStream.select(\"low\"); highTempStream.print(); System.out.println(\"-------\"); lowTempStream.print(); // job name env.execute(\"trans-form\"); } } Connect 和 CoMap DataStream,DataStream → ConnectedStreams：连接两个保持他们类型的数 据流，两个数据流被 Connect 之后，只是被放在了一个同一个流中，内部依然保持 各自的数据和形式不发生任何变化，两个流相互独立。 ConnectedStreams → DataStream：作用于 ConnectedStreams 上，功能与 map 和 flatMap 一样，对 ConnectedStreams 中的每一个 Stream 分别进行 map 和 flatMap 处理。 connetc 将俩个流放到一个流中 package com.matt.apitest.transform; import com.matt.apitest.beans.SensorReading; import org.apache.commons.math3.geometry.partitioning.SubHyperplane; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.common.functions.ReduceFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.streaming.api.collector.selector.OutputSelector; import org.apache.flink.streaming.api.datastream.*; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.co.CoMapFunction; import java.util.Collections; /** * @author matt * @create 2022-01-17 23:30 */ public class TransFromTest4_MultiplStreams { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream dataStream = inputStream.map(s -> { String[] fields = s.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); //1. 分流 SplitStream splitStream = dataStream.split(new OutputSelector() { @Override public Iterable select(SensorReading sensorReading) { return (sensorReading.getTemperatrue() > 30) ? Collections.singletonList(\"high\") : Collections.singletonList(\"low\") ; } }); DataStream highTempStream = splitStream.select(\"high\"); DataStream lowTempStream = splitStream.select(\"low\"); highTempStream.print(); System.out.println(\"-------\"); lowTempStream.print(); System.out.println(\"河流\"); DataStream> warningStream = highTempStream.map(new MapFunction>() { @Override public Tuple2 map(SensorReading v) throws Exception { return new Tuple2<>(v.getId(), v.getTemperatrue()); } }); ConnectedStreams, SensorReading> connectedStreams = warningStream.connect(lowTempStream); SingleOutputStreamOperator res = connectedStreams.map(new CoMapFunction, SensorReading, Object>() { @Override public Object map1(Tuple2 v) throws Exception { return new Tuple3<>(v.f0, v.f1, \"high temp warning\"); } @Override public Object map2(SensorReading v) throws Exception { return new Tuple2<>(v.getId(), \"normal\"); } }); res.print(); System.out.println(\"union\"); DataStream union = highTempStream.union(lowTempStream); union.print(); // job name env.execute(\"trans-form\"); } } Union DataStream → DataStream：对两个或者两个以上的 DataStream 进行 union 操 作，产生一个包含所有 DataStream 元素的新 DataStream。 Connect 与 Union 区别 Union 之前两个流的类型必须是一样，Connect 可以不一样，在之后的 coMap 中再去调整成为一样的。 Connect 只能操作两个流，Union 可以操作多个。 package com.matt.apitest.transform; import com.matt.apitest.beans.SensorReading; import org.apache.commons.math3.geometry.partitioning.SubHyperplane; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.common.functions.ReduceFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.streaming.api.collector.selector.OutputSelector; import org.apache.flink.streaming.api.datastream.*; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.co.CoMapFunction; import java.util.Collections; /** * @author matt * @create 2022-01-17 23:30 */ public class TransFromTest4_MultiplStreams { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream dataStream = inputStream.map(s -> { String[] fields = s.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); //1. 分流 SplitStream splitStream = dataStream.split(new OutputSelector() { @Override public Iterable select(SensorReading sensorReading) { return (sensorReading.getTemperatrue() > 30) ? Collections.singletonList(\"high\") : Collections.singletonList(\"low\") ; } }); DataStream highTempStream = splitStream.select(\"high\"); DataStream lowTempStream = splitStream.select(\"low\"); highTempStream.print(); System.out.println(\"-------\"); lowTempStream.print(); System.out.println(\"河流\"); DataStream> warningStream = highTempStream.map(new MapFunction>() { @Override public Tuple2 map(SensorReading v) throws Exception { return new Tuple2<>(v.getId(), v.getTemperatrue()); } }); ConnectedStreams, SensorReading> connectedStreams = warningStream.connect(lowTempStream); SingleOutputStreamOperator res = connectedStreams.map(new CoMapFunction, SensorReading, Object>() { @Override public Object map1(Tuple2 v) throws Exception { return new Tuple3<>(v.f0, v.f1, \"high temp warning\"); } @Override public Object map2(SensorReading v) throws Exception { return new Tuple2<>(v.getId(), \"normal\"); } }); res.print(); System.out.println(\"union\"); DataStream union = highTempStream.union(lowTempStream); union.print(); // job name env.execute(\"trans-form\"); } } 数据类型 Flink 支持所有的 Java 和 Scala 基础数据类型，Int, Double, Long, String, … DataStream numberStream = env.fromElements(1, 2, 3, 4); numberStream.map(data -> data * 2); Java 和 Scala 元组（Tuples） DataStream> personStream = env.fromElements( new Tuple2(\"Adam\", 17), new Tuple2(\"Sarah\", 23) ); personStream.filter(p -> p.f1 > 18); Scala 样例类（case classes） case class Person(name: String, age: Int) val persons: DataStream[Person] = env.fromElements( Person(\"Adam\", 17), Person(\"Sarah\", 23) ) persons.filter(p => p.age > 18) Java 简单对象（POJOs） public class Person { public String name; public int age; public Person() {} public Person(String name, int age) { this.name = name; this.age = age; } } DataStream persons = env.fromElements( new Person(\"Alex\", 42), new Person(\"Wendy\", 23)); 其它（Arrays, Lists, Maps, Enums, 等等） Flink 对 Java 和 Scala 中的一些特殊目的的类型也都是支持的，比如 Java 的 ArrayList，HashMap，Enum 等等。 实现 UDF 函数 一个实现接口的类或者是匿名内部类 或者是lambda函数 Rich Functions “富函数”是 DataStream API 提供的一个函数类的接口，所有 Flink 函数类都 有其 Rich 版本。它与常规函数的不同在于，可以获取运行环境的上下文，并拥有一 些生命周期方法，所以可以实现更复杂的功能。 Rich Function 有一个生命周期的概念。典型的生命周期方法有： ⚫ open()方法是 rich function 的初始化方法，当一个算子例如 map 或者 filter 被调用之前 open()会被调用。 ⚫ close()方法是生命周期中的最后一个调用的方法，做一些清理工作。 ⚫ getRuntimeContext()方法提供了函数的 RuntimeContext 的一些信息，例如函数执行的并行度，任务的名字，以及 state 状态 package com.matt.apitest.transform; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.common.functions.RichMapFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.collector.selector.OutputSelector; import org.apache.flink.streaming.api.datastream.ConnectedStreams; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.datastream.SplitStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.co.CoMapFunction; import scala.Enumeration; import java.util.Collections; /** * @author matt * @create 2022-01-24 23:55 */ public class TransfromTest5_RichFunction { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream dataStream = inputStream.map(s -> { String[] fields = s.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); DataStream> resStream = dataStream.map( new MyMapper() ); resStream.print(); // job name env.execute(\"trans-form\"); } public static class MyMapper extends RichMapFunction> { @Override public Tuple2 map(SensorReading v) throws Exception { return new Tuple2<>(v.getId(), getRuntimeContext().getIndexOfThisSubtask()); } public MyMapper() { super(); } @Override public void open(Configuration parameters) throws Exception { System.out.println(\"init....\"); } @Override public void close() throws Exception { System.out.println(\"clear...\"); } } } sink kafka org.apache.flink flink-connector-kafka-0.11_2.12 1.10.1 package com.matt.apitest.sink; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.serialization.SimpleStringSchema; import org.apache.flink.streaming.api.SimpleTimerService; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011; import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011; /** * @author matt * @create 2022-01-25 23:33 */ public class SinkTest1_kafka { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //env.setParallelism(1); DataStream dataStream = env.readTextFile(\"D:\\\\matt\\\\workspace\\\\idea\\\\hadoop\\\\study-flink\\\\src\\\\main\\\\resources\\\\sensor.txt\"); dataStream.print(\"file\"); DataStream resStream = dataStream.map(line -> { String[] f = line.split(\",\"); return new SensorReading(f[0], new Long(f[1]), new Double(f[2])).toString(); }); resStream.print(); resStream.addSink(new FlinkKafkaProducer011(\"matt05:9092\", \"test\", new SimpleStringSchema())); // job name env.execute(\"my\"); } } redis org.apache.bahir flink-connector-redis_2.11 1.0 package com.matt.apitest.sink; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.serialization.SimpleStringSchema; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011; import org.apache.flink.streaming.connectors.redis.RedisSink; import org.apache.flink.streaming.connectors.redis.common.config.FlinkJedisPoolConfig; import org.apache.flink.streaming.connectors.redis.common.mapper.RedisCommand; import org.apache.flink.streaming.connectors.redis.common.mapper.RedisCommandDescription; import org.apache.flink.streaming.connectors.redis.common.mapper.RedisMapper; /** * @author matt * @create 2022-01-25 23:57 */ public class SinkTest2_Redis { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream dataStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); dataStream.print(\"file\"); DataStream resStream = dataStream.map(line -> { String[] f = line.split(\",\"); return new SensorReading(f[0], new Long(f[1]), new Double(f[2])); }); resStream.print(); FlinkJedisPoolConfig config = new FlinkJedisPoolConfig.Builder() .setHost(\"matt05\") .setPort(6379) .build(); resStream.addSink(new RedisSink<>(config, new MyRedisMapper())); // job name env.execute(\"my\"); } public static class MyRedisMapper implements RedisMapper { // 保存到 redis 的命令，存成哈希表 public RedisCommandDescription getCommandDescription() { return new RedisCommandDescription(RedisCommand.HSET, \"sensor_tempe\"); } // key public String getKeyFromData(SensorReading data) { return data.getId(); } // v public String getValueFromData(SensorReading data) { return data.getTemperatrue().toString(); } } } es org.apache.flink flink-connector-elasticsearch6_2.12 1.10.1 package com.matt.apitest.sink; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.functions.RuntimeContext; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction; import org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer; import org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink; import org.apache.flink.streaming.connectors.redis.RedisSink; import org.apache.flink.streaming.connectors.redis.common.config.FlinkJedisPoolConfig; import org.apache.http.HttpHost; import org.elasticsearch.action.index.IndexRequest; import org.elasticsearch.client.Requests; import java.util.ArrayList; import java.util.HashMap; /** * @author matt * @create 2022-01-26 0:21 */ public class SinkTest3_ES { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream dataStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); dataStream.print(\"file\"); DataStream resStream = dataStream.map(line -> { String[] f = line.split(\",\"); return new SensorReading(f[0], new Long(f[1]), new Double(f[2])); }); resStream.print(); ArrayList httpHosts = new ArrayList<>(); httpHosts.add(new HttpHost(\"localhost\", 9200)); resStream.addSink(new ElasticsearchSink.Builder(httpHosts, new MyEsSinkFunction()).build()); // job name env.execute(\"my\"); } public static class MyEsSinkFunction implements ElasticsearchSinkFunction { @Override public void process(SensorReading element, RuntimeContext ctx, RequestIndexer indexer) { HashMap dataSource = new HashMap<>(); dataSource.put(\"id\", element.getId()); dataSource.put(\"ts\", String.valueOf(element.getTimestamp())); dataSource.put(\"temp\", element.getTemperatrue().toString()); IndexRequest indexRequest = Requests.indexRequest() .index(\"sensor\") .type(\"readingData\") .source(dataSource); indexer.add(indexRequest); } } } jdbc mysql mysql-connector-java 5.1.44 package com.matt.apitest.sink; import com.matt.apitest.beans.SensorReading; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.sink.RichSinkFunction; import java.sql.DriverManager; import java.sql.Connection; import java.sql.PreparedStatement; /** * @author matt * @create 2022-01-26 1:41 */ public class SinkTest4_MySQL { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream dataStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream resStream = dataStream.map(line -> { String[] f = line.split(\",\"); return new SensorReading(f[0], new Long(f[1]), new Double(f[2])); }); resStream.print(); resStream.addSink(new MyJdbcSink()); // job name env.execute(); } public static class MyJdbcSink extends RichSinkFunction { Connection conn = null; PreparedStatement insertStmt = null; PreparedStatement updateStmt = null; // open 主要是创建连接 @Override public void open(Configuration parameters) throws Exception { conn = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/stu_flink\", \"root\", \"root\"); // 创建预编译器，有占位符，可传入参数 insertStmt = conn.prepareStatement(\"INSERT INTO sensor_temp (id, temp) VALUES ( ?, ?)\"); updateStmt = conn.prepareStatement(\"UPDATE sensor_temp SET temp = ? WHERE id = ? \"); } // 调用连接，执行 sql @Override public void invoke(SensorReading value, Context context) throws Exception { // 执行更新语句，注意不要留 super updateStmt.setDouble(1, value.getTemperatrue()); updateStmt.setString(2, value.getId()); updateStmt.execute(); // 如果刚才 update 语句没有更新，那么插入 if (updateStmt.getUpdateCount() == 0) { insertStmt.setString(1, value.getId()); insertStmt.setDouble(2, value.getTemperatrue()); insertStmt.execute(); } } @Override public void close() throws Exception { insertStmt.close(); updateStmt.close(); conn.close(); } } } Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-19 17:36:51 "},"31bigdata/flink/4状态编程和容错机制.html":{"url":"31bigdata/flink/4状态编程和容错机制.html","title":"状态编程和容错机制","keywords":"","body":"状态编程和容错机制 举例 所有类型的窗口。例如，计算过去一小时的平均温度，就是有状态的计算。 所有用于复杂事件处理的状态机。例例如，若在一分钟内收到两个相差 20 度 以上的温度读数，则发出警告，这是有状态的计算。 流与流之间的所有关联操作，以及流与静态表或动态表之间的关联操作， 都是有状态的计算。 下图展示了无状态流处理和有状态流处理的主要区别。无状态流处理分别接收 每条数据记录(图中的黑条)，然后根据最新输入的数据生成输出数据(白条)。有状态 流处理会维护状态(根据每条输入记录进行更新)，并基于最新输入的记录和当前的状态值生成输出记录(灰条)。 有状态的算子和应用程序 Flink 内置的很多算子，数据源 source，数据存储 sink 都是有状态的，流中的数 据都是 buffer records，会保存一定的元素或者元数据。例如: ProcessWindowFunction 会缓存输入流的数据，ProcessFunction 会保存设置的定时器信息等等。 在 Flink 中，状态始终与特定算子相关联。 总的来说，有两种类型的状态： ⚫ 算子状态（operator state） ⚫ 键控状态（keyed state） 算子状态 算子状态的作用范围限定为算子任务。这意味着由同一并行任务所处理的所有 数据都可以访问到相同的状态，状态对于同一任务而言是共享的。算子状态不能由 相同或不同算子的另一个任务访问。 Flink 为算子状态提供三种基本数据结构： 列表状态（List state） 将状态表示为一组数据的列表。 联合列表状态（Union list state） 也将状态表示为数据的列表。它与常规列表状态的区别在于，在发生故障时，或者从保存点（savepoint）启动应用程序时如何恢复。 广播状态（Broadcast state） 如果一个算子有多项任务，而它的每项任务状态又都相同，那么这种特殊情况最适合应 用广播状态。 键控状态 具有相同 key 的所有数据都会访问相同的状态。 Keyed State 很类似于 一个分布式的 key-value map 数据结构，只能用于 KeyedStream（keyBy 算子处理之后）。 Flink 的 Keyed State 支持以下数据类型 ValueState保存单个的值，值的类型为 T。 get 操作: ValueState.value() set 操作: ValueState.update(T value) ListState保存一个列表，列表里的元素的数据类型为 T。基本操作如下： ListState.add(T value) ListState.addAll(List values) ListState.get()返回 Iterable ListState.update(List values) MapState保存 Key-Value 对 MapState.get(UK key) MapState.put(UK key, UV value) MapState.contains(UK key) MapState.remove(UK key) ReducingState AggregatingState State.clear()是清空操作。 package com.matt.apitest.state; import com.matt.apitest.beans.SensorReading; import org.apache.commons.digester.SetNestedPropertiesRule; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.common.functions.RichMapFunction; import org.apache.flink.api.common.state.*; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.checkpoint.ListCheckpointed; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.hadoop.util.hash.Hash; import sun.management.Sensor; import java.util.Collections; import java.util.HashMap; import java.util.List; public class StateTest2_KeyedState { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream inputStream = env.socketTextStream(\"localhost\", 777); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); SingleOutputStreamOperator res = dataStream.keyBy(\"id\").map(new MyKeyCountMapper()); res.print(); env.execute(\"my\"); } public static class MyKeyCountMapper extends RichMapFunction{ private ValueState keyCountState; private ListState listState; private MapState mapState; private ReducingState readingReducingState; @Override public void open(Configuration parameters) throws Exception { keyCountState = getRuntimeContext().getState( new ValueStateDescriptor(\"keyedCount\", Integer.class,0) ); // name:不能相同 listState = getRuntimeContext().getListState( new ListStateDescriptor(\"listState\", String.class) ); mapState = getRuntimeContext().getMapState( new MapStateDescriptor(\"mapState\", String.class, String.class) ); //readingReducingState = getRuntimeContext().getReducingState( // new ReducingStateDescriptor(\"reducingState\", new ) //); } @Override public Integer map(SensorReading sensorReading) throws Exception { Integer count = keyCountState.value(); count++; keyCountState.update(count); Iterable strings = listState.get(); for (String string : strings) { System.out.println(string) ; } listState.add(\"hello\"); // mapState // readingReducingState add 方法直接聚合 return count; } } } 案例 检测传感器的温度值，如果连 续的两个温度差值超过 10 度，就输出报警。 package com.matt.apitest.state; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.functions.RichFlatMapFunction; import org.apache.flink.api.common.state.*; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.util.Collector; public class StateTest3_keyedstate_app1 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream inputStream = env.socketTextStream(\"localhost\", 777); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); // warn SingleOutputStreamOperator> res = dataStream.keyBy(\"id\") .flatMap(new TempChangeWarning(10.0)); res.print(); env.execute(\"my\"); } public static class TempChangeWarning extends RichFlatMapFunction> { private Double threshold = 10.0; // 状态 上一次温度值 private ValueState lastTempState; public TempChangeWarning(Double threshold) { this.threshold = threshold; } @Override public void open(Configuration parameters) throws Exception { lastTempState = getRuntimeContext().getState( new ValueStateDescriptor(\"last-temp\", Double.class) ); } @Override public void flatMap(SensorReading sensorReading, Collector> collector) throws Exception { Double lastTemp = lastTempState.value(); if (lastTemp != null && Math.abs(sensorReading.getTemperatrue() - lastTemp) > threshold) { collector.collect(new Tuple3<>(\"报警\" + sensorReading.getId(), sensorReading.getTemperatrue(), lastTemp)); } lastTempState.update(sensorReading.getTemperatrue()); } @Override public void close() throws Exception { // 清理状态 lastTempState.clear(); } } } 状态一致性 正确性的级别， 发生故障恢复 和没有发生故障二者的结果 一致性级别 at-most-once: 这其实是没有正确性保障的委婉说法——故障发生之后，计数结果可能丢失。同样的还有 udp。 最多一次 at-least-once: 这表示计数结果可能大于正确值，但绝不会小于正确值。也 就是说，计数程序在发生故障后可能多算，但是绝不会少算。 最少一次 exactly-once: 这指的是系统保证在发生故障后得到的计数结果与正确值一致。 精准一次 端到端一致性 内部保证 —— 依赖 checkpoint source 端 —— 需要外部源可重设数据的读取位置 sink 端 —— 需要保证从故障恢复时，数据不会重复写入外部系统 而对于 sink 端，又有两种具体的实现方式： 幂等（Idempotent）写入和事务性 （Transactional）写入。 所谓幂等操作，是说一个操作，可以重复执行很多次，但只导致一次结果更改， 也就是说，后面再重复执行就不起作用了。 需要构建事务来写入外部系统，构建的事务对应着 checkpoint，等到 checkpoint 真正完成的时候，才把所有对应的结果写入 sink 系统中。 对于事务性写入，具体又有两种实现方式：预写日志（WAL）和两阶段提交 （2PC）。DataStream API 提供了 GenericWriteAheadSink 模板类和 TwoPhaseCommitSinkFunction 接口，可以方便地实现这两种方式的事务性写入。 TODO 预写日志 检查点 Flink 的检查点算法 现有一个算法：任务1：根据第一个元素分组， 任务2：第二个元素求和 检查点之前：b2 c1 b3 检查点之后：a2 a2 c2 当读取输入流的数据源(在本例中与 keyBy 算子内联) 遇到检查点屏障时，它将其在输入流中的位置保存到持久化存储中。如果输入流来 自消息传输系统(Kafka)，这个位置就是偏移量。Flink 的存储机制是插件化的，持久 化存储可以是分布式文件系统，如 HDFS。下图展示了这个过程。 检查点像普通数据记录一样在算子之间流动。当 map 算子处理完前 3 条数据并 收到检查点分界线时，它们会将状态以异步的方式写入持久化存储 当 map 算子的状态备份和检查点分界线的位置备份被确认之后，该检查点操作 就可以被标记为完成 如果检查点操作失败，Flink 可以丢弃该检查点并继续正常执行，因为之后的某 一个检查点可能会成功 Flink+Kafka 如何实现端到端的 exactly-once 语义 内部 —— 利用 checkpoint 机制，把状态存盘，发生故障的时候可以恢复， 保证内部的状态一致性 source —— kafka consumer 作为 source，可以将偏移量保存下来，如果后 续任务出现了故障，恢复的时候可以由连接器重置偏移量，重新消费数据， 保证一致性 sink —— kafka producer 作为 sink，采用两阶段提交 sink，需要实现一个 TwoPhaseCommitSinkFunction 当 checkpoint 启动时，JobManager 会将检查点分界线（barrier）注入数据流； barrier 会在算子间传递下去。 每个算子会对当前的状态做个快照，保存到状态后端。对于 source 任务而言， 就会把当前的 offset 作为状态保存起来。下次从 checkpoint 恢复时，source 任务可 以重新提交偏移量，从上次保存的位置开始重新消费数据。 sink 任务首先把数据写入外部 kafka，这些数据都属于预提交的事务（还不能 被消费）；当遇到 barrier 时，把状态保存到状态后端，并开启新的预提交事务。 当所有算子任务的快照完成，也就是这次的 checkpoint 完成时，JobManager 会 向所有任务发通知，确认这次 checkpoint 完成。 当 sink 任务收到确认通知，就会正式提交之前的事务，kafka 中未确认的数据 就改为“已确认”，数据就真正可以被消费了。 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-27 14:52:01 "},"31bigdata/flink/5sql.html":{"url":"31bigdata/flink/5sql.html","title":"sql","keywords":"","body":"更新： toRetractStream sqlAggr> (true,sensor_1,1) # 删除 撤回 sqlAggr> (false,sensor_1,1) # 插入 sqlAggr> (true,sensor_1,2) sqlAggr> (true,sensor_6,1) sqlAggr> (false,sensor_6,1) sqlAggr> (true,sensor_6,2) sqlAggr> (false,sensor_6,2) sqlAggr> (true,sensor_6,3) sqlAggr> (true,sensor_7,1) sqlAggr> (true,sensor_10,1) table->stream 追加 撤回 s -> t fromDataStream package com.matt.apitest.tableapi; import com.matt.apitest.beans.SensorReading; import org.apache.flink.streaming.api.TimeCharacteristic; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.java.StreamTableEnvironment; import org.apache.flink.types.Row; public class TimeAndWidow5 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 指定事件时间 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream dataStream = inputStream.map(line -> { String[] split = line.split(\",\"); return new SensorReading(split[0], new Long(split[1]), new Double(split[2])); }) // 指定延迟时间 .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor(Time.seconds(2)) { @Override public long extractTimestamp(SensorReading sensorReading) { // 指定事件时间字段 return sensorReading.getTimestamp() * 1000L; } }); // 3.创建表环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 基于流创建一张表 //Table dataTable = tableEnv.fromDataStream(dataStream, \"id, temperatrue, timestamp, pt.proctime\"); // 和类字段保持一致 Table dataTable = tableEnv.fromDataStream(dataStream, \"id, timestamp.rowtime as ts, temperatrue\"); //Table resTable = dataTable.select(\"id, temperatrue, pt\") // .where(\"id = 'sensor_1'\"); // 执行sql tableEnv.toAppendStream(dataTable, Row.class).print(\"事件时间\"); // tableEnv.toAppendStream(resTable, Row.class).print(\"res\"); // job name env.execute(\"my\"); } } 可以追加字段 也可以添加 以一个字段 package com.matt.apitest.tableapi; import com.matt.apitest.beans.SensorReading; import org.apache.flink.streaming.api.TimeCharacteristic; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.Tumble; import org.apache.flink.table.api.java.StreamTableEnvironment; import org.apache.flink.types.Row; public class TimeAndWidow5 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 指定事件时间 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream dataStream = inputStream.map(line -> { String[] split = line.split(\",\"); return new SensorReading(split[0], new Long(split[1]), new Double(split[2])); }) // 指定延迟时间 .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor(Time.seconds(2)) { @Override public long extractTimestamp(SensorReading sensorReading) { // 指定事件时间字段 return sensorReading.getTimestamp() * 1000L; } }); // 3.创建表环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 基于流创建一张表 //Table dataTable = tableEnv.fromDataStream(dataStream, \"id, temperatrue, timestamp, pt.proctime\"); // 和类字段保持一致 Table dataTable = tableEnv.fromDataStream(dataStream, \"id, timestamp.rowtime as ts, temperatrue\"); //Table resTable = dataTable.select(\"id, temperatrue, pt\") // .where(\"id = 'sensor_1'\"); // 注册table tableEnv.createTemporaryView(\"sensor\", dataTable); // 5 窗口操作 // 滚动窗口 Table resTable = dataTable.window(Tumble.over(\"10.seconds\").on(\"ts\").as(\"tw\")) .groupBy(\"id, tw\") .select(\"id, id.count, temperatrue.avg, tw.end\"); //Table resultTable = dataTable.window(Tumble.over(\"10.seconds\").on(\"ts\").as(\"tw\")) // .groupBy(\"id, tw\") // .select(\"id, id.count, temp.avg, tw.end\"); // tumble_start Table resSQLTable = tableEnv.sqlQuery(\"select id, count(id) as cnt, tumble_end(ts, interval '10' second) from sensor\" + \" group by id, tumble(ts, interval '10' second)\"); // 执行sql //tableEnv.toAppendStream(dataTable, Row.class).print(\"事件时间\"); // tableEnv.toAppendStream(resTable, Row.class).print(\"res\"); tableEnv.toAppendStream(resTable, Row.class).print(\"t1\"); tableEnv.toRetractStream(resSQLTable, Row.class).print(\"t2\"); // job name env.execute(\"my\"); } } t2:9> (true,sensor_6,1,2019-01-17 09:43:30.0) t2:1> (true,sensor_1,1,2019-01-17 09:43:20.0) t2:5> (true,sensor_10,1,2019-01-17 09:43:30.0) t1:5> sensor_10,1,38.1,2019-01-17 09:43:30.0 t1:1> sensor_1,1,990.8,2019-01-17 09:43:20.0 t1:9> sensor_6,1,200.4,2019-01-17 09:43:30.0 t1:9> sensor_6,1,999.4,2019-01-17 09:48:30.0 t2:9> (true,sensor_6,1,2019-01-17 09:48:30.0) t1:1> sensor_7,1,6.7,2019-01-17 09:43:30.0 t2:9> (true,sensor_6,1,2019-01-17 09:55:10.0) t2:1> (true,sensor_7,1,2019-01-17 09:43:30.0) t1:9> sensor_6,1,0.4,2019-01-17 09:55:10.0 t2:1> (true,sensor_1,1,2019-01-17 09:46:50.0) t1:1> sensor_1,1,100.8,2019-01-17 09:46:50.0 如果都是空则返回0 FIELD.sum0 topN 表聚合 返回多行多列数据 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-25 02:15:35 "},"31bigdata/hbase/hbase概述.html":{"url":"31bigdata/hbase/hbase概述.html","title":"概述","keywords":"","body":"1.hbase简介 1.1定义 HBase 是一种分布式、可扩展、支持海量数据存储的 NoSQL（非关系型） 数据库。 1.2数据模型 逻辑上，HBase 的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。 但从 HBase 的底层物理存储结构（K-V）来看，HBase 更像是一个 multi-dimensional map。 1.2.1逻辑结构 store:真正存储的东西 1.2.2物理结构 1.2.3数据模型 1.Name Space 命名空间，类似于关系型数据库的 DatabBase 概念，每个命名空间下有多个表。HBase 有两个自带的命名空间，分别是 hbase 和 default，hbase 中存放的是 HBase 内置的表， default 表是用户默认使用的命名空间。 2.Region 类似于关系型数据库的表概念。不同的是，HBase 定义表时只需要声明列族即可，不需 要声明具体的列。这意味着，往 HBase 写入数据时，字段可以动态、按需指定。因此，和关 系型数据库相比，HBase 能够轻松应对字段变更的场景。 3.Row HBase 表中的每行数据都由一个 RowKey 和多个 Column（列）组成，数据是按照 RowKey 的字典顺序存储的，并且查询数据时只能根据 RowKey 进行检索，所以 RowKey 的设计十分重 要。 4.Column HBase 中的每个列都由 Column Family(列族)和 Column Qualifier（列限定符）进行限 定，例如 info：name，info：age。建表时，只需指明列族，而列限定符无需预先定义。 5.Time Stamp 用于标识数据的不同版本（version），每条数据写入时，如果不指定时间戳，系统会 自动为其加上该字段，其值为写入 HBase 的时间。 6.Cell 由{rowkey, column Family：column Qualifier, time Stamp} 唯一确定的单元。cell 中的数据是没有类型的，全部是字节码形式存贮。 1.3基础架构 架构角色： 1.Region Server Region Server 为 Region 的管理者，其实现类为 HRegionServer，主要作用如下: 对于数据的操作：get, put, delete； 对于 Region 的操作：splitRegion、compactRegion。 2.Master Master 是所有 Region Server 的管理者，其实现类为 HMaster，主要作用如下： 对于表的操作：create, delete, alter 对于 RegionServer 的操作：分配 regions 到每个 RegionServer，监控每个 RegionServer的状态，负载均衡和故障转移。 3.Zookeeper HBase 通过 Zookeeper 来做 Master 的高可用、RegionServer 的监控、元数据的入口以及集群配置的维护等工作。 4.HDFS HDFS 为 HBase 提供最终的底层数据存储服务，同时为 HBase 提供高可用的支持。 2.安装 提前安装zk,hadoop。二者都为集群 2.1安装 1.解压 tar -zxvf hbase-1.3.1-bin.tar.gz -C /opt/module/ 2.2配置 1.进入配置目录 /opt/module/hbase-1.3.1/conf 2.记录 JAVA_HOME 路径 echo $JAVA_HOME /opt/module/jdk1.8.0_212 3.编写配置文件hbase-env.sh vim hbase-env.sh // The java implementation to use. Java 1.7+ required. export JAVA_HOME=/opt/module/jdk1.8.0_212 // hbase 有自带的zk,不要使用自带的 // Tell HBase whether it should manage it's own instance of Zookeeper or not. export HBASE_MANAGES_ZK=false 4.hbase-site.xml vim hbase-site.xml hbase-site.xml=core-site.xml hbase.rootdir=fs.defaultFS fs.defaultFS hdfs://matt05:8020 hbase.rootdir hdfs://matt05:8020/HBase hbase.cluster.distributed true hbase.master.port 16000 hbase.zookeeper.property.dataDir /opt/module/zookeeper-3.5.7/zkData hbase.zookeeper.quorum matt05,matt06,matt07 5.集群文件regionservers vim regionservers matt05 matt06 matt07 6建立hadoop快捷方式 ln -s /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml /opt/module/hbase-1.3.1/conf/core-site.xml ln -s /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml /opt/module/hbase-1.3.1/conf/hdfs-site.xml 7.分发安装 xsync hbase-1.3.1/ 2.3启动 记得开启同步时间 方式一 [matt@matt05 bin]$ ./hbase-daemon.sh start master ./hbase-daemon.sh start regionserver 方式二 bin/start-hbase.sh bin/stop-hbase.sh 2.4访问 web页面 ip:port 192.168.96.135:16010 3.使用 3.1基础 进入hbase shell ./hbase shell quit exit 退出； 帮助 help 查看有哪些表 list 命名相关 // 创建命名空间 create_namespace 'test1' create 'test1:student','info1' // 需要先删除该命名空间下的表 drop_namespace 'test1' list_namespace 表相关ddl hbase(main):023:0> create ERROR: wrong number of arguments (0 for 1) Here is some help for this command: Creates a table. Pass a table name, and a set of column family specifications (at least one), and, optionally, table configuration. Column specification can be a simple string (name), or a dictionary (dictionaries are described below in main help output), necessarily including NAME attribute. Examples: Create a table with namespace=ns1 and table qualifier=t1 hbase> create 'ns1:t1', {NAME => 'f1', VERSIONS => 5} 创建表 create 't2','info' 删除表 disable 'test' drop 'test' 修改列组版本 alter 't1', {NAME => 'info', VERSIONS => 1} 查看表的信息 describe 't1' 查看所有表 list dml crud put 'student','1001','info1:name','matt' scan 'student' // 表名 rowKey get 'student','1001' scan 'student',{COLUMNS => ['info1:name', 'info2:sex'], LIMIT => 10, STARTROW => '1001',STOPROW => '1003'} scan 'student', {RAW => true, VERSIONS => 3} hbase(main):010:0> scan 't2', {RAW => true, VERSIONS => 3} ROW COLUMN+CELL 1001 column=info:, timestamp=1630203659417, type=DeleteColumn 1001 column=info:name, timestamp=1630203905379, type=DeleteColumn 1001 column=info:name, timestamp=1630203448221, value=b 1001 column=info:name, timestamp=1630203437357, value=a 1 row(s) in 0.0240 seconds // update put 'student','1001','info1:name','matt1' // 指定到列组则不会删除 delete 'student','1001','info1:sex' deleteall 'student','1002' truncate 'student' Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-02-13 23:20:17 "},"33mq/kafka/1概述.html":{"url":"33mq/kafka/1概述.html","title":"kafka","keywords":"","body":"概述 概念 Kafka 是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于 大数据实时处理领域 mq模式 1.发布/订阅（1:1 1:n） 拉取 缺点可能需要长轮训 2.队列主动推 造成消费者阻塞 kafka：拉取 长轮训 设置超时时间 好处 解耦 削峰 架构 1）Producer ：消息生产者，就是向kafka broker 发消息的客户端； 2）Consumer ：消息消费者，向kafka broker 取消息的客户端； 3）Consumer Group （CG）：消费者组，消费者组可以有一个或多个消费者，一个topic的一个分区只能由消费者组内一个消费者消费 4）Broker ：一台kafka 服务器就是一个broker。一个集群由多个broker 组成。 5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic； 6）Partition：一个topic可由多个分区组成 7）Replica：副本，leader follower 8) leader Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-21 08:59:50 "},"33mq/kafka/2安装.html":{"url":"33mq/kafka/2安装.html","title":"安装","keywords":"","body":"​ 地址 http://kafka.apache.org/downloads.html 规划 matt05 zk kafka matt06 zk kafka matt07 zk kafka 解压 tar -zxvf kafka_2.11-0.11.0.0.tgz -C /opt/module/ 重命名 mv kafka_2.11-0.11.0.0/ kafka 2.scanla版本 0.11kafka版本 在安装目录下创建data文件夹 默认日志放在logs data:存储数据 mkdir data 修改配置文件 cd config/ vi server.properties # 改 broker 的全局唯一编号，不能重复 broker.id=0 # 改 删除 topic 功能使能 delete.topic.enable=true #处理网络请求的线程数量 num.network.threads=3 #用来处理磁盘 IO 的现成数量 num.io.threads=8 #发送套接字的缓冲区大小 socket.send.buffer.bytes=102400 #接收套接字的缓冲区大小 socket.receive.buffer.bytes=102400 # 请求套接字的缓冲区大小 socket.request.max.bytes=104857600 # 改 kafka 运行日志存放的路径 日志目录 log.dirs=/opt/module/kafka/data #topic 在当前 broker 上的分区个数 num.partitions=1 #用来恢复和清理data 下数据的线程数量 num.recovery.threads.per.data.dir=1 #segment 文件保留的最长时间，超时将被删除 log.retention.hours=168 # 改 配置连接 Zookeeper 集群地址 zookeeper.connect=zookeeper.connect=192.168.96.135:2181,192.168.96.136:2181,192.168.96.137:2181 分发安装包 xsync ./kafka/ 记得修改机器的broker.id 因为他是惟一的 启动,安装目录bin目录下， 分别进入三台机器 ./kafka-server-start.sh -daemon ../config/server.properties 关闭,安装目录bin目录下 ./kafka-server-stop.sh stop 使用时可能会无法连接kafka，在server.properties进行如下配置 advertised.listeners=PLAINTEXT://192.168.96.128:9092 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-21 08:59:55 "},"33mq/kafka/4原理.html":{"url":"33mq/kafka/4原理.html","title":"原理.md","keywords":"","body":"整体架构 kafka工作流程 kafka中消息按照topic进行分类 topic是逻辑上的，partition是物理的 每个partition对应一个log文件，该log文件存储producer生产的数据，producer生产的数据会追加到该文件的末端，且每条数据都有自己offset，记录消费者消费数据的位置 由于生产者生产的消息会不断追加到 log 文件末尾，为防止 log 文件过大导致数据定位 效率低下，Kafka 采取了分片和索引机制，将每个 partition 分为多个 segment。每个 segment 对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名 规则为：topic 名称+分区序号。例如，first 这个 topic 有三个分区，则其对应的文件夹为 first-0,first-1,first-2。 index 和 log 文件以当前 segment 的第一条消息的 offset 命名。下图为 index 文件和 log 文件的结构示意图。 “.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元 数据指向对应数据文件中 message 的物理偏移地址。 默认会存7天 168h 生产者 分区策略 分区原因 方便在集群扩展：可以修改分区数量以适应它所在的机器 高并发：因为可以以 Partition 为单位读写了。 如何分区 （1）指明 partition 的情况下，直接将指明的值直接作为 partiton 值； （2）没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值； （3）既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后 面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。 数据可靠性 为保证 producer 发送的数据，能可靠的发送到指定的 topic，topic 的每个 partition 收到 producer 发送的数据后，都需要向 producer 发送 ack（acknowledgement 确认收到），如果 producer 收到 ack，就会进行下一轮的发送，否则重新发送数据。 副本同步策略 方案 优点 缺点 半数以上完成同步，就发 送 ack 延迟低 选举新的 leader 时，容忍 n 台 节点的故障，需要 2n+1 个副 本 全部完成同步，才发送 ack 选举新的 leader 时，容忍 n 台 节点的故障，需要 n+1 个副 本 延迟高 Kafka 选择了第二种方案，原因如下： 1.同样为了容忍 n 台节点的故障，第一种方案需要 2n+1 个副本，而第二种方案只需要 n+1 个副本， 而 Kafka 的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。 2.虽然第二种方案的网络延迟会比较高，但网络延迟对 Kafka 的影响较小 副本数 3 1 个 leader 2 个 follower ISR 问题：如果采用方案二同步，因为某种故障，leader不能和follower同步，那么leader就会一直不能给生产者发送ack Leader 维护了一个动态的 in-sync replica set (ISR)，意为和 leader 保持同步的 follower 集 合。当 ISR 中的 follower 完成数据的同步之后，leader 就会给 follower 发送 ack。如果 follower 长时间 未 向 leader 同 步 数 据 ， 则 该 follower 将 被 踢 出 ISR ， 该 时 间 阈 值 由 replica.lag.time.max.ms 参数设定。Leader 发生故障之后，就会从 ISR 中选举新的 leader。 ack 应答机制 acks： 0：producer 不等待 broker 的 ack，这一操作提供了一个最低的延迟，broker 一接收到还 没有写入磁盘就已经返回，当 broker 故障时有可能丢失数据； 1：producer 等待 broker 的 ack，partition 的 leader 落盘成功后返回 ack，如果在 follower 同步成功之前 leader 故障，那么将会丢失数据； -1（all）：producer 等待 broker 的 ack，partition 的 leader 和 follower 全部落盘成功后才 返回 ack。但是如果在 follower 同步完成后，broker 发送 ack 之前，leader 发生故障，那么会 造成数据重复。 故障处理 1.follower故障 follower 发生故障后会被临时踢出 ISR，待该 follower 恢复后，follower 会读取本地磁盘 记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。 等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重 新加入 ISR 了。 2.leader故障 leader 发生故障之后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的数据一致性，其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader 同步数据。 注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。 Exactly Once 将服务器的 ACK 级别设置为-1，可以保证 Producer 到 Server 之间不会丢失数据，即 At Least Once 语义。相对的，将服务器 ACK 级别设置为 0，可以保证生产者每条消息只会被 发送一次，即 At Most Once 语义。 At Least Once:不丢失，可能会重复 At Most Once：不重复，可能会丢失 At Least Once + 幂等性 = Exactly Once 要启用幂等性，只需要将 Producer 的参数中 enable.idompotence 设置为 true 即可。Kafka 的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的 Producer 在 初始化的时候会被分配一个 PID，发往同一 Partition 的消息会附带 Sequence Number。而 Broker 端会对做缓存，当具有相同主键的消息提交时，Broker 只 会持久化一条。 但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨 分区跨会话的 Exactly Once。 消费者 消费方式 consumer 采用 pull（拉）模式从 broker 中读取数据。 push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。消费者会阻塞 pull 模式不足之处是，如果 kafka 没有数据，消费者会一直轮询。针对这一点，Kafka 的消费者在消费数据时会传入一个时长参数 timeout，如果当前没有 数据可供消费，consumer 会等待一段时间之后再返回，这段时长即为 timeout。 分区分配策略 参考 分区分配 一个topic会有多个分区。一个消费者组会有多个消费者 RoundRobin 1.消费者按照字典序排序 c0 c1 c2... 2.topic按照字典序排序，并得到每个topic所有的分区，从而得到分区集合 3.遍历分区集合，同时轮询消费者 4.如果轮询的消费者没有订阅当前分区所在的topic则跳过当前消费者,g给当前分区给下一个消费者否则分配给当前消费者 同时遍历分区 3个Topic：T0（3个分区0, 1, 2）, T1（两个分区0, 1）, T2（4个分区0, 1, 2, 3）； 3个consumer: C0订阅了[T0, T1]， C1订阅了[T1, T2]， C2订阅了[T2, T0]； roundrobin结果分配结果如下： T0-P0分配给C0，T0-P1分配给C2，T0-P2分配给C0， T1-P0分配给C1，T1-P1分配给C0， T2-P0分配给C1，T2-P1分配给C2，T2-P2分配给C1，T0-P3分配给C2； range 1.获取topic下的所有分区 p0 p1 p2 2.消费者按照字典序排序 c0 c1 c2 3.分区数除消费者数，得到n 4.分区数对消费者数取余，得到m 5.消费者集合中，前m个消费者有n+1分区，剩余的消费者分配n个分区 注意 当消费者组内消费者发生变化时 分区重新分配 offset 维护 记录消费者消费到哪条消息 Kafka 0.9 版本之前，consumer 默认将 offset 保存在 Zookeeper 中，从 0.9 版本开始， consumer 默认将 offset 保存在 Kafka 一个内置的 topic 中，该 topic 为__consumer_offsets。 offset 消费者组 + 主题 + 分区 consumer.properties exclude.internal.topics=false 1.zk ./kafka-console-consumer.sh --topic __consumer_offsets --zookeeper matt05:2181 --formatter \"kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter\" --consumer.config ../config/consumer.properties --from-beginning 2、mq [console-consumer-39623,bigdata,0]::[OffsetMetadata[4,NO_METADATA],CommitTime 1637488435475,ExpirationTime 1637574835475] [console-consumer-39623,bigdata,1]::[OffsetMetadata[4,NO_METADATA],CommitTime 1637488435475,ExpirationTime 1637574835475] 验证一个分区只可以被一个消费者消费 matt05 matt06 vi consumer.properties matt05 matt06 group.id=matt 5 6 启动消费者 ./kafka-console-consumer.sh --zookeeper matt05:2181 --topic first --consumer.config ../config/consumer.properties 7生产者 ./kafka-console-producer.sh --broker-list matt05:9092 --topic first kafka 1）顺序写磁盘 Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端， 为顺序写。官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这 与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。 2）零拷贝 zk Kafka 集群中有一个 broker 会被选举为 Controller，负责管理集群 broker 的上下线，所 有 topic 的分区副本分配和 leader 选举等工作。 Controller 的管理工作都是依赖于 Zookeeper 的 zk:监听节点 事务 Kafka 从 0.11 版本开始引入了事务支持。事务可以保证 Kafka 在 Exactly Once 语义的基 础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。 producer Transaction ID，并将 Producer 获得的PID 和Transaction ID 绑定。并写入内部topic。 TransactionL：用户给定的 consumer 消费者 事务较弱 用户可以修改offset Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-21 09:00:04 "},"33mq/kafka/5api.html":{"url":"33mq/kafka/5api.html","title":"api","keywords":"","body":"Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-21 09:00:10 "},"34mw/zookeeper/zookeeper学习.html":{"url":"34mw/zookeeper/zookeeper学习.html","title":"zookeeper笔记","keywords":"","body":"1.概述 1.1概述 Zookeeper 是一个开源的分布式的，为分布式框架提供协调服务的 Apache 项目。 Zookeeper从设计模式角度来理解:是一个基于观察者模式设计的分布式服务管理框架，它负责 存储和管理大家都关心的数据，然后接受观察者的 注册，一旦这些数据的状态发生变化，Zookeeper 就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应。 存储少量数据，一般是配置信息。 1.2特点 1.Zookeeper:一个领导者(Leader)，多个跟随者(Follower)组成的集群。 2.集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。所以Zookeeper适合安装奇数台服务器。 3.全局数据一致:每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的。 4.更新请求顺序执行，来自同一个Client的更新请求按其发送顺序依次执行。 5.数据更新原子性，一次数据更新要么成功，要么失败。 6.实时性，在一定时间范围内，Client能读到最新数据。 1.3数据结构 ZooKeeper 数据模型的结构与 Unix 文件系统很类似，整体上可以看作是一棵树，每个 节点称做一个 ZNode。每一个 ZNode 默认能够存储 1MB 的数据，每个 ZNode 都可以通过 其路径唯一标识。 1.4应用场景 提供的服务包括:统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下 线、软负载均衡等。 统一命名服务 在分布式环境下，经常需要对应用/服 务进行统一命名，便于识别。 例如:IP不容易记住，而域名容易记住。 统一配置管理 1.分布式环境下，配置文件同步非常常见。 1.1一般要求一个集群中，所有节点的配置信息是一致的，比如 Kafka 集群。 1.2对配置文件修改后，希望能够快速同步到各个节点上。 2配置管理可交由ZooKeeper实现。 2.1可将配置信息写入ZooKeeper上的一个Znode。 2.2各个客户端服务器监听这个Znode。 2.3一旦Znode中的数据被修改，ZooKeeper将通知 各个客户端服务器。 统一集群管理 1分布式环境中，实时掌握每个节点的状态是必要的。 1.1可根据节点实时状态做出一些调整。 2.ZooKeeper可以实现实时监控节点状态变化 2.1可将节点信息写入ZooKeeper上的一个ZNode。 2.2监听这个ZNode可获取它的实时状态变化。 服务器动态上下线 软负载均衡 1.5下载地址 3.5.7 https://archive.apache.org/dist/zookeeper/zookeeper-3.5.7/ 2.安装 2.1前置条件 需要安装jdk,参考linux 目录下的常用软件安装即可看到jdk的安装。 2.2本地安装 安装 1.将tar包发送到服务器 scp -r apache-zookeeper-3.5.7-bin.tar.gz root@123.56.135.43:/opt/software 2.解压 tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/ 3.改名 mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7/ 配置修改 /tmp存储临时数据，一定周期就会删除 1.在 zookeeper 安装目录下创建数据目录 mkdir zkData pwd 2.配置文件目录下的zoo_sample.cfg改为zoo.cfg cd conf mv zoo_sample.cfg zoo.cfg 3.修改配置文件 vim zoo.cfg dataDir=/opt/module/zookeeper-3.5.7/zkData 2.3集群安装 安装 1.将tar包发送到服务器 scp -r apache-zookeeper-3.5.7-bin.tar.gz root@123.56.135.43:/opt/software 2.解压 tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/ 3.改名 mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7/ 配置 1.在zookeeper安装目录下创建zkData目录 mkdir zkData 2.在zkData目录下创建一个 myid 的文件 touch myid 3.编写myid文件，文件中写该服务器唯一标识符 1 4.将该zookeeper安装目录分发到其他服务器 xsync zookeeper-3.5.7 5.更改其他服务中zkData中的myid文件中的唯一标识 6.配置文件目录下的zoo_sample.cfg改为zoo.cfg cd conf mv zoo_sample.cfg zoo.cfg 7.修改配置文件 vim zoo.cfg 8.修改数据存储路径配置 dataDir=/opt/module/zookeeper-3.5.7/zkData 9.添加集群信息 #######################cluster########################## server.2=matt05:2888:3888 server.3=matt06:2888:3888 server.4=matt07:2888:3888 配置文件解读 server.A=B:C:D A 是一个数字，表示这个是第几号服务器;集群模式下配置一个文件 myid，这个文件在 dataDir 目录下，这个文件里面有一个数据 就是 A 的值，Zookeeper 启动时读取此文件，拿到里面的数据zoo.cfg 里面的配置信息比 较从而判断到底是哪个 server。 B 是这个服务器的地址; C 是这个服务器 Follower 与集群中的 Leader 服务器交换信息的端口; D 是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而个端口就是用来执行选举时服务器相互通信的端口。 10.同步配置文件，发送到其他服务器 xsync zoo.cfg 启动集群脚本 cd /home/matt/bin vim zk.sh #!/bin/bash case $1 in \"start\"){ for i in matt05 matt06 matt07 do echo ---------- zookeeper $i 启动 ------------ ssh $i \"/opt/module/zookeeper-3.5.7/bin/zkServer.sh start\" done };; \"stop\"){ for i in matt05 matt06 matt07 do echo ---------- zookeeper $i 停止 ------------ ssh $i \"/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop\" done };; \"status\"){ for i in matt05 matt06 matt07 do echo ---------- zookeeper $i 状态 ------------ ssh $i \"/opt/module/zookeeper-3.5.7/bin/zkServer.sh status\" done };; esac chmod u+x zk.sh zk.sh start zk.sh stop 写数据：半数以上同意就可以给客户端发送ack。 3.客户端操作 启动 服务端启动 ./zkServer.sh start 查看服务端状态 ./zkServer.sh status 服务端关闭 ./zkServer.sh stop 客户端启动 // 不指定服务端 ./zkCli.sh // 指定服务端 ./zkCli.sh -server 192.168.96.132:2181 全格式查看jps jps -l 查看所有命令 help 创建节点 create /matt \"matt\" -e:短暂，客户端关闭再次开启该节点就会删除 -s:带序号 create -e -s /matt \"1\" create -s /matt \"2\" 删除节点 delete /matt // 递归删除节点 deleteall /matt 修改节点 set /matt \"matt\" 查看子节点 // 查看当前节点包含哪些子节点 ls /matt // 更加详细 查看当前节点包含哪些子节点 ls -s /matt 查看当前节点的值 get /matt // 详细 get -s /matt // 查看节点的状态，就是不显示当前的值 stat /matt 1.czxid:创建节点的事务 zxid 每次修改ZooKeeper状态都会产生一个ZooKeeper事务ID。事务ID是ZooKeeper中所 有修改总的次序。每次修改都有唯一的 zxid，如果 zxid1 小于 zxid2，那么 zxid1 在 zxid2 之 前发生。 2.ctime:znode 被创建的毫秒数(从 1970 年开始) 3.mzxid:znode 最后更新的事务 zxid 4.mtime:znode 最后修改的毫秒数(从 1970 年开始) 5.pZxid:znode 最后更新的子节点 zxid 6.cversion:znode 子节点变化号，znode 子节点修改次数 7.dataversion:znode 数据变化号 8.aclVersion:znode 访问控制列表的变化号 9.ephemeralOwner:如果是临时节点，这个是 znode 拥有者的 session id。如果不是 临时节点则是 0。 10.dataLength:znode 的数据长度 11.numChildren:znode 子节点数量 监听 监听器：使用一次就会失效 监听节点的值发生变化 get -w /matt 监听节点的子节点发生变化 ls -w /matt 错误 org.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Address 2021-08-14 11:44:23,004 [myid:] - ERROR [main:QuorumPeerMain@89] - Invalid config, exiting abnormally org.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Address unresolved: 192.168.96.135:3887 在集群配置有空格，删除即可 待做 软件包管理 2.2本地安装配置文件解读 3.1.2选举机制 监听器原理 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-02-13 23:20:17 "},"temp.html":{"url":"temp.html","title":"temp","keywords":"","body":"临时记录 插件 https://jiangminggithub.github.io/gitbook/chapter-plugins/8-code.html Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-27 17:29:51 "},"留言墙.html":{"url":"留言墙.html","title":"留言墙","keywords":"","body":"留言板 var gitalk = new Gitalk({ \"clientID\": \"8ab96766a9f6ec805ff4\", \"clientSecret\": \"df4b41cc098c4e9e2949fb5b72f98cefadb4f29f\", \"repo\": \"iweiwan.github.io\", \"owner\": \"iweiwan\", \"admin\": [\"iweiwan\"], \"id\": decodeURI(location.pathname), \"distractionFreeMode\": false }); gitalk.render(\"gitalk-container\"); Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-26 02:15:39 "}}