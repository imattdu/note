{"./":{"url":"./","title":"快速开始","keywords":"","body":"Introduction 4.7 go 4.8-4.12 gin 4.13-4.17 gom 4.18-4.20 mysql Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-04-05 17:09:26 "},"00go/0base/0安装.html":{"url":"00go/0base/0安装.html","title":"安装","keywords":"","body":"安装 mac 安装 brew install go 配置 vim .zshrc GOROOT=/opt/homebrew/Cellar/go/1.17.6/libexec export GOROOT export GOPATH=/Users/matt/workspace/go export GOBIN=$GOPATH/bin export PATH=$PATH:$GOBIN:$GOROOT/bin win 安装 下载地址 win10/64-》选择x86-64 配置 GOROOT:指定go的安装目录 path:指定go的安装目录下的bin目录 GOPATH:工作目录，go项目的工作目录 系统环境变量中添加 GOROOT D:\\develop\\env\\go path %GOROOT%\\bin GOPATH D:\\matt\\workspace\\go 设置 设置国内镜像 go env -w GOPROXY=https://goproxy.cn,direct go的依赖管理，开启gomod go env -w GO111MODULE=on 验证配置,可能验证的默认网站无法访问 sum.golang.org go env -w GOSUMDB=\"sum.golang.google.cn\" 或者关闭验证 go env -w GOSUMDB=off 验证 验证go环境是否安装成功 go version 格式化代码 go fmt -w main.go 资料 中文API 官网 使用 项目放在目录下 项目都放在src下面 下载的依赖都会进入到pkg文件夹 可执行文件进入bin 包的管理 GO111MODULE可以设置为：off、on、auto（默认值），从GO111MODULE变量名可以看出，是Go1.11版本之后才出来有依赖包管理办法。 为off时，则不使用go mod，查找依赖包的顺序是：当前项目根目录/vendor，其次是$GOPATH/src $GOROOT/src（这是Golang1.11版本之前的用法）。 为on时，则开启go mod，查找依赖包是以当前项目根目录的go.mod文件为基准，会忽略 $GOPATH 和 vendor 文件夹，只根据go.mod下载依赖。 为auto或未设置，则go命令根据当前目录启用或禁用模块支持。根据当前文件夹是否有go.mod来决定是否开启gomod goland 注意事项 1.Global GOPATH 选则你在环境变量中配置的GOPATH路径 2.Project GOPATH 项目的GOPATH,最好不好设置Global GOPATH,因为那你的项目将会使用到所用配置到GOPATH的文件 3.Use GOPATH that`s defined in system environment 如果选中这个，他将使用系统定义的环境变量，并设置到Global GOPATH 4.Index entire GOPATH: 会使用系统中配置的gopath,一般不要勾选这个 常用命令 编译 go build -o ./bin/main main.go 运行 go run main.go -config app.cfg go get download compile install 参数 go get -u xxx -d 只下载不安装 -u 更新下载，默认不会更新下载 指定版本 go get k8s.io/klog@v1.0.0 go install compile install go mod init go mod tidy go mod download 下载模块到本地缓存，缓存路径是 $GOPATH/pkg/mod/cache go mod edit 是提供了命令版编辑 go.mod 的功能，例如 go mod edit -fmt go.mod 会格式化 go.mod go mod graph 把模块之间的依赖图显示出来 go mod init 初始化模块（例如把原本dep管理的依赖关系转换过来） go mod tidy 增加缺失的包，移除没用的包 go mod vendor 把依赖拷贝到 vendor/ 目录下 go mod verify 确认依赖关系 go mod why 解释为什么需要包和模块 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-29 22:23:50 "},"00go/0base/1基础.html":{"url":"00go/0base/1基础.html","title":"基础","keywords":"","body":"入门 基础 程序 新建项目 项目下创建src文件夹，创建bin文件夹，创建pkg文件夹 package main import \"fmt\" func main() { fmt.Println(\"hello word\") } 编译运行 编译运行分开 编译 go build hello.go 运行 hello.exe 编译运行不分开 go run hello.go 分开和不分开区别 1) 如果我们先编译生成了可执行文件，那么我们可以将该可执行文件拷贝到没有 go 开发环境的机器上，仍然可以运行 2) 如果我们是直接 go run go 源代码，那么如果要在另外一个机器上这么运行，也需要 go 开发环境，否则无法执行。 3) 在编译时，编译器会将程序运行依赖的库文件包含在可执行文件中，所以，可执行文件变大了很多。 注意事项 go源文件以 go 为扩展名 go 的入口函数是 main() 函数 go 严格区分大小写 go 语句不需要加分号，go语言自动会加分号 go语句是一行一行编译，所以一行不可以写多条语句 引用的包或者定义的变量必须使用，不使用会报错 注释 package main import \"fmt\" import \"time\" func main() { // 行注释，推荐使用行注释 /* 块注释 你好块注释 */ fmt.Println(time.Now()) } 变量 基础 转义字符 一些特殊的字符：比如用户想要我们打出换行 1) \\t : 表示一个制表符，通常使用它可以排版。 2) \\n ：换行符 3) \\\\ ：一个\\ 4) \\\\\" ：一个\" 5) \\r ：一个回车 fmt.Println(\"aaa\\r bb\"); 变量声明 指定变量类型， 整数：0 浮点数：0 bool:false // 如果不赋值，则使用它的默认值 var j int 不指定类型，自己判断 var i = 100 var j 第三种,省略var // i没有声明过 i := 100 多变量声明 var a, b, c int fmt.Println(a, b, c) var d, e, f = 1, 2, 3 fmt.Println(d, e, f) g, h := \"1\", \"2\" fmt.Println(g, h) 全局变量 // 全局变量 var a1 = 100 var a2 = 200 // 推荐使用 var ( a3 = 300 a4 = 400 ) 常量 // 常量 const i = 10 数据类型 基本数据类型 整数 有符号 无符号 package main // import \"fmt\" // import \"unsafe\" import ( \"fmt\" \"unsafe\" ) func main() { var i int = 100 fmt.Println(i, unsafe.Sizeof(i)) } 浮点数 系统默认使用float64,推荐使用float64 类型 占用存储大小 范围 float32 4 -3.4E38～3.4E38 float64 8 -1.7E308～1.7E308 package main import ( \"fmt\" ) func main() { var i float64 = 1.1 fmt.Println(\"浮点数\", i) } 字符 使用'',字符有一个码值 package main import \"fmt\" func main() { var i byte = '1' fmt.Println(i) fmt.Printf(\"%c\\n\", i) // %d %s %c // 如果一个字符超过byte表示的类型，使用int类型即可 var j int = '中' fmt.Printf(\"%c\", j) } 字符 存储到 计算机中，需要将字符对应的码值（整数）找出来 存储：字符--->对应码值---->二进制-->存储 读取：二进制----> 码值 ----> 字符 --> 读取 ​ bool package main import ( \"fmt\" \"unsafe\" ) func main() { var i bool = false fmt.Println(i, unsafe.Sizeof(i)) } string package main import \"fmt\" func main() { var name string = \"matt\" var address = ` hahahninrerer ` fmt.Println(name) fmt.Println(address) } 1一般使用\"\",也可以输出反引号`` 2反引号可以赋值多行 3字符串拼接 +要写在每一行后面 var address = \"hello\" + \"word\" 4 // 多行输出 fmt.Println(\"hello word\", \"你好\") 类型转换 基本数据类型转换 注意 var i int8 = 10 // i + 8 是int8类型 一般都是强转 var i int8 = 1 var j = int16(i) 基本类型转字符串 使用fmt下的Sprintf package main import ( \"fmt\" ) func main() { var i int8 = 100 var address string = fmt.Sprintf(\"%d hello3434\", i) fmt.Println(address) } 使用strconv包 字符串转其他基本数据类型 使用strconv包 package main import ( \"fmt\" \"strconv\" ) func main() { var i string = \"100\" var j int64 // 因为这个函数会有俩个返回值 j, _ = strconv.ParseInt(i, 10, 8) fmt.Println(j) } 注意 如果是\"hello\"转int，那么直接转为0，并不会报错 高精度到低精度只会精度丢失，并不会报错 指针类型 基础 package main import \"fmt\" func main() { var num int = 10 var p *int = &num fmt.Println(p) *p = 20 fmt.Println(num) } &num:获取num地址 *int:int类型的指针 *p:获取p指针指向变量的地址 值类型和引用类型 (1) 值类型，都有对应的指针类型， 形式为 数据类型，比如 int 的对应的指针就是 int, float32对应的指针类型就是 *float32, 依次类推。 (2) 值类型包括：基本数据类型 int 系列, float 系列, bool, string 、数组和结构体 struct (1) 值类型：基本数据类型 int 系列, float 系列, bool, string 、数组和结构体 struct (2) 引用类型：指针、slice 切片、map、管道 chan、interface 等都是引用类型 值类型：变量直接存储值，内存通常在栈中分配 引用类型：变量存储的是一个地址，这个地址对应的空间才真正存储数据(值)，内存通常在堆 上分配，当没有任何变量引用这个地址时，该地址对应的数据空间就成为一个垃圾，由GC 来回收 标识符 基础 Golang 对各种变量、方法、函数等命名时使用的字符序列称为标识符 凡是自己可以起名字的地方都叫标识符 命名规则 (1) 由 26 个英文字母大小写，0-9 ，_ 组成 (2) 数字不可以开头。var num int //ok var 3num int //error (3) Golang 中严格区分大小写 (4) 下划线\"_\"本身在Go 中是一个特殊的标识符，称为空标识符。可以代表任何其它的标识符，但 是它对应的值会被忽略(比如：忽略某个返回值)。所以仅能被作为占位符使用，不能作为标识符使用 系统保留关键字 系统的预定义标识符 当然，这是在 go 中，go 的关键字多是语句语法，而预定义标识符多是类型 运算符 算术运算符 注意事项 1.取模% a % b = a - a / b * b 与a的正负一致 2.除法 10/3 = 3 3.++ -- go 语言只支持 i++, i-- 这样的，没有 --i , ++i 同时 j = i++这样也是错误的 关系运算符 逻辑运算符 短路 (1) &&也叫短路与：如果第一个条件为 false，则第二个条件不会判断，最终结果为 false (2) ||也叫短路或：如果第一个条件为 true，则第二个条件不会判断，最终结果为 true 赋值运算符 原码反码补码 正数和0：原码反码补码相同 负数的反码：符号位不变，其他位取反 负数的补码：反码+1 计算机都是以补码进行运算 位运算符 因此所说的 有符号、无符号 看的就是二进制的符号位， 无符号:3就不管符号位，右移只填充0；有符号，就是符号位是啥，我就填充啥，Java中也是同理。 运算符优先级 1：括号，++, -- 2: 单目运算 3：算术运算符 4：移位运算 5：关系运算符 6：位运算符 7：逻辑运算符 8：赋值运算符 9：逗号 go没有三木运算符 流程控制 6.1IF ELSE package main import ( \"fmt\" ) func main() { var age int fmt.Scanln(&age) if age 注意：即使if语句中只有以及仍然要加{} 6.2switch package main import ( \"fmt\" ) func main() { var ch byte // 不要使用 Scanln fmt.Scanf(\"%c\", &ch) // switch 'a' // switch test('a') switch ch { case 'a', 'b': fmt.Println(\"星期一或者星期二\") case 'c': fmt.Println(\"星期三\") default: fmt.Println(\"其他\") } } switch：不需要写break，go已经帮我们添加了 golang 的 case 后的表达式可以有多个，使用 逗号 间隔,表示或的意思 case 后的各个表达式的值的数据类型，必须和 switch 的表达式数据类型一致 case/switch 后是一个表达式( 即：常量值、变量、一个有返回值的函数等都可以) // switch 'a' // switch test('a') default:不是必须的 case 后面的表达式如果是常量值(字面量)，则要求不能重复 switch:可以声明一个变量，不推荐 switch age := 12; { case age == 18: fmt.Println(\"18\") // 穿透 fallthrough case age switch：模拟ifel switch { case 1 == 1 && 2 == 3: fmt.Println(\"error\") default: fmt.Println(\"true\") } switch 穿透-fallthrough ，如果在 case 语句块后增加 fallthrough ,则会继续执行下一个 case，也 叫 switch 穿透,但是只可以穿透一次 6.3for//go没有while package main import \"fmt\" func main() { for i := 0; i i := 0 for i 遍历字符串 根据字符遍历的 var str = \"hello wordz中文\" // 如果使用传统的字符串遍历就会出错，因为3个字节 for index, val := range str { //fmt.Println(index, val) fmt.Printf(\"%d %c\\n\", index, val) } 6.4break break:跳出for循环 6.5continue continue:跳出当前循环 6.6label label:使用break跳出指定的for循环 同样continue也可以使用 package main import \"fmt\" func main() { label1: for i := 0; i 6.7goto goto:跳转到指定的行，不推荐使用 package main import ( \"fmt\" ) func main() { goto label1 fmt.Print(\"不执行\") label1: fmt.Print(\"执行\") } 输出：执行 问题 在函数外使用如下就会出错 i := 1 // 等价于 var i int // i = 1 赋值语句不可以在函数外使用，是执行语句 i = 1 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-04-01 00:36:59 "},"00go/0base/2常用数据结构.html":{"url":"00go/0base/2常用数据结构.html","title":"常用数据结构","keywords":"","body":"数组 介绍 存放多个同一类型的数据 使用 声明 var arr1 [3]int 四种初始化方法 var arr1 [3]int = [3]int{1, 2, 3} fmt.Println(arr1) var arr2 = [5]int{1, 2, 3, 4, 5} fmt.Println(arr2) arr3 := [...]float64{1, 2, 3} fmt.Println(arr3) var arr4 = [...]string{1: \"aa\", 0: \"bb\"} fmt.Println(arr4) 遍历 for index, val := range arr4 { fmt.Println(index, val) } for i := 0; i 注意 1.数组一旦声明，其长度不能发生改变；数组创建后，没有赋值，会有默认值 2.go 语言中数组是值类型，而不是引用类型 3.传递参数需要指定数组长度 // [3]int [4]int 认为是不同数据类型 func valueTest(arr [3]int) { arr[0] = 1 } 二维数组 func main() { var arr = [2][2]int{{1, 2}, {3, 4}} fmt.Println(arr) // 只有第一个可以为... var arr1 = [...][3]int{{1, 2}, {3, 4}} fmt.Println(arr1) } 切片 基础 初始化 数组的切 - make - 直接初始化 - 直接声明 1 var arr = [5]int{1, 2, 3, 45, 5} slice := arr[2:4] arr[2] = -1 fmt.Println(slice) // 元素数量fmt.Println(len(slice)) // 容量fmt.Println(cap(slice)) 2 var slice1 []int = make([]int, 2, 10) slice1[0] = 0 slice1[1] = 1 fmt.Println(slice1) 3 fmt.Println(\"方式三\") var slice2 []float64 = []float64{1, 2, 3} fmt.Println(cap(slice2)) 4 var s []int type slice struct { ptr *[2]int len int cap int } 引用一个数组 数量 容量 遍历 for index, value := range slice2 { fmt.Printf(\"---%v:%v \", index, value) } 使用 数组切片分割 var slieceDemo = arr[startIndex: endIndex] 包含左边，不包含右边 var slice = arr[0:end] 可以简写 var slice = arr[:end] var slice = arr[start:len(arr)] 可以简写： var slice = arr[start:] var slice = arr[0:len(arr)] 可以简写: var slice = arr[:] cap 是一个内置函数，用于统计切片的容量，即最大可以存放多少个元素。 切片仍然可以切片 // 元素数量fmt.Println(len(slice))// 容量fmt.Println(cap(slice)) append 用 append 内置函数，可以对切片进行动态追加 var arr = [5]int{1, 2, 3, 4, 5} var slice []int = arr[:] fmt.Println(slice)// 返回 slice = append(slice, 10) fmt.Println(slice) 切片的拷贝 package mainimport \"fmt\" func main() { var slice1 []int = make([]int, 5) var slice2 []int = make([]int, 10) slice1[0] = 1 copy(slice2, slice1) fmt.Println(slice2) } 注意 切片是引用传递 string底层也是数组，可以使用切片 Map使用 介绍 key-value 的数据结构，类似于 java 中的 map key 的类型：bool, 数字，string, 指针, channel , 还可以是只包含前面几个类型的 接口, 结构体, 数组，通常 key 为 int 、string 注意: slice， map 还有 function 不可以，因为这几个没法用 == 来判断 valuetype 的类型和 key 基本一样，通常为: 数字(整数,浮点数),string,map,struct 使用 声明 var map1 map[int]int 1.map 在使用前一定要make(需要分配内存) 2.map 的 key 是不能重复，v 可以重复 map 的 key-value 是无序 三种使用方式 var map1 map[int]int map1 = make(map[int]int, 10) fmt.Println(map1) map2 := make(map[int]int, 10) fmt.Println(map2) map3 := map[int]int { 1: 1, 2: 2, } fmt.Println(map3) map中的方法 添加和更新 var a = make(map[int]string) a[0] = \"matt\" a[1] = \"pony\" a[0] = \"jack\" a[100] = \"aa\" 删除 // 删除不存在的 key 也不会出错 delete(a, 1) 查找 // val:值 ok:是否找到 val, ok := a[3] fmt.Println(ok, val) 遍历 a1 := make(map[string]string) a1[\"1\"] = \"a\" a1[\"2\"] = \"b\" a1[\"3\"] = \"c\" a1[\"4\"] = \"d\" for k, v := range a1 { fmt.Println(k, v) } 长度 fmt.Println(len(a1), \"map的长度\") 注意 map是引用类型 map会自动扩容 map的 value 经常是struct Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-29 23:04:44 "},"00go/0base/3函数.html":{"url":"00go/0base/3函数.html","title":"函数","keywords":"","body":"函数 认识 func add(i int, j int) int { return i + j } func 函数名 (参数列表) (返回值类型列表) ​ 函数体 ​ 可以有返回值也可以没有 包 什么是包 一个文件夹，对应一个包 使用 package db var Ip int = 11 package main import ( // 包名的路径是在 %GOPATH/src路径下的 \"db\" \"fmt\" \"utils\" ) func main() { fmt.Println(\"hello word\") fmt.Println(utils.Add(1, 2)) fmt.Println(db.Ip) } 注意事项 1.文件的包名通常和它的文件夹名一样 2.包的引入 import \"utils\" 推荐这种 import ( \"fmt\" \"utils\" ) 关于包的路径：包的路径是在 $GOPATH/src 开始找 3.包名.方法 包名.变量 大小写 不能重复变量名或者方法名 4.包名可以起别名 import tDb \"db\" 5.如果想把一个.go文件编译成可执行文件，则把该文件包名设置为 main return 3 只有一个返回值，可以不写 () 可以用_接收方法的返回值 func add(i int, j int) int { return i + j } func add(i int, j int) (int, int) { return i + j, 1 } func main() { res, _ = add(1, 2) } 函数注意细节 1.参数值 、引用 类型可以是值类型，也可以是引用类型 基础数据类型、数组使用值得传递，引用类型使用引用传递 1)值类型：基本数据类型 int 系列, float 系列, bool, string 、数组和结构体 struct 2)引用类型：指针、slice 切片、map、管道 chan、interface 等都是引用类型 2.支持可变参数 func test4(args... int) () { fmt.Println(args) fmt.Println(len(args)) } func main() { test4(1, 2, 3) } 3.支持对方法返回值命名 可以不写 return sum // 直接对返回值命名 func test3(i int, j int) (sum int) { sum = i + j return } 4.使用 _接收方法的返回值 5.函数 注意变量的访问范围，函数内声明的，函数外就不可以访问，如果函数内和函数声明相同的变量，则函数内使用函数内声明的变量 大小写控制访问范围 不支持重载 6.函数可以赋值给一个变量，也可以作为参数 a := test1 fmt.Println(a(1)) func test2(test1 func(int) (int, int), n int) (){ fmt.Println(test1(1)) return } init函数 main函数执行之前被执行 func init() { // main函数执行之前被执行 fmt.Println(\"init function\") } 变量定义 -> init 函数 -> main 函数 如果引用了其他文件，先执行其他文件的 匿名函数 认知 匿名函数：没有名字的函数 使用 1.在定义的时候就使用 // 定义的时候就使用 res := func(i int, j int) int { return i + j }(1, 2) fmt.Println(res) 2.将函数赋值一个变量 // 将该函数给一个变量，通过变量来调用 add := func(i int, j int) int { return i + j } fmt.Println(add(1, 2)) 闭包 认识 闭包：一个函数和与其相关的引用环境组合的一个整体(实体) 函数返回一个函数 使用 func makeSuffix(suffix string) func(string) string { return func(fileName string) string { if strings.HasSuffix(fileName, suffix) { return fileName } return fileName + suffix } } 返回的函数和suffix组成一个整体，返回的函数对suffix进行操作 func main() { b := makeSuffix(\".jpg\") fmt.Println(b(\"aaa.js\")) fmt.Println(b(\"aa.jpg\")) } defer使用 func testDefer(n int) int { // 值的拷贝 defer fmt.Println(\"defer\" + fmt.Sprintln(n)) n++ fmt.Println(\"111\") return 1 } defer fmt.Println(\"defer\" + fmt.Sprintln(n)) 这样会依次压入栈，并在该函数执行完后在从栈取出，同时n也是值的拷贝 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-04-01 00:35:04 "},"00go/0base/4结构体.html":{"url":"00go/0base/4结构体.html","title":"结构体","keywords":"","body":"结构体 概述 go 语言并没有类，而采用结构体，仍然有面向对象中封装继承多态等特性。 结构体使用 声明结构体 type 结构体名 struct { 字段名 类型 } type Person struct { Name string Age int // 不是一个包访问不到 p *int slice []int map1 map[string]string } 结构体是值类型 创建一个结构体变量后，如果没有给字段赋值，都会有零值，如果是引用类型，在没有进行分配内存则它的值为 nil 。 结构体初始化 func main() { // 1 var person Person person.Age = 11 person.Name = \"matt\" fmt.Println(person) //2 var p1 = Person{} fmt.Println(p1) // 3 var p3 *Person = new(Person) (*p3).Name = \"33\" p3.Name = \"44优化等价于上面的\" fmt.Println(*p3) // 4 var p4 *Person = &Person{} p4.Age = 111 fmt.Println(*p4) } 创建结构体变量也可以指定字段值 func main() { var c Computer = Computer{ Age: 1, Name: \"matt\", } fmt.Println(c) } 可以只写部分字段，这样指定字段可以不按结构体中的顺序，否则就要按结构体中的顺序。 结构体使用注意 1.结构体所有字段在内存中连续的 2.struct 的每个字段上，可以写上一个 tag, 该 tag 可以通过反射机制获取，常见的使用场景就是序列化和反序列化。 import \"fmt\" import \"encoding/json\" type A struct { age int num int } type B struct { age int num int } type C struct { Name string `json:\"name\"` } func main() { var a A var b B a.age = 1 // 要求字段一致 b = B(a) var c C c.Name = \"hello ma\" str1, _ := json.Marshal(c) fmt.Println(string(str1)) fmt.Println(b) } 3.俩个结构体进行转换的时候，要求这俩个结构体需要有相同的字段 a = B(b) 4.对结构体进行重新定义， golang 认为是不同的数据类型 type P Person 方法 使用 import \"fmt\" type Person struct { Age int } func (p Person) getAge() int { return p.Age } func main() { var p Person p.Age = 18 fmt.Println(p.getAge()) } 该方法和 Person 结构体进行绑定 方法进行调用会把调用该方法的变量赋值给该方法 方法使用注意 1.结构体类型是值类型，在方法调用中，遵守值类型的传递机制，是值拷贝传递方式 方法也可以接收结构体指针,引用传递 方法接收类型是什么就是什么传递 func (circle *Circle) test() { // (*circle).R // 进行了优化 fmt.Println(circle.R) } 2.方法不仅可以作用于结构体，还可以作用于 int,string等 import \"fmt\" func (i integer) testInt() { fmt.Println(i) } type integer int func main() { var j integer = 10 j.testInt() } 3.方法的访问范围控制的规则，和函数一样。方法名首字母小写，只能在本包访问，方法首字母 大写，可以在本包和其它包访问。 4.如果一个类型实现了 String()这个方法，那么 fmt.Println 默认会调用这个变量的 String()进行输出，类似于 java 中的 toString() 方法 func (c Computer) String() string { str := fmt.Sprintf(\"Name=%s Age=%d\", c.Name, c.Age) return str } 面向对象特性 封装 概念 将对象属性隐藏起来，属性的操作只可以通过被授权的方法。 使用 结构体属性、方法首字母小写，其他包就不可以访问，当前包仍然可以访问。 为结构体提供一个创建的函数，相当于java中的构造函数。 编写set,get 方法。 package main type Person struct { name string age int } func NewPerson() Person { var person = Person{} return person } func (p *Person) SetName(name string) { p.name = name } func (p *Person) GetName() string { return p.name } 继承 概述 子类具有父类的属性和方法 使用 type S struct { Name string Age int } type X struct { S } func (s S) showInfo() { fmt.Println(s.Name) } func (x *X) showInfo() { fmt.Printf(\"x---name=%v age=%v\\n\",x.Name, x.Age) } func main() { var x = &X{} x.S.Name = \"matt\" x.S.Name = \"aa\" x.Name = \"ma\" x.S.showInfo() x1 := X{ S{ Name: \"aa\", Age: 11, }, } fmt.Println(x1) } 继承使用注意 结构体可以使用嵌套匿名结构体所有的字段和方法(首字母大小写都可以获得) 结构体访问可以简化 func main() { var x = &X{} x.S.Name = \"matt\" // 对上面进行简化 x.Name = \"ma\" x.S.showInfo() } 1.上述Name字段，编译器首先会从 X 中查找该属性，找不到在从不S中找，最后找不到则报错。 当结构体和匿名结构体有相同的字段或者方法时，编译器采用就近访问原则访问，如希望访问 匿名结构体的字段和方法，可以通过匿名结构体名（x.S.Name）来区分 2.结构体嵌入两个(或多个)匿名结构体，如两个匿名结构体有相同的字段和方法(同时结构体本身 没有同名的字段和方法)，在访问时，就必须明确指定匿名结构体名字，否则编译报错。 3.如果一个 struct 嵌套了一个有名结构体，这种模式就是组合，如果是组合关系，那么在访问组合 的结构体的字段或方法时，必须带上结构体的名字 字段需要指定 方法不需要指定 type A struct { Name string } type B struct { a A } func main() { var b B = B{} b.a.Name = \"matt\" fmt.Println(b) } 可以在创建结构体变量时指定匿名结构体值 var a A = B{ B{ Name: \"matt sir\" } } package main import \"fmt\" type F1 struct { Name string } type S1 struct { F1 Age int } func main() { s1 := S1{ Age: 11, F1: F1{ Name:\"matt\", }, } fmt.Println(s1) } 接口 使用 type I interface { start() } type A struct { } func (a *A) start() { fmt.Println(\"A 开始...\") } 结构体 A 实现接口 I 使用接口注意 1.接口中不可以有变量,只有方法，接口中所有的方法都没有实现 2.一个自定义类型（包括结构体）实现了一个接口的所有方法，就认为实现了该接口 3.接口本身不能创建实例,但是可以指向一个实现了该接口的自定义类型的变量(实例) interface 类型默认是一个指针(引用类型)，如果没有对 interface 初始化就使用，那么会输出 nil // I是接口 s 是S的实例，S实现I var s I 4.一个接口可以继承多个接口，比如A接口继承B，C, 如果实现A接口，那么就要把A,B,C中所有方法实现。 5.空接口是没有方法，即任何变量实现了空接口 实现和继承 继承的价值主要在于：解决代码的复用性和可维护性。 接口的价值主要在于：设计，设计好各种规范(方法)，让其它自定义类型去实现这些方法 多态 多态参数： 一个方法中的一个参数是I类型，那么就可以接收实现I类型的所有类 多态数组 一个数组中的I类型，那么就可以接收实现I类型的所有类 类型断言 概述 一个变量知道他是某一个接口类型，但是不知道它是哪一个具体类型，所以使用断言。 使用 func main() { var a interface{} var b B = B{} a = b c, ok := a.(B) fmt.Println(ok) fmt.Println(c) } Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-04-04 17:53:40 "},"00go/1concurrent/0groutinue.html":{"url":"00go/1concurrent/0groutinue.html","title":"groutine","keywords":"","body":"groutinue TODO： ​ 进程线程协程区别 分别是啥 ​ 底层原理 线程：需要维护线程池，线程的调度去执行任务 协程：go语言已经帮我们实现了，我们只需定义任务即可,让系统将我们的任务分配给cpu执行 go 函数() main 协程结束 整体程序就会结束 不会等待其他协程 time.Sleep package main import ( \"fmt\" \"time\" ) func main() { go func() { fmt.Println(\"start...\") time.Sleep(time.Second * 10) fmt.Println(\"end...\") }() fmt.Println(\"main start...\") time.Sleep(time.Second * 5) fmt.Println(\"main end...\") } wg package main import ( \"fmt\" \"sync\" ) var wg sync.WaitGroup func w1() { defer wg.Done() fmt.Println(\"wg1\") } func main() { for i := 0; i Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-29 00:43:03 "},"00go/1concurrent/1runtime.html":{"url":"00go/1concurrent/1runtime.html","title":"runtime","keywords":"","body":"runtime runtime.Gosched() runtime.Goexit() runtime.GOMAXPROCS(1) package main import ( \"fmt\" \"runtime\" ) func main() { // 1.15 默认是cpu核心数 无需配置 runtime.GOMAXPROCS(1) go func() { for i := 0; i Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-28 22:01:10 "},"00go/1concurrent/2channel.html":{"url":"00go/1concurrent/2channel.html","title":"channel","keywords":"","body":"channel 声明+初始化 package main import \"fmt\" func main() { // 需要make 5：容量 var t1 chan int = make(chan int, 5) // var t1 = make(chan int, 5) // 写入 t1 注意 1.对一个关闭的通道再发送值就会导致panic。 2.对一个关闭的通道进行接收会一直获取值直到通道为空。 3.对一个关闭的并且没有值的通道执行接收操作会得到对应类型的零值。 4.关闭一个已经关闭的通道会导致panic。 无缓冲管道 不使用协程就会报错，使用协程不会报错 报错 package main func main() { c1 := make(chan int) c1 阻塞 使用协程 package main import \"fmt\" func main() { //c1 := make(chan int) //c1 有缓冲管道 package main import \"fmt\" func main() { //c1 := make(chan int) //c1 读取 俩种读取 package main import ( \"fmt\" ) func main() { t3 := make(chan int, 100) for i := 0; i package main import ( \"fmt\" \"time\" ) func main() { c1 := make(chan int, 3) for i := 0; i 单向管道 可以把双向管道赋值给单向管道，比如函数的参数是单向管道 package main func main() { t1 := make(chan 异常 channel nil 非空 空 满 非满 接收 阻塞 接收值 阻塞 接收值 接收值 发送 阻塞 发送值 发送值 阻塞 发送值 关闭 panic 关闭成功，读完数据返回零值 关闭成功，读完数据返回零值 关闭成功，读完数据返回零值 关闭成功，读完数据返回零值 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-30 00:30:43 "},"00go/1concurrent/3定时器.html":{"url":"00go/1concurrent/3定时器.html","title":"定时器","keywords":"","body":"定时器 只执行一次 package main import ( \"fmt\" \"time\" ) func main() { // 只执行一次 fmt.Println(time.Now(), \"begin\") timer := time.NewTimer(time.Second * 2) fmt.Println( 执行多次 package main import ( \"fmt\" \"time\" ) func main() { // 定时器执行多次 ticker := time.NewTicker(time.Second * 2) for { Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-28 22:39:31 "},"00go/1concurrent/4select.html":{"url":"00go/1concurrent/4select.html","title":"select","keywords":"","body":"select 基础 可以同时判断多个管道是否可以发送、接收 如果同时有多个管道可以执行，那么随机选择一个 如果都有准备好，那么执行default 也可以判断管道是否存满 package main import ( \"fmt\" \"math/rand\" ) func main() { t1 := make(chan int, 10) t2 := make(chan int, 10) for i := 0; i Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-28 22:49:18 "},"00go/1concurrent/5lock.html":{"url":"00go/1concurrent/5lock.html","title":"lock","keywords":"","body":"问题 同时有多个协程对共享数据修改，可能会产生问题 互斥锁、读写锁 package main import ( \"fmt\" \"sync\" ) var ( cnt int // 互斥锁 lock sync.Mutex // 读写锁 rwLock sync.RWMutex w sync.WaitGroup ) func add() { // 互斥锁 for i:= 0; i 互斥锁：如果一个协程获得互斥锁，其他协程只能等待，如果该协程释放锁，那么会随机环形等待该锁的一个协程 读写锁：如果有一个协程获取读锁，其他协程仍然可以获取读锁，但是不能获取写锁，只能等待； 如果有一个协程获取写锁，其他协程只能等待 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-28 22:55:43 "},"00go/1concurrent/6sync.html":{"url":"00go/1concurrent/6sync.html","title":"sync","keywords":"","body":"sync sync.WaitGroup sync.WaitGroup内部维护着一个计数器，计数器的值可以增加和减少。 Add(n):+n Done:-1 Wait:等待计数器为0 注意sync.WaitGroup是一个结构体，传递的时候要传递指针。 package main import ( \"fmt\" \"sync\" ) var ( wg1 sync.WaitGroup ) func t1() { fmt.Println(\"t1...\") // 计数器-2 defer wg1.Done() } func main() { // 计数器+2 wg1.Add(2) go add() go add() // 让main协程等待wg计数器为0在执行 wg1.Wait() } sync.Once 只执行一次 sync.Once其实内部包含一个互斥锁和一个布尔值，互斥锁保证布尔值和数据的安全，而布尔值用来记录初始化是否完成。这样设计就能保证初始化操作的时候是并发安全的并且初始化操作也不会被执行多次。 package main import ( \"fmt\" \"sync\" ) var ( so sync.Once m map[int]string ) func a2() { m = make(map[int]string) } func a1() { // 不使用 so 通过判断 m != nil 可能没有初始化完成但是 != nil // 使用协程又会有安全问题 so.Do(a2) fmt.Println(\"this is good\") } func main() { go a1() go a1() } sync.Map 系统内置的map不安全 package main import ( \"fmt\" \"sync\" ) func main() { m1 := sync.Map{} // 添加 m1.Store(\"name\", \"matt\") m1.Store(\"age\", 22) // 获取 fmt.Println(m1.Load(\"name\")) // 存储或者更新 v, ok := m1.LoadOrStore(\"name\", \"matt\") if ok { fmt.Println(v) } // 删除 m1.Delete(\"name\") // 存在返回false v, ok = m1.LoadOrStore(\"name\", \"aaa\") if !ok { fmt.Println(v, \"2222load and store\") } //m1.Delete(\"age\") // 不存在返回 false v, ok = m1.Load(\"ccc\") fmt.Println(v, ok) // 遍历 m1.Range(func(k, v interface{}) bool { fmt.Println(k, v) return true }) } Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-28 23:24:01 "},"00go/1concurrent/7atomic.html":{"url":"00go/1concurrent/7atomic.html","title":"atomic","keywords":"","body":"atomic 基础操作 原子操作 方法 解释 func LoadInt32(addr int32) (val int32) func LoadInt64(addr `int64) (val int64)func LoadUint32(addruint32) (val uint32)func LoadUint64(addruint64) (val uint64)func LoadUintptr(addruintptr) (val uintptr)func LoadPointer(addrunsafe.Pointer`) (val unsafe.Pointer) 读取操作 func StoreInt32(addr *int32, val int32) func StoreInt64(addr *int64, val int64) func StoreUint32(addr *uint32, val uint32) func StoreUint64(addr *uint64, val uint64) func StoreUintptr(addr *uintptr, val uintptr) func StorePointer(addr *unsafe.Pointer, val unsafe.Pointer) 写入操作 func AddInt32(addr *int32, delta int32) (new int32) func AddInt64(addr *int64, delta int64) (new int64) func AddUint32(addr *uint32, delta uint32) (new uint32) func AddUint64(addr *uint64, delta uint64) (new uint64) func AddUintptr(addr *uintptr, delta uintptr) (new uintptr) 修改操作 func SwapInt32(addr *int32, new int32) (old int32) func SwapInt64(addr *int64, new int64) (old int64) func SwapUint32(addr *uint32, new uint32) (old uint32) func SwapUint64(addr *uint64, new uint64) (old uint64) func SwapUintptr(addr *uintptr, new uintptr) (old uintptr) func SwapPointer(addr *unsafe.Pointer, new unsafe.Pointer) (old unsafe.Pointer) 交换操作 func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool) func CompareAndSwapInt64(addr *int64, old, new int64) (swapped bool) func CompareAndSwapUint32(addr *uint32, old, new uint32) (swapped bool) func CompareAndSwapUint64(addr *uint64, old, new uint64) (swapped bool) func CompareAndSwapUintptr(addr *uintptr, old, new uintptr) (swapped bool) func CompareAndSwapPointer(addr *unsafe.Pointer, old, new unsafe.Pointer) (swapped bool) 比较并交换操作 package main import ( \"fmt\" \"sync/atomic\" ) func main() { var i int64 = 100 res := atomic.AddInt64(&i, 100) res = atomic.SwapInt64(&i, 3) fmt.Println(res, i) } Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-29 00:59:31 "},"00go/2常用包/0string.html":{"url":"00go/2常用包/0string.html","title":"string","keywords":"","body":"字符串常用的函数 1.len():统计字符串字节的长度 一个中文三个字节 var str string = \"hello 中文\" fmt.Println(len(str)) 2.字符串遍历，根据字符进行遍历 str1 := []rune(str) for i := 0; i 3.字符串转整数 str2 := \"1111111\" var i int64 // bitsize指定8， 如果是3333 则返回128 i, _ = strconv.ParseInt(str2, 10, 64) fmt.Println(i) str3 := \"123\" //var k int k, err := strconv.Atoi(str3) if err != nil { fmt.Println(\"转换错误\") } else { fmt.Println(k) } 4.整数转字符串 var str4 string = strconv.Itoa(11) fmt.Println(str4) 5.字符串 转 []byte: var bytes = []byte(\"hello go\") var str string = \"hello word\" bytes := []byte(str) fmt.Println(bytes) 6.字节转字符串 fmt.Println(string(bytes)) 7.十进制转任意进制 // 十进制转二进制 fmt.Println(strconv.FormatInt(67,2)) 8.查找子串是否在指定的字符串中: strings.Contains(\"seafood\", \"foo\") //true // strings Contains fmt.Println(strings.Contains(\"hello\", \"ele\")) 9.统计一个字符串有几个指定的字符串 // strings Count fmt.Println(\"strings Count:\" ,strings.Count(\"hello\", \"e\")) 10.字符串比较 EqualFold:忽略大小写 // strings.EqualFold fmt.Println(strings.EqualFold(\"ab\", \"Ab\")) fmt.Println(\"ab\" == \"Ab\") 11返回子串第一次在字符串出现的位置 fmt.Println(\"Index\") fmt.Println(strings.Index(\"abcdef mcatt\", \"c\")) fmt.Println(strings.LastIndex(\"abcdef mcactt\", \"c\")) 12返回子串最后一次在字符串出现的位置 fmt.Println(strings.LastIndex(\"abcdef mcactt\", \"c\")) 13将指定的子串替换成 另外一个子串: strings.Replace(\"go go hello\", \"go\", \"go 语言\", n) n 可以指 定你希望替换几个，如果 n=-1 表示全部替换 fmt.Println(strings.Replace(\"aa bb ccbc bb bb \", \"bb\", \"BB\", -1)) 14字符串分割 str := \"aabcbebeeee\" strArr := strings.Split(str, \"b\") for i := 0; i 15字符串大小写转换 fmt.Println(strings.ToLower(\"Abc\")) fmt.Println(strings.ToUpper(\"Abc\")) 16去除字符串俩边的空格 fmt.Println(strings.TrimSpace(\" aa dfer dfdf \")) 17字符串指定的字符去除 fmt.Println(strings.Trim(\"aabbccaa\", \"aa\")) fmt.Println(strings.TrimLeft(\"aabbccaa\", \"aa\")) fmt.Println(strings.TrimRight(\"aabbcccaa\", \"aa\")) 18字符串是否包含某个前缀、后缀 fmt.Println(strings.HasPrefix(\"aabbcc\", \"aa\")) fmt.Println(strings.HasSuffix(\"aabbcc\", \"aa\")) Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-04-05 15:53:04 "},"00go/2常用包/1time.html":{"url":"00go/2常用包/1time.html","title":"time","keywords":"","body":"时间 时间类型 time.Time类型表示时间。 time.Now()函数获取当前的时间对象，然后获取时间对象的年月日时分秒等信息 package main import ( \"fmt\" \"time\" ) func main() { // 1获取当前时间 t := time.Now() fmt.Println(t) year := t.Year() month := t.Month() day := t.Day() hour := t.Hour() minute := t.Minute() second := t.Second() fmt.Println(year, month, day, hour, minute, second) // 2.获取时间戳 // s timestamp1 := t.Unix() // ns timestamp2 := t.UnixNano() fmt.Println(timestamp1, timestamp2) // 时间戳 -> 时间对象 t1 := time.Unix(timestamp1, 0) fmt.Println(t1) // 3.时间间隔 fmt.Println(time.Second, time.Minute) // 4.时间操作 op := time.Now() opAdd := op.Add(time.Hour) fmt.Println(op, opAdd) fmt.Println(op.Sub(opAdd)) fmt.Println(op.Equal(opAdd), \"equal\") // op 是否在opAdd之前 是：true fmt.Println(op.Before(opAdd), \"before\") fmt.Println(op.After(opAdd), \"after\") // 定时器 可参考并发 fmt.Println(\"时间格式化--------------\") // 5 时间格式化 ft := time.Now() // 格式化的模板为Go的出生时间2006年1月2号15点04分 Mon Jan // 24小时制 fmt.Println(ft.Format(\"2006-01-02 15:04:05.000 Mon Jan\")) // 12小时制 fmt.Println(ft.Format(\"2006-01-02 03:04:05.000 PM Mon Jan\")) fmt.Println(ft.Format(\"2006/01/02 15:04\")) fmt.Println(ft.Format(\"15:04 2006/01/02\")) fmt.Println(ft.Format(\"2006-01-02\")) // 加载时区 loc, err := time.LoadLocation(\"Asia/Shanghai\") if err != nil { fmt.Println(err) return } // 按照指定时区和指定格式解析字符串时间 pT, err := time.ParseInLocation(\"2006/01/02 15:04:05\", \"2018/08/04 14:15:20\", loc) if err != nil { fmt.Println(err) return } fmt.Println(pT) } 时间戳 时间戳是自1970年1月1日（08:00:00GMT）至当前时间的总秒数。它也被称为Unix时间戳（UnixTimestamp）。 package main import ( \"fmt\" \"time\" ) func main() { // 2.获取时间戳 // s timestamp1 := t.Unix() // ns timestamp2 := t.UnixNano() fmt.Println(timestamp1, timestamp2) // 时间戳 -> 时间对象 t1 := time.Unix(timestamp1, 0) fmt.Println(t1) } 时间间隔 时间间隔 const ( Nanosecond Duration = 1 Microsecond = 1000 * Nanosecond Millisecond = 1000 * Microsecond Second = 1000 * Millisecond Minute = 60 * Second Hour = 60 * Minute ) 时间操作 add t1.Add(time.Second) sub 返回一个时间段t-u。如果结果超出了Duration可以表示的最大值/最小值，将返回最大值/最小值。 要获取时间点t-d（d为Duration），可以使用t.Add(-d)。 time.Sub(time.Second) Equal 判断俩个时间是否相等， 和t1 == t2不同，这个方法还会比较地区和时间 t1.Equal(t2) Before After package main import ( \"fmt\" \"time\" ) func main() { // 4.时间操作 op := time.Now() opAdd := op.Add(time.Hour) fmt.Println(op, opAdd) fmt.Println(op.Sub(opAdd)) fmt.Println(op.Equal(opAdd), \"equal\") // op 是否在opAdd之前 是：true fmt.Println(op.Before(opAdd), \"before\") fmt.Println(op.After(opAdd), \"after\") // 定时器 可参考并发 } 时间格式化 go诞生时间： 2006 0102 15:04 2006 1234 如果想格式化为12小时方式，需指定PM。 package main import ( \"fmt\" \"time\" ) func main() { fmt.Println(\"时间格式化--------------\") // 5 时间格式化 ft := time.Now() // 格式化的模板为Go的出生时间2006年1月2号15点04分 Mon Jan // 24小时制 fmt.Println(ft.Format(\"2006-01-02 15:04:05.000 Mon Jan\")) // 12小时制 fmt.Println(ft.Format(\"2006-01-02 03:04:05.000 PM Mon Jan\")) fmt.Println(ft.Format(\"2006/01/02 15:04\")) fmt.Println(ft.Format(\"15:04 2006/01/02\")) fmt.Println(ft.Format(\"2006-01-02\")) // 加载时区 loc, err := time.LoadLocation(\"Asia/Shanghai\") if err != nil { fmt.Println(err) return } // 按照指定时区和指定格式解析字符串时间 pT, err := time.ParseInLocation(\"2006/01/02 15:04:05\", \"2018/08/04 14:15:20\", loc) if err != nil { fmt.Println(err) return } fmt.Println(pT) } Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-04-05 11:22:53 "},"00go/2常用包/2flag.html":{"url":"00go/2常用包/2flag.html","title":"flag","keywords":"","body":"flag os.Args package main import ( \"fmt\" \"os\" ) func main() { if len(os.Args) > 0 { for index, arg := range os.Args { fmt.Println(index, arg) } } } go build -o \"mflag\" mflag.go os.Args是一个存储命令行参数的字符串切片，它的第一个元素是执行文件的名称。 flag 支持的类型 flag包支持的命令行参数类型有bool、int、int64、uint、uint64、float float64、string、duration。 flag参数 有效值 字符串flag 合法字符串 整数flag 1234、0664、0x1234等类型，也可以是负数。 浮点数flag 合法浮点数 bool类型flag 1, 0, t, f, T, F, true, false, TRUE, FALSE, True, False。 时间段flag 任何合法的时间段字符串。如”300ms”、”-1.5h”、”2h45m”。合法的单位有”ns”、”us” /“µs”、”ms”、”s”、”m”、”h”。 定义 flag.Type() name := flag.StringVar(\"name\", \"jack\", \"姓名\") name:对应类型的指针 flag.TypeVar() 推荐使用 flag.StringVar(&name, \"name\", \"jack\", \"姓名\") flag.IntVar(&age, \"age\", 17, \"年龄\") flag.BoolVar(&married, \"married\", false, \"是否结婚\") flag.DurationVar(&delay, \"delay\", 0, \"时间间隔\") 解析 flag.Parse() -flag xxx （使用空格，一个-符号） --flag xxx （使用空格，两个-符号） -flag=xxx （使用等号，一个-符号） --flag=xxx （使用等号，两个-符号） Flag解析在第一个非flag参数（单个”-“不是flag参数）之前停止 其他函数 flag.Args() ////返回命令行参数后的其他参数，以[]string类型 flag.NArg() //返回命令行参数后的其他参数个数 flag.NFlag() //返回使用的命令行参数个数 使用 package main import ( \"flag\" \"fmt\" \"os\" \"time\" ) func main() { if len(os.Args) > 0 { for index, arg := range os.Args { fmt.Println(index, arg) } } var name string var age int var married bool var delay time.Duration flag.StringVar(&name, \"name\", \"jack\", \"姓名\") flag.IntVar(&age, \"age\", 17, \"年龄\") flag.BoolVar(&married, \"married\", false, \"是否结婚\") flag.DurationVar(&delay, \"delay\", 0, \"时间间隔\") // 解析 flag.Parse() fmt.Println(name, age, married, delay) fmt.Println(flag.Args()) fmt.Println(flag.NArg()) fmt.Println(flag.NFlag()) } Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-04-05 14:12:24 "},"00go/2常用包/3log.html":{"url":"00go/2常用包/3log.html","title":"log","keywords":"","body":"log logger 使用 Print系列(Print|Printf|Println）、Fatal系列（Fatal|Fatalf|Fatalln）、和Panic系列（Panic|Panicf|Panicln） package main import ( \"fmt\" \"log\" ) func main() { v := \"this is log\" log.Print(v) log.Println(v) log.Printf(\"普通日志%s\", v) // Fatal系列函数会在写入日志信息后调用os.Exit(1) log.Fatalln(\"fatal 日志\") log.Panicln(\"panic 日志\") } 配置 log标准库中的Flags函数会返回标准logger的输出配置，而SetFlags函数用来设置标准logger的输出配置。 const ( // 控制输出日志信息的细节，不能控制输出的顺序和格式。 // 输出的日志在每一项后会有一个冒号分隔：例如2009/01/23 01:23:23.123123 /a/b/c/d.go:23: message Ldate = 1 基础配置 配置前缀 配置输出的位置 package main import ( \"fmt\" \"log\" \"os\" ) func main() { logFile, err := os.OpenFile(\"./imatt.log\", os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644) if err != nil { fmt.Println(\"open log file failed, err:\", err) return } log.SetFlags(log.Llongfile | log.Lmicroseconds | log.Ldate) log.SetPrefix(\"[imatt]\") log.SetOutput(logFile) v := \"this is log\" log.Print(v) log.Println(v) log.Printf(\"普通日志%s\", v) // Fatal系列函数会在写入日志信息后调用os.Exit(1) log.Fatalln(\"fatal 日志\") log.Panicln(\"panic 日志\") } 创建logger func New(out io.Writer, prefix string, flag int) *Logger package main import ( \"log\" \"os\" ) func main() { logger := log.New(os.Stdout, \"\", log.Lshortfile|log.Ldate|log.Ltime) logger.Println(\"this is log\") } 第三方日志库 logrus、zap Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-04-05 14:31:21 "},"00go/2常用包/4io.html":{"url":"00go/2常用包/4io.html","title":"io","keywords":"","body":"io os.Stdin：标准输入的文件实例，类型为*File os.Stdout：标准输出的文件实例，类型为*File os.Stderr：标准错误输出的文件实例，类型为*File func Create(name string) (file *File, err Error) 根据提供的文件名创建新的文件，返回一个文件对象，默认权限是0666 func NewFile(fd uintptr, name string) *File 根据文件描述符创建相应的文件，返回一个文件对象 func Open(name string) (file *File, err Error) 只读方式打开一个名称为name的文件 func OpenFile(name string, flag int, perm uint32) (file *File, err Error) 打开名称为name的文件，flag是打开的方式，只读、读写等，perm是权限 func (file *File) Write(b []byte) (n int, err Error) 写入byte类型的信息到文件 func (file *File) WriteAt(b []byte, off int64) (n int, err Error) 在指定位置开始写入byte类型的信息 func (file *File) WriteString(s string) (ret int, err Error) 写入string信息到文件 func (file *File) Read(b []byte) (n int, err Error) 读取数据到b中 func (file *File) ReadAt(b []byte, off int64) (n int, err Error) 从off开始读取数据到b中 func Remove(name string) Error 删除文件名为name的文件 打开关闭文件 package main import ( \"fmt\" \"os\" ) func main() { f, err := os.Open(\"./f.go\") if err != nil { fmt.Println(\"open file failed! err:\", err) return } defer f.Close() } 写文件 package main import ( \"fmt\" \"os\" ) func main() { f, err := os.Create(\"./a.txt\") if err != nil { fmt.Println(\"create file failed, err = \", err) return } defer f.Close() for i := 0; i 读文件 package main import ( \"fmt\" \"io\" \"os\" ) func main() { //var a1 [10]int = [10]int{1, 2, 3, 4} //var a2 []int //// 用于一次添加多个 //a2 = append(a2, a1[:10]...) //fmt.Println(a2) f, err := os.Open(\"./m.txt\") if err != nil { fmt.Println(err) return } defer f.Close() var buf [128]byte var content []byte for { n, err := f.Read(buf[:]) if err == io.EOF { break } if err != nil { fmt.Println(\"read file err \", err) return } content = append(content, buf[:n]...) } fmt.Println(string(content)) } 拷贝文件 package main import ( \"fmt\" \"io\" \"os\" ) // 拷贝文件 func main() { srcF, err := os.Open(\"./m.txt\") if err != nil { fmt.Println(err) return } dstF, err := os.Create(\"./m1.txt\") if err != nil { fmt.Println(err) return } buf := make([]byte, 1024) for { n, err := srcF.Read(buf) if err == io.EOF { break } if err != nil { fmt.Println(err) return } dstF.Write(buf[:n]) } srcF.Close() dstF.Close() } bufio 模式 含义 os.O_WRONLY 只写 os.O_CREATE 创建文件 os.O_RDONLY 只读 os.O_RDWR 读写 os.O_TRUNC 清空 os.O_APPEND 追加 package main import ( \"bufio\" \"fmt\" \"io\" \"os\" ) func w() { f, err := os.OpenFile(\"./m.txt\", os.O_CREATE|os.O_WRONLY, 0777) if err != nil { return } defer f.Close() writer := bufio.NewWriter(f) for i := 0; i ioutil package main import ( \"fmt\" \"io/ioutil\" ) func iw() { err := ioutil.WriteFile(\"m.txt\", []byte(\"www.baidu.com\"), 0777) if err != nil { fmt.Println(err) return } } func ir() { content, err := ioutil.ReadFile(\"m.txt\") if err != nil { return } fmt.Println(string(content)) } func main() { //iw() ir() } Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-04-05 15:38:30 "},"00go/2常用包/9反射.html":{"url":"00go/2常用包/9反射.html","title":"反射","keywords":"","body":"反射 概念 反射：是在程序运行过程中对程序本身进行访问和修改 var i int = 100 i的类型信息是int 值信息是100 类型信息：是静态的元信息，是预先定义好的 值信息：是程序运行过程中动态改变的 接口 接口相关反射 package main import ( \"fmt\" \"log\" \"reflect\" ) // 获取类型 func reflect_type(a interface{}) { t := reflect.TypeOf(a) fmt.Println(t) // 具体类型 k := t.Kind() log.Println(k) switch k { case reflect.String: log.Println(\"string\") case reflect.Int: log.Println(\"int\") default: log.Println(\"nooo\") } } // 获取值 func reflectValue(i interface{}) { v := reflect.ValueOf(i) log.Println(v) k := v.Kind() log.Println(k) switch k { case reflect.Int: log.Println(\"int\") } } // 修改值 func reflectSetValue(i interface{}) { v := reflect.ValueOf(i) log.Println(v) k := v.Kind() log.Println(k) switch k { case reflect.Int: v.SetInt(17) fmt.Println(\"set\", v.Int()) case reflect.Ptr: v.Elem().SetInt(200) // Elem 获取地址指向的变量 fmt.Println(\"set\", v.Elem().Int()) fmt.Println(v.Pointer()) } } func main() { var i = 100 //reflect_type(i) //log.Println(\"-----\") //reflectValue(1000) reflectSetValue(&i) reflectSetValue(\"mattt\") } 结构体 获取字段方法 package main import ( \"fmt\" \"reflect\" ) type A struct { Id int Name string Sex bool } type B struct { A T1 int } func (a A) t1() { fmt.Println(a.Sex, a.Name, a.Id) } func r(a interface{}) { t := reflect.TypeOf(a) fmt.Println(t) fmt.Println(t.Name()) v := reflect.ValueOf(a) fmt.Println(v) for i := 0; i 查看匿名字段 package main import ( \"fmt\" \"reflect\" ) type A struct { Id int Name string Sex bool } type B struct { A T1 int } func (a A) t1() { fmt.Println(a.Sex, a.Name, a.Id) } func main() { b := B{ A: A{ Id: 1, Name: \"matt\", Sex: false, }, T1: 11, } t := reflect.TypeOf(b) fmt.Println(t, t.Field(0)) fmt.Println(reflect.ValueOf(b).Field(0)) } 修改结构体的值 需要使用指针 package main import ( \"fmt\" \"reflect\" ) type S3 struct { Name string Id int } func (s S3) S3M(str string, i int) { fmt.Println(str, i) } func setValue(s interface{}) { v := reflect.ValueOf(s) v = v.Elem() nameF := v.FieldByName(\"Name\") if nameF.Kind() == reflect.String { nameF.SetString(\"matt\") } } // 属性方法 私有获取不到 func main() { s := S3{} //setValue(&s) //fmt.Println(s) } 调用方法 package main import ( \"fmt\" \"reflect\" ) type S3 struct { Name string Id int } func (s S3) S3M(str string, i int) { fmt.Println(str, i) } // 属性方法 私有获取不到 func main() { s := S3{} v := reflect.ValueOf(s) m := v.MethodByName(\"S3M\") args := []reflect.Value{reflect.ValueOf(\"matt\"), reflect.ValueOf(111)} m.Call(args) } 获取字段的 tag package main import ( \"fmt\" \"reflect\" ) type S4 struct { Id int `json:\"id\" db:\"idd\"` } func main() { s := S4{} v := reflect.ValueOf(s) t := v.Type() f := t.Field(0) j := f.Tag.Get(\"json\") fmt.Println(j) } Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-04-07 01:21:11 "},"09tool/docker/docker.html":{"url":"09tool/docker/docker.html","title":"docker","keywords":"","body":" 概述 是什么 一次封装， 到处运行 通过镜像构建一个容器，这个容器会提前安装一些软件 安装卸载 安装 该教程适用centos7 参考手册 https://docs.docker.com/install/linux/docker-ce/centos/ 查看版本号 cat /etc/redhat-release 确保网络正常，安装gcc yum -y install gcc 安装gcc-c++ yum -y install gcc-c++ 卸载旧的版本 yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 安装需要的软件包 yum install -y yum-utils 设置stable镜像仓库 推荐使用这个 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 更新yum软件包索引 yum makecache fast 安装DOCKER CE yum install docker-ce docker-ce-cli containerd.io 启动docker systemctl start docker docker version 测试 docker run hello-world 配置阿里云镜像 mkdir -p /etc/docker vim /etc/docker/daemon.json 阿里云 { \"registry-mirrors\": [\"https://l0s7i35d.mirror.aliyuncs.com\"] } systemctl daemon-reload systemctl restart docker 卸载 systemctl stop docker yum -y remove docker-ce docker-ce-cli containerd.io rm -rf /var/lib/docker rm -rf /var/lib/containerd 错误 可以参考linux 目录下Linux安装和基本配置.md中的磁盘扩容，更好的方式是安装的时候推荐50g空间 空间不足 基本组成 镜像：Docker 镜像（Image）就是一个只读的模板。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。 容器：Docker 利用容器（Container）独立运行的一个或一组应用。容器是用镜像创建的运行实例。 仓库：仓库（Repository）是集中存放镜像文件的场所。 原理 如何工作 Docker是一个Client-Server结构的系统，Docker守护进程运行在主机上， 然后通过Socket连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。 容器，是一个运行时环境，就是我们前面说到的集装箱。 为什么比vm快 (1)docker有着比虚拟机更少的抽象层。由亍docker不需要Hypervisor实现硬件资源虚拟化,运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。 (2)docker利用的是宿主机的内核,而不需要Guest OS。因此,当新建一个容器时,docker不需要和虚拟机一样重新加载一个操作系统内核。仍而避免引寻、加载操作系统内核返个比较费时费资源的过程,当新建一个虚拟机时,虚拟机软件需要加载Guest OS,返个新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统,则省略了返个过程,因此新建一个docker容器只需要几秒钟。 常用命令 帮助命令 docker version docker info docker --help 镜像命令 docker images列出本地主机上的镜像 docker images REPOSITORY：表示镜像的仓库源 TAG：镜像的标签 IMAGE ID：镜像ID CREATED：镜像创建时间 SIZE：镜像大小 [root@iz2zeiw2bqogm8ir3ugpvqz ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE hello-world latest d1165f221234 5 months ago 13.3kB 同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。 如果你不指定一个镜像的版本标签，例如你只使用 ubuntu，docker 将默认使用 ubuntu:latest 镜像 -a :列出本地所有的镜像（含中间映像层） -q :只显示镜像ID。 --digests :显示镜像的摘要信息 --no-trunc :显示完整的镜像信息 docker images --digests docker search docker search:从docker hub中搜，下载从阿里云镜像 docker search [OPTIONS] 镜像名字 https://hub.docker.com docker search tomcat --no-trunc : 显示完整的镜像描述 docker pull docker pull 镜像名字[:TAG] docker pull tomcat docker rmi docker rmi 某个XXX镜像名字ID 删除单个 docker rmi -f 镜像ID 删除多个 docker rmi -f 镜像名1:TAG 镜像名2:TAG 删除全部 docker rmi -f $(docker images -qa) 容器命令 新建并启动容器 docker run [OPTIONS] IMAGE [COMMAND] [ARG...] OPTIONS说明（常用）：有些是一个减号，有些是两个减号 --name=\"容器新名字\": 为容器指定一个名称； -d: 后台运行容器，并返回容器ID，也即启动守护式容器； -i：以交互模式运行容器，通常与 -t 同时使用； -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用； -P: 随机端口映射； -p: 指定端口映射，有以下四种格式 ip:hostPort:containerPort ip::containerPort hostPort:containerPort containerPort docker run -it centos /bin/bash 退出容器 容器不停止 ctrl+p+q 容器停止 exit 列出当前运行的容器 docker ps [OPTIONS] docker ps OPTIONS说明（常用）： -a :列出当前所有正在运行的容器+历史上运行过的 -l :显示最近创建的容器。 -n：显示最近n个创建的容器。 -q :静默模式，只显示容器编号。 --no-trunc :不截断输出。 常用 docker ps -qa docker ps -a 启动容器 //docker start 容器ID或者容器名 docker start centos:centos7 重启容器 docker restart 容器ID或者容器名 停止容器 docker stop 容器ID或者容器名 强制停止容器 docker kill 容器ID或者容器名 删除已停止的容器 docker rm 容器ID 一次删除多个 docker rm -f $(docker ps -a -q) docker ps -a -q | xargs docker rm 进入容器 docker exec -it 容器id /bin/baah docker attach 容器i 重要命令 以后台方式启动 docker run -d 容器名 docker run -d tomcat 查看容器日志 docker logs -f -t --tail 容器ID -t 是加入时间戳 -f 跟随最新的日志打印 --tail 数字 显示最后多少条 查看容器内运行的进程 docker top 容器ID 查看容器内部细节 docker inspect 容器ID 进入正在运行的容器并以命令行交互 docker exec -it 容器ID bashShell docker exec -it xxccc /bin/bash docker attach 容器ID 区别 attach 直接进入容器启动命令的终端，不会启动新的进程 exec 是在容器中打开新的终端，并且可以启动新的进程 从容器内拷贝文件到主机上 docker cp 容器ID:容器内路径 目的主机路径 attach Attach to a running container # 当前 shell 下 attach 连接指定运行镜像 build Build an image from a Dockerfile # 通过 Dockerfile 定制镜像 commit Create a new image from a container changes # 提交当前容器为新的镜像 cp Copy files/folders from the containers filesystem to the host path #从容器中拷贝指定文件或者目录到宿主机中 create Create a new container # 创建一个新的容器，同 run，但不启动容器 diff Inspect changes on a container's filesystem # 查看 docker 容器变化 events Get real time events from the server # 从 docker 服务获取容器实时事件 exec Run a command in an existing container # 在已存在的容器上运行命令 export Stream the contents of a container as a tar archive # 导出容器的内容流作为一个 tar 归档文件[对应 import ] history Show the history of an image # 展示一个镜像形成历史 images List images # 列出系统当前镜像 import Create a new filesystem image from the contents of a tarball # 从tar包中的内容创建一个新的文件系统映像[对应export] info Display system-wide information # 显示系统相关信息 inspect Return low-level information on a container # 查看容器详细信息 kill Kill a running container # kill 指定 docker 容器 load Load an image from a tar archive # 从一个 tar 包中加载一个镜像[对应 save] login Register or Login to the docker registry server # 注册或者登陆一个 docker 源服务器 logout Log out from a Docker registry server # 从当前 Docker registry 退出 logs Fetch the logs of a container # 输出当前容器日志信息 port Lookup the public-facing port which is NAT-ed to PRIVATE_PORT # 查看映射端口对应的容器内部源端口 pause Pause all processes within a container # 暂停容器 ps List containers # 列出容器列表 pull Pull an image or a repository from the docker registry server # 从docker镜像源服务器拉取指定镜像或者库镜像 push Push an image or a repository to the docker registry server # 推送指定镜像或者库镜像至docker源服务器 restart Restart a running container # 重启运行的容器 rm Remove one or more containers # 移除一个或者多个容器 rmi Remove one or more images # 移除一个或多个镜像[无容器使用该镜像才可删除，否则需删除相关容器才可继续或 -f 强制删除] run Run a command in a new container # 创建一个新的容器并运行一个命令 save Save an image to a tar archive # 保存一个镜像为一个 tar 包[对应 load] search Search for an image on the Docker Hub # 在 docker hub 中搜索镜像 start Start a stopped containers # 启动容器 stop Stop a running containers # 停止容器 tag Tag an image into a repository # 给源中镜像打标签 top Lookup the running processes of a container # 查看容器中运行的进程信息 unpause Unpause a paused container # 取消暂停容器 version Show the docker version information # 查看 docker 版本号 wait Block until a container stops, then print its exit code # 截取容器停止时的退出状态值 镜像 是什么 一种可以创建运行环境的软件 特点 Docker镜像都是只读的 当容器启动时，一个新的可写层被加载到镜像的顶部。 这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。 docker commit docker commit提交容器副本使之成为一个新的镜像 docker commit -m=“提交的描述信息” -a=“作者” 容器ID 要创建的目标镜像名:[标签名] docker run -it -p 8080:8080 tomcat cd /usr/local/tomcat/webapps touch a.txt docker commit -m=\"my centos\" -a=\"matt\" xxcc mcentos:1.0 容器数据卷 可以干什么 容器的持久化 容器建继承+共享数据 数据卷 命令 docker run -it -v /宿主机绝对路径目录:/容器内目录 镜像名 # 查看容器是否挂载成功 docker inspect 容器ID # 只读 docker run -it -v /宿主机绝对路径目录:/容器内目录:ro 镜像名 容器停止，对宿主机的修改仍可以对容器发生改变 docker run -it -v /home/matt/dataVolumeContainer:/dataVolumeContainer centos /bin/bash Dockerfile # volume test FROM centos VOLUME [\"/dataVolumeContainer1\",\"/dataVolumeContainer2\"] CMD echo \"finished,--------success1\" CMD /bin/bash docker build -f /home/matt/docker/Dockerfile -t matt/centos . 镜像是千层饼，一个镜像可能多层组成 !> 一段重要的内容，随机端口号 docker run -dit -P 8080 tomcat 宿主机端口随机分配 问题 没遇到 Docker挂载主机目录Docker访问出现cannot open directory .: Permission denied 解决办法：在挂载目录后多加一个--privileged=true参数即可 数据卷容器 是什么 命名的容器挂载数据卷，其它容器通过挂载这个(父容器)实现数据共享，挂载数据卷的容器，称之为数据卷容器 c1 docker run -it --name c1 matt/centos c2继承c1 docker run -it --name c2 --volumes-from c1 matt/centos docker run -it --name c3 --volumes-from c1 matt/centos c2，c3数据就可以共享，即使删除c1，仍然可以共享 容器之间配置信息的传递，数据卷的生命周期一直持续到没有容器使用它为止 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-12 11:54:57 "},"09tool/docker/dockerfile.html":{"url":"09tool/docker/dockerfile.html","title":"dockerfile","keywords":"","body":"dockerfile 是什么 Dockerfile是用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本。 构建过程解析 Dockerfile内容基础知识 每条保留字指令都必须为大写字母且后面要跟随至少一个参数 指令按照从上到下，顺序执行 表示注释 每条指令都会创建一个新的镜像层，并对镜像进行提交 Dockerfile是软件的原材料 Docker镜像是软件的交付品 Docker容器则可以认为是软件的运行态。 大致流程 （1）docker从基础镜像运行一个容器 （2）执行一条指令并对容器作出修改 （3）执行类似docker commit的操作提交一个新的镜像层 （4）docker再基于刚提交的镜像运行一个新容器 （5）执行dockerfile中的下一条指令直到所有指令都执行完成 关键字 关键字 描述 FROM 基础镜像，当前新镜像是基于哪个镜像的 MAINTAINER 镜像维护者的姓名和邮箱地址 RUN 容器构建时需要运行的命令 EXPOSE 当前容器对外暴露出的端口 WORKDIR 指定在创建容器后，终端默认登陆的进来工作目录，一个落脚点 ENV 用来在构建镜像过程中设置环境变量 ADD 将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar压缩包 COPY 类似ADD，拷贝文件和目录到镜像中。将从构建上下文目录中 的文件/目录复制到新的一层的镜像内的 位置 VOLUME 容器数据卷，用于数据保存和持久化工作 CMD 指定一个容器启动时要运行的命令（Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换） ENTRYPOINT 指定一个容器启动时要运行的命令（ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数） ONBUILD 当构建一个被继承的Dockerfile时运行命令，父镜像在被子继承后父镜像的onbuild被触发 ENV MY_PATH /usr/mytest 这个环境变量可以在后续的任何RUN指令中使用，这就如同在命令前面指定了环境变量前缀一样； 也可以在其它指令中直接使用这些环境变量， 比如：WORKDIR $MY_PATH ONBUILD 案例 dockerfile FROM centos:centos7 MAINTAINER matt #把宿主机当前上下文的c.txt拷贝到容器/usr/local/路径下 COPY c.txt /usr/local/cincontainer.txt #把java与tomcat添加到容器中 ADD jdk-8u171-linux-x64.tar.gz /usr/local/ ADD apache-tomcat-9.0.8.tar.gz /usr/local/ #安装vim编辑器 RUN yum -y install vim #设置工作访问时候的WORKDIR路径，登录落脚点 ENV MYPATH /usr/local WORKDIR $MYPATH #配置java与tomcat环境变量 ENV JAVA_HOME /usr/local/jdk1.8.0_171 ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar ENV CATALINA_HOME /usr/local/apache-tomcat-9.0.8 ENV CATALINA_BASE /usr/local/apache-tomcat-9.0.8 ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin #容器运行时监听的端口 EXPOSE 8080 #启动时运行tomcat # ENTRYPOINT [\"/usr/local/apache-tomcat-9.0.8/bin/startup.sh\" ] # CMD [\"/usr/local/apache-tomcat-9.0.8/bin/catalina.sh\",\"run\"] CMD /usr/local/apache-tomcat-9.0.8/bin/startup.sh && tail -F /usr/local/apache-tomcat-9.0.8/bin/logs/catalina.out 构建 docker build -t mcentos:1 # 默认会使用该文件夹Dockerfile 也可以通过-f指定其他文件夹 docker build -f Dockerfile -t mcentos:1 常用软件安装 mysql docker pull mysql:5.6 docker run -p 12345:3306 --name mysql -v /zzyyuse/mysql/conf:/etc/mysql/conf.d -v /zzyyuse/mysql/logs:/logs -v /zzyyuse/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 docker run -p 12345:3306 --name mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 redis docker pull redis:3.2 # --appendonly yes 开启持久化 docker run -p 6379:6379 -v /home/matt/myredis/data:/data -v /home/matt/myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf -d redis:3.2 redis-server /usr/local/etc/redis/redis.conf --appendonly yes aliyun https://dev.aliyun.com/search.html Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-05 13:30:56 "},"09tool/jetBrains.html":{"url":"09tool/jetBrains.html","title":"jetbrain","keywords":"","body":"安装 下一步下一步安装即可，安装时选择要安装的组件，注意存储位置 配置 IDEA配置 常规配置 视图显示 显示菜单字体 控制台字体 设置鼠标滚轮修改字体大小 设置自动导包 忽略大小写 Git配置 maven配置 注释 类的注释 /** @author matt @create ${YEAR}-${MONTH}-${DAY} ${TIME} */ 方法的注释 ** * 功能： * @author matt * @date $date$ $param$ * @return $return$ */ groovyScript(\"def result=''; def params=\\\"${_1}\\\".replaceAll('[\\\\\\\\[|\\\\\\\\]|\\\\\\\\s]', '').split(',').toList(); for(i = 0; i 注释顶格 取消自动更新 终端配置 插件 关闭拼写检查 Spelling spring 复制到idea中的文件需要重构项目否则无法访问404 junit无法从控制台输入 help ->Edit Custom VM Options -Deditable.java.test.console=true properties文件乱码 如果还是不起作用删除文件重新创建即可 pycharm配置 pycharm取消波浪线提示 进入波浪线设置界面看看到上方有三个设置项None、Syntax、Inspections，可以拖动箭头设置。 1.None表示没有波浪线； 2.Syntax表示只有语法错误显示波浪线； 3.Inspections表示语法错误和不符合PEP8规范显示波浪线。 使用 IDEA 打开一个项目 对于maven项目我们打开选择pom.xml即可 常规使用 如何重启 pycharm如何创建一个项目 1.设置新项目名称和存储路径（untitled可以修改）； 2.Project Interpreter设置新建项目所依赖的python环境； ​ 2.1 New environment using 设置新的依赖环境。在项目中新建一个venv（virtualenv）目录，用于存放虚拟的python环境，这里所有的类库依赖都可以直接脱离系统安装的python独立运行； ​ 2.1.1 勾选上Inherit global site-packages则可以使用base interpreter（基础解释器）中安装的第三方库（即本地Python的site-packages目录中的类库）；不选将和外界完全隔离（会在base interpreter的基础上创建一个新的虚拟解释器）； ​ 2.1.2 勾选上Make available to all projects则可以将此项目的虚拟环境提供给其他项目使用； ​ 2.2 Existing Interpreter关联已经存在的python解释器，可以使用该解释器所安装的Python库； 建议选择 New environment using 可以在Base Interpreter选择系统中安装的Python解释器，这样可以让项目独立部署运行，也可以避免一台服务器部署多个项目之间存在类库的版本依赖问。 压缩包引入到IDEA，help->reset auto -> reset goland 配置 使用goland创建项目 clion 如何 多个main函数 CMakeLists.txt cmake_minimum_required(VERSION 3.19) project(demo) set(CMAKE_CXX_STANDARD 14) # 遍历项目根目录下所有的 .cpp 文件 file (GLOB_RECURSE files *.cpp) foreach (file ${files}) string(REGEX REPLACE \".+/(.+)/(.+)\\\\..*\" \"\\\\1-\\\\2\" exe ${file}) add_executable (${exe} ${file}) message (\\ \\ \\ \\ --\\ src/${exe}.cpp\\ will\\ be\\ compiled\\ to\\ bin/${exe}) endforeach () clion 常用配置 导入bits/stdc++.h出错 brew install gcc /opt/homebrew/Cellar/gcc/11.2.0_3/bin/gcc-11 /opt/homebrew/Cellar/gcc/11.2.0_3/bin/g++-11 -D CMAKE_CXX_COMPILER=/opt/homebrew/Cellar/gcc/11.2.0_3/bin/g++-11 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-26 19:44:16 "},"10algorithm/base/0基础算法.html":{"url":"10algorithm/base/0基础算法.html","title":"基础算法","keywords":"","body":"123434 快排 模板 algorithm x j, j+1 l j,j+1 r i-1,i void quick_sort(int q[], int l, int r) { if (l >= r) return; int i = l - 1, j = r + 1, x = q[l + r >> 1]; while (i x); if (i 快速排序 给定你一个长度为 n 的整数数列。 请你使用快速排序对这个数列按照从小到大进行排序。 并将排好序的数列按顺序输出。 输入格式 输入共两行，第一行包含整数 n。 第二行包含 n 个整数（所有整数均在 1∼1091∼109 范围内），表示整个数列。 输出格式 输出共一行，包含 nn 个整数，表示排好序的数列。 数据范围 1≤n≤100000 输入样例： 5 3 1 2 4 5 输出样例： 1 2 3 4 5 code #include #include using namespace std; const int N = 1e5+10; void quick_sort(int q[], int l, int r) { if (l >= r) return; int i = l - 1, j = r + 1, x = q[l + r >> 1]; while (i x); if (i 第k个数 给定一个长度为 nn 的整数数列，以及一个整数 kk，请用快速选择算法求出数列从小到大排序后的第 kk 个数。 输入格式 第一行包含两个整数 nn 和 kk。 第二行包含 n个整数（所有整数均在 1∼1e9 范围内），表示整数数列。 输出格式 输出一个整数，表示数列的第 k 小数。 数据范围 1≤n≤100000, 1≤k≤n 输入样例： 5 3 2 4 1 5 3 输出样例： 3 code #include #include using namespace std; const int N = 1e5+10; int q[N]; int quick_sort(int l, int r, int k) { if (l == r) return q[l]; int i = l - 1, j = r + 1, x = q[l + r >> 1]; while (i x); if (i 归并排序 给定你一个长度为 nn 的整数数列。 请你使用归并排序对这个数列按照从小到大进行排序。 并将排好序的数列按顺序输出。 输入格式 输入共两行，第一行包含整数 nn。 第二行包含 nn 个整数（所有整数均在 1∼1091∼109 范围内），表示整个数列。 输出格式 输出共一行，包含 nn 个整数，表示排好序的数列。 数据范围 1≤n≤1000001≤n≤100000 输入样例： 5 3 1 2 4 5 输出样例： 1 2 3 4 5 #include using namespace std; const int N = 1e5 + 10; int q[N]; void merge_sort(int l, int r) { if (l >= r) return; int mid = (l + r) >> 1; merge_sort(l, mid), merge_sort(mid + 1, r); int i = l, j = mid + 1, k = 0, tmp[N]; while (i > n; for (int i = 0; i > q[i]; merge_sort(0, n - 1); for (int i = 0; i 逆序对数量 给定一个长度为 nn 的整数数列，请你计算数列中的逆序对的数量。 逆序对的定义如下：对于数列的第 ii 个和第 jj 个元素，如果满足 ia[j]a[i]>a[j]，则其为一个逆序对；否则不是。 输入格式 第一行包含整数 nn，表示数列的长度。 第二行包含 nn 个整数，表示整个数列。 输出格式 输出一个整数，表示逆序对的个数。 数据范围 1≤n≤1000001≤n≤100000， 数列中的元素的取值范围 [1,109][1,109]。 输入样例： 6 2 3 4 5 6 1 输出样例： 5 #include #include using namespace std; typedef long long LL; const int N = 1e5 + 10; int q[N]; LL merge_sort(int l, int r) { if (l >= r) return 0; int mid = l + r >> 1; LL res = merge_sort(l, mid) + merge_sort(mid + 1, r); int i = l, j = mid + 1, k = 0, tmp[N]; while (i > n; for (int i = 0; i Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-04-12 08:45:53 "},"10algorithm/base/1数据结构.html":{"url":"10algorithm/base/1数据结构.html","title":"数据结构","keywords":"","body":"链表 单链表 实现一个单链表，链表初始为空，支持三种操作： 向链表头插入一个数； 删除第 kk 个插入的数后面的数； 在第 kk 个插入的数后插入一个数。 现在要对该链表进行 MM 次操作，进行完所有操作后，从头到尾输出整个链表。 注意:题目中第 k 个插入的数并不是指当前链表的第 k 个数。例如操作过程中一共插入了 n 个数，则按照插入的时间顺序，这 n 个数依次为：第 11 个插入的数，第 22 个插入的数，…第 nn 个插入的数。 输入格式 第一行包含整数 M，表示操作次数。 接下来 M 行，每行包含一个操作命令，操作命令可能为以下几种： H x，表示向链表头插入一个数 xx。 D k，表示删除第 kk 个插入的数后面的数（当 kk 为 00 时，表示删除头结点）。 I k x，表示在第 kk 个插入的数后面插入一个数 xx（此操作中 kk 均大于 00）。 输出格式 共一行，将整个链表从头到尾输出。 数据范围 1≤M≤100000 所有操作保证合法。 输入样例： 10 H 9 I 1 1 D 1 D 0 H 6 I 3 6 I 4 5 I 4 5 I 3 4 D 6 输出样例： 6 4 6 5 code #include using namespace std; const int N = 1e5 + 10; int head, e[N], ne[N], idx; void init() { head = -1; } void add_to_head(int x) { e[idx] = x; ne[idx] = head; head = idx++; } void add(int k, int x) { e[idx] = x; ne[idx] = ne[k]; ne[k] = idx++; } void remove(int k) { ne[k] = ne[ne[k]]; } int main() { init(); int m; cin >> m; while (m--) { char ch; int k, x; cin >> ch; if (ch == 'H') { cin >> x; add_to_head(x); } else if (ch == 'I') { cin >> k >> x; add(k - 1, x); } else if(ch == 'D') { cin >> k; if (k == 0) head = ne[head]; else remove(k - 1); } } int t = head; while (t != -1) { cout 双链表 实现一个双链表，双链表初始为空，支持 55 种操作： 在最左侧插入一个数； 在最右侧插入一个数； 将第 kk 个插入的数删除； 在第 kk 个插入的数左侧插入一个数； 在第 kk 个插入的数右侧插入一个数 现在要对该链表进行 MM 次操作，进行完所有操作后，从左到右输出整个链表。 注意:题目中第 kk 个插入的数并不是指当前链表的第 kk 个数。例如操作过程中一共插入了 nn 个数，则按照插入的时间顺序，这 nn 个数依次为：第 11 个插入的数，第 22 个插入的数，…第 nn 个插入的数。 输入格式 第一行包含整数 MM，表示操作次数。 接下来 MM 行，每行包含一个操作命令，操作命令可能为以下几种： L x，表示在链表的最左端插入数 xx。 R x，表示在链表的最右端插入数 xx。 D k，表示将第 kk 个插入的数删除。 IL k x，表示在第 kk 个插入的数左侧插入一个数。 IR k x，表示在第 kk 个插入的数右侧插入一个数。 输出格式 共一行，将整个链表从左到右输出。 数据范围 1≤M≤1000001≤M≤100000 所有操作保证合法。 输入样例： 10 R 7 D 1 L 3 IL 2 10 D 3 IL 2 7 L 8 R 9 IL 4 7 IR 2 2 输出样例： 8 7 7 3 2 9 左 -> 右 code #include using namespace std; const int N = 1e5 + 10; int e[N], l[N], r[N], idx; void init() { r[0] = 1; l[1] = 0; idx = 2; } void add(int k, int x) { e[idx] = x; l[idx] = k; r[idx] = r[k]; l[r[k]] = idx; r[k] = idx++; } void remove(int k) { r[l[k]] = r[k]; l[r[k]] = l[k]; } int main() { init(); int m; cin >> m; while (m--) { string s; int k, x; cin >> s; if (s == \"L\") { cin >> x; add(0, x); } else if (s == \"R\") { cin >> x; add(l[1], x); } else if (s == \"D\") { cin >> k; remove(k + 1); } else if (s == \"IL\") { cin >> k >> x; add(l[k + 1], x); } else { cin >> k >> x; add(k + 1, x); } } int t = r[0]; while (t != 1) { cout 栈 栈 实现一个栈，栈初始为空，支持四种操作： push x – 向栈顶插入一个数 xx； pop – 从栈顶弹出一个数； empty – 判断栈是否为空； query – 查询栈顶元素。 现在要对栈进行 MM 个操作，其中的每个操作 33 和操作 44 都要输出相应的结果。 输入格式 第一行包含整数 MM，表示操作次数。 接下来 MM 行，每行包含一个操作命令，操作命令为 push x，pop，empty，query 中的一种。 输出格式 对于每个 empty 和 query 操作都要输出一个查询结果，每个结果占一行。 其中，empty 操作的查询结果为 YES 或 NO，query 操作的查询结果为一个整数，表示栈顶元素的值。 数据范围 1≤M≤1000001≤M≤100000, 1≤x≤1091≤x≤109 所有操作保证合法。 输入样例： 10 push 5 query push 6 pop query pop empty push 4 query empty 输出样例： 5 5 YES 4 NO code #include #include using namespace std; const int N = 1e5 + 10; int stk[N], tt; int main() { int m; cin >> m; string op; int x; while (m--) { cin >> op; if (op == \"push\") { scanf(\"%d\", &x); stk[++tt] = x; } else if (op == \"pop\") tt--; else if (op == \"query\") printf(\"%d\\n\", stk[tt]); else if (op == \"empty\") { if (tt) cout 表达式求值 给定一个表达式，其中运算符仅包含 +,-,*,/（加 减 乘 整除），可能包含括号，请你求出表达式的最终值。 注意： 数据保证给定的表达式合法。 题目保证符号 - 只作为减号出现，不会作为负号出现，例如，-1+2,(2+2)*(-(1+1)+2) 之类表达式均不会出现。 题目保证表达式中所有数字均为正整数。 题目保证表达式在中间计算过程以及结果中，均不超过 231−1231−1。 题目中的整除是指向 00 取整，也就是说对于大于 00 的结果向下取整，例如 5/3=15/3=1，对于小于 00 的结果向上取整，例如 5/(1−4)=−15/(1−4)=−1。 C++和Java中的整除默认是向零取整；Python中的整除//默认向下取整，因此Python的eval()函数中的整除也是向下取整，在本题中不能直接使用。 输入格式 共一行，为给定表达式。 输出格式 共一行，为表达式的结果。 数据范围 表达式的长度不超过 105105。 输入样例： (2+2)*(1+1) 输出样例： 8 code #include #include #include using namespace std; stack op; stack num; unordered_map pr{ {'+', 1}, {'-', 1}, {'*', 2}, {'/', 2} }; void eval() { int b = num.top(); num.pop(); int a = num.top(); num.pop(); int x = 0; char c = op.top(); op.pop(); if (c == '+') x = a + b; else if (c == '-') x = a - b; else if (c == '*') x = a * b; else x = a / b; num.push(x); } int main() { string str; cin >> str; for (int i = 0; i = pr[str[i]]) eval(); op.push(str[i]); } } while (op.size()) eval(); cout 队列 模拟队列 实现一个队列，队列初始为空，支持四种操作： push x – 向队尾插入一个数 xx； pop – 从队头弹出一个数； empty – 判断队列是否为空； query – 查询队头元素。 现在要对队列进行 MM 个操作，其中的每个操作 33 和操作 44 都要输出相应的结果。 输入格式 第一行包含整数 MM，表示操作次数。 接下来 MM 行，每行包含一个操作命令，操作命令为 push x，pop，empty，query 中的一种。 输出格式 对于每个 empty 和 query 操作都要输出一个查询结果，每个结果占一行。 其中，empty 操作的查询结果为 YES 或 NO，query 操作的查询结果为一个整数，表示队头元素的值。 数据范围 1≤M≤1000001≤M≤100000, 1≤x≤1091≤x≤109, 所有操作保证合法。 输入样例： 10 push 6 empty query pop empty push 3 push 4 pop query push 6 输出样例： NO 6 YES 4 code #include #include using namespace std; const int N = 1e5 + 10; int q[N], hh, tt = -1; int main() { int n; cin >> n; string op; int x; while (n--) { cin >> op; if (op == \"push\") { cin >> x; q[++tt] = x; } else if (op == \"pop\") hh++; else if (op == \"query\") printf(\"%d\\n\", q[hh]); else { if (hh 循环队列 // hh 表示队头，tt表示队尾的后一个位置 int q[N], hh = 0, tt = 0; // 向队尾插入一个数 q[tt ++ ] = x; if (tt == N) tt = 0; // 从队头弹出一个数 hh ++ ; if (hh == N) hh = 0; // 队头的值 q[hh]; // 判断队列是否为空 if (hh != tt) { } 单调栈 给定一个长度为 N 的整数数列，输出每个数左边第一个比它小的数，如果不存在则输出 −1。 输入格式 第一行包含整数 N，表示数列长度。 第二行包含 N 个整数，表示整数数列。 输出格式 共一行，包含 N 个整数，其中第 i 个数表示第 i 个数的左边第一个比它小的数，如果不存在则输出 −1。 数据范围 1≤N≤105 1≤数列中元素≤109 输入样例： 5 3 4 2 7 5 输出样例： -1 3 -1 2 2 #include using namespace std; const int N = 1e5 + 10; int a[N], stk[N], tt; int main() { int n; scanf(\"%d\", &n); for (int i = 0; i = a[i]) tt--; if (tt) printf(\"%d \", stk[tt]); else printf(\"-1 \"); stk[++tt] = a[i]; } } 单调队列 滑动窗口 给定一个大小为 n≤10^6 的数组。 有一个大小为 k 的滑动窗口，它从数组的最左边移动到最右边。 你只能在窗口中看到 k 个数字。 每次滑动窗口向右移动一个位置。 以下是一个例子： 该数组为 [1 3 -1 -3 5 3 6 7]，k 为 3。 窗口位置 最小值 最大值 [1 3 -1] -3 5 3 6 7 -1 3 1 [3 -1 -3] 5 3 6 7 -3 3 1 3 [-1 -3 5] 3 6 7 -3 5 1 3 -1 [-3 5 3] 6 7 -3 5 1 3 -1 -3 [5 3 6] 7 3 6 1 3 -1 -3 5 [3 6 7] 3 7 你的任务是确定滑动窗口位于每个位置时，窗口中的最大值和最小值。 输入格式 输入包含两行。 第一行包含两个整数 n 和 k，分别代表数组长度和滑动窗口的长度。 第二行有 n 个整数，代表数组的具体数值。 同行数据之间用空格隔开。 输出格式 输出包含两个。 第一行输出，从左至右，每个位置滑动窗口中的最小值。 第二行输出，从左至右，每个位置滑动窗口中的最大值。 输入样例： 8 3 1 3 -1 -3 5 3 6 7 输出样例： -1 -3 -3 -3 3 3 3 3 5 5 6 7 code #include using namespace std; const int N = 1e6 + 10; int a[N], q[N], hh = 0, tt = -1; int main() { int n, k; scanf(\"%d %d\", &n, &k); for (int i = 0; i q[hh]) hh++; while (hh = a[i]) tt--; q[++tt] = i; if (i - k + 1 >= 0) printf(\"%d \", a[q[hh]]); } puts(\"\"); hh = 0, tt = -1; for (int i = 0; i q[hh]) hh++; while (hh = 0) printf(\"%d \", a[q[hh]]); } puts(\"\"); return 0; } Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-29 02:53:05 "},"10algorithm/base/2搜索与图论.html":{"url":"10algorithm/base/2搜索与图论.html","title":"搜索与图论","keywords":"","body":"搜索与图论 dfs 排列数字 给定一个整数 n，将数字 1∼n排成一排，将会有很多种排列方法。 现在，请你按照字典序将所有的排列方法输出。 输入格式 共一行，包含一个整数 n。 输出格式 按字典序输出所有排列方案，每个方案占一行。 数据范围 1≤n≤7 输入样例： 3 输出样例： 1 2 3 1 3 2 2 1 3 2 3 1 3 1 2 3 2 1 code #include using namespace std; const int N = 10; int path[N]; bool st[N]; int n; void dfs(int u) { if (u == n) { for (int i = 0; i > n; dfs(0); return 0; } n-皇后问题 n−n−皇后问题是指将 n 个皇后放在 n×n 的国际象棋棋盘上，使得皇后不能相互攻击到，即任意两个皇后都不能处于同一行、同一列或同一斜线上。 现在给定整数 nn，请你输出所有的满足条件的棋子摆法。 输入格式 共一行，包含整数 n。 输出格式 每个解决方案占 n 行，每行输出一个长度为 n 的字符串，用来表示完整的棋盘状态。 其中 . 表示某一个位置的方格状态为空，Q 表示某一个位置的方格上摆着皇后。 每个方案输出完成后，输出一个空行。 注意：行末不能有多余空格。 输出方案的顺序任意，只要不重复且没有遗漏即可。 数据范围 1≤n≤9 输入样例： 4 输出样例： .Q.. ...Q Q... ..Q. ..Q. Q... ...Q .Q.. code 2^(n^2) 每个点都需要访问俩次 #include using namespace std; const int N = 13; char g[N][N]; int n; bool row[N], col[N], dg[2 * N], udg[2 * N]; void dfs(int x, int y, int s) { if (y == n) { x++, y = 0; } if (x == n) { if (s == n) { for (int i = 0; i > n; for (int i = 0; i o(n^2) #include using namespace std; const int N = 13; char g[N][N]; bool col[N], dg[2 * N], udg[2 * N]; int n; void dfs(int u) { if (u == n) { for (int i = 0; i > n; for (int i = 0; i bfs 走迷宫 给定一个 n×m 的二维整数数组，用来表示一个迷宫，数组中只包含 0 或 1，其中 0 表示可以走的路，1 表示不可通过的墙壁。 最初，有一个人位于左上角 (1,1) 处，已知该人每次可以向上、下、左、右任意一个方向移动一个位置。 请问，该人从左上角移动至右下角 (n,m) 处，至少需要移动多少次。 数据保证 (1,1) 处和 (n,m) 处的数字为 0，且一定至少存在一条通路。 输入格式 第一行包含两个整数 n 和 m。 接下来 n 行，每行包含 m 个整数（0 或 1），表示完整的二维数组迷宫。 输出格式 输出一个整数，表示从左上角移动至右下角的最少移动次数。 数据范围 1≤n,m≤100 输入样例： 5 5 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 输出样例： 8 code #include #include #include using namespace std; typedef pair PII; const int N = 110; int g[N][N], d[N][N]; int n, m; void bfs() { queue q; memset(d, -1, sizeof d); d[0][0] = 0; q.push({0, 0}); int dx[4] = {-1, 0, 1, 0}, dy[4] = {0, 1, 0, -1}; while (!q.empty()) { auto t = q.front(); q.pop(); for (int i = 0; i = 0 && x = 0 && y > n >> m; for (int i = 0; i > g[i][j]; } bfs(); return 0; } 八数码 在一个 3×33×3 的网格中，1∼81∼8 这 88 个数字和一个 x 恰好不重不漏地分布在这 3×33×3 的网格中。 例如： 1 2 3 x 4 6 7 5 8 在游戏过程中，可以把 x 与其上、下、左、右四个方向之一的数字交换（如果存在）。 我们的目的是通过交换，使得网格变为如下排列（称为正确排列）： 1 2 3 4 5 6 7 8 x 例如，示例中图形就可以通过让 x 先后与右、下、右三个方向的数字交换成功得到正确排列。 交换过程如下： 1 2 3 1 2 3 1 2 3 1 2 3 x 4 6 4 x 6 4 5 6 4 5 6 7 5 8 7 5 8 7 x 8 7 8 x 现在，给你一个初始网格，请你求出得到正确排列至少需要进行多少次交换。 输入格式 输入占一行，将 3×33×3 的初始网格描绘出来。 例如，如果初始网格如下所示： 1 2 3 x 4 6 7 5 8 则输入为：1 2 3 x 4 6 7 5 8 输出格式 输出占一行，包含一个整数，表示最少交换次数。 如果不存在解决方案，则输出 −1−1。 输入样例： 2 3 4 1 5 x 7 6 8 输出样例 19 code #include #include #include #include using namespace std; int bfs(string start) { int dx[4] = {-1, 0, 1, 0}, dy[4] = {0, 1, 0 , -1}; string end = \"12345678x\"; queue q; q.push(start); unordered_map dist; dist[start] = 0; while (!q.empty()) { auto t = q.front(); q.pop(); int distance = dist[t]; if (t == end) return distance; int k = t.find('x'); int x = k / 3, y = k % 3; for (int i = 0; i = 0 && a = 0 && b > c; start += c; } cout 图的深度优先遍历 树的重心 给定一颗树，树中包含 n 个结点（编号 1∼n）和 n−1 条无向边。 请你找到树的重心，并输出将重心删除后，剩余各个连通块中点数的最大值。 重心定义：重心是指树中的一个结点，如果将这个点删除后，剩余各个连通块中点数的最大值最小，那么这个节点被称为树的重心。 输入格式 第一行包含整数 n，表示树的结点数。 接下来 n−1 行，每行包含两个整数 a 和 b，表示点 a 和点 b 之间存在一条边。 输出格式 输出一个整数 m，表示将重心删除后，剩余各个连通块中点数的最大值。 数据范围 1≤n≤10^5 输入样例 9 1 2 1 7 1 4 2 8 2 5 4 3 3 9 4 6 输出样例： 4 #include #include #include using namespace std; const int N = 1e5 + 10, M = 2 * N; int h[N], e[M], ne[M], idx; bool st[N]; int ans = N, n; void add(int a, int b) { e[idx] = b, ne[idx] = h[a], h[a] = idx++; } int dfs(int u) { int sum = 1, res = 0; st[u] = true; for (int i = h[u]; i != -1; i = ne[i]) { int j = e[i]; if (!st[j]) { int s = dfs(j); res = max(res, s); sum += s; } } res = max(res, n - sum); ans = min(ans, res); return sum; } int main() { cin >> n; memset(h, -1, sizeof h); for (int i = 0; i 图的广度优先遍历 图中点的层次 给定一个 n 个点 m 条边的有向图，图中可能存在重边和自环。 所有边的长度都是 1，点的编号为 1∼n。 请你求出 1 号点到 n 号点的最短距离，如果从 1 号点无法走到 n号点，输出 −1。 输入格式 第一行包含两个整数 n 和 m。 接下来 mm 行，每行包含两个整数 a 和 b，表示存在一条从 a 走到 b 的长度为 1 的边。 输出格式 输出一个整数，表示 1 号点到 n 号点的最短距离。 数据范围 1≤n,m≤10^5 输入样例： 4 5 1 2 2 3 3 4 1 3 1 4 输出样例： 1 #include #include #include using namespace std; const int N = 1e5 + 10; int h[N], e[N], ne[N], idx; int d[N], hh, tt, q[N], n, m; void add(int a, int b) { e[idx] = b, ne[idx] = h[a], h[a] = idx++; } int bfs() { memset(d, -1, sizeof d); q[0] = 1; d[1] = 0; while (hh > n >> m; memset(h, -1, sizeof h); while (m--) { int a, b; scanf(\"%d %d\", &a, &b); add(a, b); } cout n皇后 截距 1111 图： 1.邻间矩阵 二维数组 # a -> b g[a][b] 2.邻间表 很多个单链表 1 -> 3 -> 4 2 -> 3 -> 4 5 -> 4 -> 3 e :值 h 头 ne next idx 边 ans = answer res = result vis = visited dx, dy 方向向量 stk = stack q = queue dummy 链表虚拟头节点 st state 有向图才有拓扑序列 有向无环图：拓扑图 1->2 1->3 2->3 1 2 3 起点在终点前面 1 入度 有几点边指向他 0 出度 有几点边指出去 2 有向图的拓扑序列 给定一个 n 个点 m 条边的有向图，点的编号是 1 到 n，图中可能存在重边和自环。 请输出任意一个该有向图的拓扑序列，如果拓扑序列不存在，则输出 −1。 若一个由图中所有点构成的序列 A 满足：对于图中的每条边 (x,y)，x 在 A 中都出现在 y 之前，则称 A 是该图的一个拓扑序列。 输入格式 第一行包含两个整数 n 和 m。 接下来 m 行，每行包含两个整数 x 和 y，表示存在一条从点 x 到点 y 的有向边 (x,y)。 输出格式 共一行，如果存在拓扑序列，则输出任意一个合法的拓扑序列即可。 否则输出 −1。 数据范围 1≤n,m≤10^5 输入样例： 3 3 1 2 2 3 1 3 输出样例： 1 2 3 #include #include #include using namespace std; const int N = 1e5 + 10; int h[N], e[N], ne[N], idx; int q[N], hh, tt = -1, d[N], n, m; void add(int a, int b) { e[idx] = b, ne[idx] = h[a], h[a] = idx++; } bool tup_sort() { for (int i = 1; i > n >> m; memset(h, -1, sizeof h); while (m--) { int a, b; scanf(\"%d %d\", &a, &b); add(a, b), d[b]++; } if (tup_sort()) { for (int i = 0; i 最短路 最短路 1单源最短路 一个点到其他所有点的最短路 1.1 n:点的数量 m 边的数量 所有边权都是正数（朴素Dijkstra算法 n*n 稠密图、堆优化版的Dijkstra算法 mlogn ） 1.2 存在负权边 （Bellman-Ford nm, SPFA m 最坏 nm） 2多源汇最短路： 起点 终点 不确定 Floyd n^3 稠密图 领接矩阵 朴素dijkstra-Dijkstra求最短路 I 给定一个 n 个点 m 条边的有向图，图中可能存在重边和自环，所有边权均为正值。 请你求出 1 号点到 n 号点的最短距离，如果无法从 1 号点走到 n 号点，则输出 −1。 输入格式 第一行包含整数 n 和 m。 接下来 m 行每行包含三个整数 x,y,z 表示存在一条从点 x 到点 y 的有向边，边长为 z。 输出格式 输出一个整数，表示 1 号点到 n 号点的最短距离。 如果路径不存在，则输出 −1。 数据范围 1≤n≤500, 1≤m≤10^5, 图中涉及边长均不超过10000。 输入样例： 3 3 1 2 2 2 3 1 1 3 4 输出样例： 3 o(n^2 + m) #include #include #include using namespace std; const int N = 510, INF = 0x3f3f3f3f; int g[N][N], d[N], n, m; bool st[N]; int dijkstra() { memset(d, 0x3f, sizeof d); d[1] = 0; for (int i = 0; i > n >> m; int a, b, w; memset(g, 0x3f, sizeof g); while (m--) { scanf(\"%d%d%d\", &a, &b, &w); g[a][b] = min(g[a][b], w); } cout 堆优化-Dijkstra求最短路 II 给定一个 n 个点 m 条边的有向图，图中可能存在重边和自环，所有边权均为非负值。 请你求出 1 号点到 n 号点的最短距离，如果无法从 1 号点走到 n 号点，则输出 −1。 输入格式 第一行包含整数 n 和 m。 接下来 m 行每行包含三个整数 x,y,z，表示存在一条从点 x 到点 y 的有向边，边长为 z。 输出格式 输出一个整数，表示 1 号点到 n 号点的最短距离。 如果路径不存在，则输出 −1。 数据范围 1≤n,m≤1.5×1e5, 图中涉及边长均不小于 0，且不超过 10000。 数据保证：如果最短路存在，则最短路的长度不超过 1e9。 输入样例： 3 3 1 2 2 2 3 1 1 3 4 输出样例： 3 mlogn #include using namespace std; const int N = 150010, INF = 0x3f3f3f3f; typedef pair PII; int h[N], e[N], ne[N], w[N], idx, d[N], n, m; bool st[N]; void add(int a, int b, int c) { w[idx] = c, e[idx] = b, ne[idx] = h[a], h[a] = idx++; } int dijkstra() { memset(d, 0x3f, sizeof d); priority_queue, greater> q; q.push({0, 1}); d[1] = 0; while (!q.empty()) { auto t = q.top(); q.pop(); int distance = t.first, ver = t.second; if (st[ver]) continue; st[ver] = true; for (int i = h[ver]; i != -1; i = ne[i]) { int j = e[i]; if (distance + w[i] > n >> m; int a, b, c; memset(h, -1, sizeof h); while (m--) { scanf(\"%d%d%d\", &a, &b, &c); add(a, b, c); } cout Bellman-ford 算法-有边数限制的最短路 给定一个 n 个点 m 条边的有向图，图中可能存在重边和自环， 边权可能为负数。 请你求出从 1 号点到 n 号点的最多经过 k 条边的最短距离，如果无法从 1 号点走到 n 号点，输出 impossible。 注意：图中可能 存在负权回路 。 输入格式 第一行包含三个整数 n,m,k。 接下来 m 行，每行包含三个整数 x,y,z，表示存在一条从点 x 到点 y 的有向边，边长为 z。 输出格式 输出一个整数，表示从 1 号点到 n 号点的最多经过 k 条边的最短距离。 如果不存在满足条件的路径，则输出 impossible。 数据范围 1≤n,k≤500, 1≤m≤10000, 任意边长的绝对值不超过 10000。 输入样例： 3 3 1 1 2 1 2 3 1 1 3 3 输出样例： 3 bellman_ford o(nm) 允许有负权边 经过1条边最短 经过2条边最短 .... #include #include #include using namespace std; const int N = 510, M = 1e5 + 10; struct Edge { int a, b, w; } e[M]; int dist[N], back[N]; int n, m, k; void bellman_ford() { memset(dist, 0x3f, sizeof dist); dist[1] = 0; for (int i = 0; i 0x3f3f3f / 2) puts(\"impossible\"); else printf(\"%d\", dist[n]); } int main() { scanf(\"%d%d%d\", &n, &m, &k); for (int i = 0; i dijkstra 不能负边 为了避免如下的串联情况， 在边数限制为一条的情况下，节点3的距离应该是3，但是由于串联情况，利用本轮更新的节点2更新了节点3的距离，所以现在节点3的距离是2。 1 2 1 2 3 1 1 3 3 Bellman-Ford：不能有负环 spfa 最短路 -> bellman_ford的队列优化 给定一个 nn 个点 mm 条边的有向图，图中可能存在重边和自环， 边权可能为负数。 请你求出 11 号点到 nn 号点的最短距离，如果无法从 11 号点走到 nn 号点，则输出 impossible。 数据保证不存在负权回路。 输入格式 第一行包含整数 nn 和 mm。 接下来 mm 行每行包含三个整数 x,y,zx,y,z，表示存在一条从点 xx 到点 yy 的有向边，边长为 zz。 输出格式 输出一个整数，表示 11 号点到 nn 号点的最短距离。 如果路径不存在，则输出 impossible。 数据范围 1≤n,m≤1051≤n,m≤105, 图中涉及边长绝对值均不超过 10000 输入样例： 3 3 1 2 5 2 3 -3 1 3 4 输出样例： 2 st:是否在队列中 #include using namespace std; const int N = 1e5 + 10; int h[N], e[N], ne[N], w[N], idx, n, m, d[N]; bool st[N]; void add(int a, int b, int c) { w[idx] = c, e[idx] = b, ne[idx] = h[a], h[a] = idx++; } void spfa() { memset(d, 0x3f, sizeof d); queue q; q.push(1); d[1] = 0; st[1] = true; while (q.size()) { auto t = q.front(); q.pop(); st[t] = false; for (int i = h[t]; i != -1; i = ne[i]) { int j = e[i]; if (d[t] + w[i] > n >> m; memset(h, -1, sizeof h); int a, b, c; while (m--) { scanf(\"%d%d%d\", &a, &b, &c); add(a, b, c); } spfa(); return 0; } 3f3f3f3f 最小生成树 1普利姆算法prim 稠密 朴素 n^2 稀疏 堆优化 mlogn 2克鲁斯卡尔算法 Kruskal 稀疏 mlogm 二分图 染色法 n + m 匈牙利算法 最坏 mn 一般远小于o(mn) 最小生成树：n个城市建公路 最短 二分图：当且仅当图中不奇数环（环有奇数条边） 二分图：集合集合之间有边 集合内部没有边 算法导论：算法证明 prim算法：更新每个值 先更新后面就不会更新了 最小生成树 一个连通图可能有多个生成树。当图中的边具有权值时，总会有一个生成树的边的权值之和小于或者等于其它生成树的边的权值之和 Prim算法求最小生成树 给定一个 n 个点 m 条边的无向图，图中可能存在重边和自环，边权可能为负数。 求最小生成树的树边权重之和，如果最小生成树不存在则输出 impossible。 给定一张边带权的无向图 G=(V,E)，其中 V 表示图中点的集合，E 表示图中边的集合，n=|V|，m=|E|。 由 V 中的全部 n 个顶点和 E 中 n−1 条边构成的无向连通子图被称为 G 的一棵生成树，其中边的权值之和最小的生成树被称为无向图 G 的最小生成树。 输入格式 第一行包含两个整数 n 和 m。 接下来 m 行，每行包含三个整数 u,v,w表示点 u和点 v 之间存在一条权值为 w 的边。 输出格式 共一行，若存在最小生成树，则输出一个整数，表示最小生成树的树边权重之和，如果最小生成树不存在则输出 impossible。 数据范围 1≤n≤500, 1≤m≤10^5, 图中涉及边的边权的绝对值均不超过 10000。 输入样例： 4 5 1 2 1 1 3 2 1 4 3 2 3 2 3 4 4 输出样例： 6 code o(n*n + m) 稠密图 #include #include using namespace std; const int N = 510, INF = 0x3f3f3f3f; int g[N][N], dist[N], n, m, a, b, w; bool st[N]; int prim() { int res = 0; memset(dist, 0x3f, sizeof dist); for (int i = 0; i b -> c c是b的基础来的， 所以需要更新值 // 为下次找最近的点做准备：更新所有点到集合的距离 for (int j = 1; j > n >> m; for (int i = 1; i > a >> b >> w; g[a][b] = g[b][a] = min(g[a][b], w); } int res = prim(); if (res == INF) puts(\"impossible\"); else cout st[t] = true; Kruskal算法求最小生成树 给定一个 n 个点 m 条边的无向图，图中可能存在重边和自环，边权可能为负数。 求最小生成树的树边权重之和，如果最小生成树不存在则输出 impossible。 给定一张边带权的无向图 G=(V,E)，其中 V 表示图中点的集合，E 表示图中边的集合，n=|V|，m=|E|。 由 V 中的全部 n 个顶点和 EE 中 n−1 条边构成的无向连通子图被称为 G 的一棵生成树，其中边的权值之和最小的生成树被称为无向图 G 的最小生成树。 输入格式 第一行包含两个整数 n 和 m。 接下来 m 行，每行包含三个整数 u,v,w，表示点 u 和点 v 之间存在一条权值为 w 的边。 输出格式 共一行，若存在最小生成树，则输出一个整数，表示最小生成树的树边权重之和，如果最小生成树不存在则输出 impossible。 数据范围 1≤n≤1e5, 1≤m≤2∗1e5, 图中涉及边的边权的绝对值均不超过 1000。 输入样例： 4 5 1 2 1 1 3 2 1 4 3 2 3 2 3 4 4 输出样例： 6 code mlogm 稀疏图 #include using namespace std; const int N = 1e5 + 10, M = 2 * N, INF = 0x3f3f3f3f; struct Edge { int a, b, w; bool operator> n >> m; for (int i = 1; i > a >> b >> c; e[i] = {a, b, c}; } sort(e, e + m); int res = kurskal(); if (res == INF) puts(\"impossible\"); else cout m for (int i = 0; i > a >> b >> c; e[i] = {a, b, c}; } if (find(a) != find(b)) { 二分图 染色法判定二分图 给定一个 n 个点 m 条边的无向图，图中可能存在重边和自环。 请你判断这个图是否是二分图。 输入格式 第一行包含两个整数 n 和 m。 接下来 m 行，每行包含两个整数 u 和 v，表示点 u 和点 v 之间存在一条边。 输出格式 如果给定图是二分图，则输出 Yes，否则输出 No。 数据范围 1≤n,m≤10^5 输入样例： 4 4 1 3 1 4 2 3 2 4 输出样例： Yes code i - j i j属于不同的集合 o(n+m) #include #include using namespace std; const int N = 1e5 + 10, M = 2 * N; int h[N], e[M], ne[M], idx; int color[N], n, m; void add(int a, int b) { e[idx] = b, ne[idx] = h[a], h[a] = idx++; } bool dfs(int u, int c) { color[u] = c; for (int i = h[u]; i != -1; i = ne[i]) { int j = e[i]; if (!color[j] && !dfs(j, 3 - c)) return false; else if (color[j] == c) return false; } return true; } int main() { cin >> n >> m; memset(h, -1, sizeof h); while (m--) { int a, b; cin >> a >> b; add(a, b), add(b, a); } bool flag = true; for (int i = 1; i 二分图最大匹配 给定一个二分图，其中左半部包含 n1 个点（编号 1∼n1），右半部包含 n2 个点（编号 1∼n2），二分图共包含 m 条边。 数据保证任意一条边的两个端点都不可能在同一部分中。 请你求出二分图的最大匹配数。 二分图的匹配：给定一个二分图 G，在 G 的一个子图 M 中，M 的边集 {E}中的任意两条边都不依附于同一个顶点，则称 M 是一个匹配。 二分图的最大匹配：所有匹配中包含边数最多的一组匹配被称为二分图的最大匹配，其边数即为最大匹配数。 输入格式 第一行包含三个整数 n1、 n2 和 m。 接下来 m 行，每行包含两个整数 u 和 v，表示左半部点集中的点 u 和右半部点集中的点 v 之间存在一条边。 输出格式 输出一个整数，表示二分图的最大匹配数。 数据范围 1≤n1,n2≤500, 1≤u≤n1, 1≤v≤n2, 1≤m≤10^5 输入样例： 2 2 4 1 1 1 2 2 1 2 2 输出样例： 2 code o(nm) #include #include #include using namespace std; const int N = 510, M = 1e5 + 10; int h[N], e[M], ne[M], idx; int match[N]; bool st[N]; int n1, n2, m; void add(int a, int b) { e[idx] = b, ne[idx] = h[a], h[a] = idx++; } bool find(int u) { for (int i = h[u]; i != -1; i = ne[i]) { int j = e[i]; if (!st[j]) { st[j] = true; if (match[j] == 0 || find(match[j])) { match[j] = u; return true; } } } return false; } int main() { cin >> n1 >> n2 >> m; memset(h, -1, sizeof h); while (m--) { int a, b; scanf(\"%d%d\", &a, &b); add(a, b); } int res = 0; for (int i = 1; i if (!match[j] || find(match[j])) match[j]放弃 test 1 bfs1: n m bfs2 unordered_map 2 bellman_ford: 忘记初始化d 3 prim: st[t] = true kruskal: for (int i = 0; i 3.4 if (match[j] || find(match[j])) {} Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-04-11 10:07:16 "},"10algorithm/base/3数学.html":{"url":"10algorithm/base/3数学.html","title":"数学","keywords":"","body":"数学 质数 试除法判定质数 给定 n 个正整数 ai，判定每个数是否是质数。 输入格式 第一行包含整数 n。 接下来 n 行，每行包含一个正整数 ai。 输出格式 共 n 行，其中第 i 行输出第 i 个正整数 ai 是否为质数，是则输出 Yes，否则输出 No。 数据范围 1≤n≤100,1≤ai≤231−1 输入样例： 2 2 6 输出样例： Yes No #include using namespace std; bool is_prime(int a) { if (a > n; while (n--) { cin >> a; if (is_prime(a)) puts(\"Yes\"); else puts(\"No\"); } return 0; } 分解质因数 给定 n 个正整数 ai，将每个数分解质因数，并按照质因数从小到大的顺序输出每个质因数的底数和指数。 输入格式 第一行包含整数 n。 接下来 n 行，每行包含一个正整数 ai。 输出格式 对于每个正整数 ai，按照从小到大的顺序输出其分解质因数后，每个质因数的底数和指数，每个底数和指数占一行。 每个正整数的质因数全部输出完毕后，输出一个空行。 数据范围 1≤n≤100,2≤ai≤2×109 输入样例： 2 6 8 输出样例： 2 1 3 1 2 3 #include using namespace std; void divide(int x) { for (int i = 2; i 1) cout > n; while (n--) { cin >> x; // cout 筛质数 给定一个正整数 n，请你求出 1∼n 中质数的个数。 输入格式 共一行，包含整数 n。 输出格式 共一行，包含一个整数，表示 1∼n 中质数的个数。 数据范围 1≤n≤106 输入样例： 8 输出样例： 4 code nloglogn #include using namespace std; const int N = 1e6 + 10; int primes[N], cnt; bool st[N]; void get_primes(int n) { for (int i = 2; i > n; get_primes(n); return 0; } o(n) pj 是x的最小质因子 ，所以判断到根号n即可 pj #include using namespace std; const int N = 1e6 + 10; int primes[N], cnt; bool st[N]; void get_primes(int n) { for (int i = 2; i > n; get_primes(n); cout 质数： 大于 1 的整数中 ，如果只包含1和本身这俩个约数 质数的判定1.试除法 分解质因数 试除法 任何数都可以表示成质数的乘积。 N只会被 最小质因子 i%pj == 0 pj 是 I的最小质因子 也是pj * i的最小质因子 i%pj != 0 pj 是 pj * i的最小质因子 对于一个合数x,一定存在最小质因子 假设pj 是x的最小质因子， 当i 美剧导 x/pj 每个数都有一个最小质因子 i是 合数 最小质因子就会停下来 i是质数 pj == i 也会停下来 n + n/2 + n/3 + ... 1 1的倍数 2的倍数 3的倍数 1的约数 2的约束 nlogn 1是所有倍数的约数 int 范围内某个数1500个约数字 222 欧拉函数 1-n 中和n互质的个数 容质原理： 等价 n更好i 400w - 500w 34 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-30 08:30:49 "},"20db/0mysql基础/0mysql概述.html":{"url":"20db/0mysql基础/0mysql概述.html","title":"概述","keywords":"","body":"概述 持久化 持久化(persistence): 把数据保存到可掉电式存储设备中以供之后使用 掉电设备：数据库、文件、其他 数据库概念 数据库 db 存储数据 DBMS:数据库管理系统(Database Management System) 对数据库进行统一管理和控制 SQL:结构化查询语言(Structured Query Language) 专门用来与数据库通信的语言 数据库分类 关系型数据库 概述 这种类型的数据库是 最古老 的数据库类型，关系型数据库模型是把复杂的数据结构归结为简单的 二元关系 (即二维表格形式)。 优缺点 复杂查询 可以用SQL语句方便的在一个表以及多个表之间做非常复杂的数据查询。 事务支持 使得对于安全性能很高的数据访问要求得以实现。 非关系型数据库 非关系型数据库，可看成传统关系型数据库的功能 阉割版本 ，基于键值对存储数据，不需要经过SQL层 的解析， 性能非常高 。同时，通过减少不常用的功能，进一步提高性能。 键值数据库 文档数据库 搜索引擎数据库 列式数据库 安装 mac 安装 官网：https://dev.mysql.com/downloads/mysql/ 一步一步安装即可 这里选择5.x加密 设置root密码 启动 在系统偏好设置里启动或者关闭服务 配置 vim .zshrc # mysql PATH=$PATH:/usr/local/mysql/bin export PATH Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-04-23 01:22:03 "},"31bigdata/flink/0概述.html":{"url":"31bigdata/flink/0概述.html","title":"概述","keywords":"","body":"基本概念 是什么 Apache Flink 是一个框架和分布式处理引擎，用于对无界和有界数据流进行状态计算 特点 支持事件时间(event-time)和处理时间(processing-time) 语义 精确一次(exactly-once)的状态一致性保证 低延迟，每秒处理数百万个事件，毫秒级延迟 与众多常用存储系统的连接 高可用，动态扩展，实现7*24小时全天候运行 Flink VS Spark Streaming 数据模型 spark 采用 RDD 模型，spark streaming 的 DStream 实际上也就是一组组小批 数据 RDD 的集合 flink 基本数据模型是数据流，以及事件(Event)序列 运行时架构 spark 是批计算，将 DAG 划分为不同的 stage，一个完成后才可以计算下一个 flink 是标准的流执行模式，一个事件在一个节点处理完后可以直接发往下一个节点进行处理 迟到的数据放入侧边流 8-9 数据产生8-9 处理的时候9:01 85763421 俩个通可以同时存在 [1,5] 1 2 4 3 5 [6,10] 6 7 8 t=3 8 来了关【1-5】的窗 wm传递：最小的值 在并行度不为1的时候： 下以上游分区woater mark最小值 状态保存在本地内存中， 检查点 taskmanager state.backend: filesystem 检查点 a b 时间还没到检查点a, 正在处理1 1 2会被丢弃 黄1 蓝1 1 2 3 4 保存1 5 6 // 前一次保存结束 到下一次保存开始 setMinPauseBetweenCheckpoints 可能检查点到了但是没超过这个数字 也不保存 端到端：输入源头 flink 幂等写入：最终一致性 5 10 15 5 10 15 5保存 预写日志(WAL):往外部系统写入并不能保证事务 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-19 17:36:35 "},"31bigdata/flink/1快速上手.html":{"url":"31bigdata/flink/1快速上手.html","title":"快速上手","keywords":"","body":"快速开始 2.1 pom.xml 4.0.0 com.matt study-flink 1.0-SNAPSHOT 8 8 org.apache.flink flink-java 1.10.1 org.apache.flink flink-streaming-java_2.12 1.10.1 org.apache.flink flink-connector-kafka-0.11_2.12 1.10.1 org.apache.bahir flink-connector-redis_2.11 1.0 org.apache.flink flink-connector-elasticsearch6_2.12 1.10.1 mysql mysql-connector-java 5.1.44 org.apache.flink flink-statebackend-rocksdb_2.12 1.10.1 2.2批处理wordcount com.matt.wc.WordCount 输出结构 (are,1) (tks,1) (how,1) (fink,1) (spark,1) (you,1) (matt,1) (hello,3) 2.3 流处理 com.matt.wc.StreamWordCount 部署 mac-Standalone 模式 修改文件所有者 # 查看当前用户 whoami # 查看用户组 id matt # 用户名：组名 chown -R matt:staff /opt/software chown -R matt:staff /opt/module 查看更多用户相关信息 https://blog.csdn.net/qq_26129413/article/details/109675386 解压 tar -zxvf flink-1.10.1-bin-scala_2.12.tgz -C /opt/module 配置 修改 flink/conf/flink-conf.yaml 文件 如果是单机安装默认即可，集群安装配置某台主机 jobmanager.rpc.address: localhost 修改 /conf/slaves 文件 vim slaves 从机机器列表 localhost 如果是集群安装可以配置为 matt06 matt07 如果是集群安装需要把flink-1.10.1同步到其他机器 启动 start-cluster.sh stop-cluster.sh 提交任务 可以通过命令行提交也可以ui进行提交 ./flink run -c com.matt.wc.StreamWordCount –p 2 FlinkTutorial-1.0-SNAPSHOT-jar-with-dependencies.jar --host lcoalhost –port 777 http://localhost:8081/#/overview TODO yarn/k8s部署 运行架构 Flink运行时的组件 作业管理器(JobManager） 控制一个应用程序执行的主进程，也就是说，每个应用程序都会被一个不同的 JobManager 所控制执行。 JobManager 会先接收到要执行的应用程序，这个应用程序会包括:作业图 (JobGraph)、逻辑数据流图(logical dataflow graph)和打包了所有的类、 库和其它资源的JAR包。 JobManager 会把JobGraph转换成一个物理层面的数据流图，这个图被叫做 “执行图”(ExecutionGraph)，包含了所有可以并发执行的任务。 JobManager 会向资源管理器(ResourceManager)请求执行任务必要的资源， 也就是任务管理器(TaskManager)上的插槽(slot)。一旦它获取到了足够的 资源，就会将执行图分发到真正运行它们的TaskManager上。而在运行过程中， JobManager会负责所有需要中央协调的操作，比如说检查点(checkpoints) 的协调。 任务管理器(TaskManager） Flink中的工作进程。通常在Flink中会有多个TaskManager运行，每一 个TaskManager都包含了一定数量的插槽（slots）。插槽的数量限制 了TaskManager能够执行的任务数量。 启动之后，TaskManager会向资源管理器注册它的插槽；收到资源管理 器的指令后，TaskManager就会将一个或者多个插槽提供给 JobManager调用。JobManager就可以向插槽分配任务（tasks）来 执行了。 在执行过程中，一个TaskManager可以跟其它运行同一应用程序的 TaskManager交换数据。 资源管理器（ResourceManager） ​ 主要负责管理任务管理器（TaskManager）的插槽（slot），TaskManger 插槽是 Flink 中 定义的处理资源单元。Flink 为不同的环境和资源管理工具提供了不同资源管理器，比如 YARN、Mesos、K8s，以及 standalone 部署。当 JobManager 申请插槽资源时，ResourceManager 会将有空闲插槽的 TaskManager 分配给 JobManager。如果 ResourceManager 没有足够的插槽 来满足 JobManager 的请求，它还可以向资源提供平台发起会话，以提供启动 TaskManager 进程的容器。另外，ResourceManager 还负责终止空闲的 TaskManager，释放计算资源。 分发器（Dispatcher） 可以跨作业运行，它为应用提交提供了REST接口。 当一个应用被提交执行时，分发器就会启动并将应用移交给一个 JobManager。 Dispatcher也会启动一个Web UI，用来方便地展示和监控作业 执行的信息。 Dispatcher在架构中可能并不是必需的，这取决于应用提交运行 的方式。 任务提交流程 任务提交流程（yarn）-TODO 任务调度原理 ​ 客户端不是运行时和程序执行 的一部分，但它用于准备并发送 dataflow(JobGraph)给 Master(JobManager)，然后，客户端断开连接或者维持连接以 等待接收计算结果。 ​ 当 Flink 集 群 启 动 后 ， 首 先 会 启 动 一 个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。 TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。 ​ Client 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境 连通即可）。提交 Job 后，Client 可以结束进程（Streaming 的任务），也可以不 结束并等待结果返回。 ​ JobManager 主 要 负 责 调 度 Job 并 协 调 Task 做 checkpoint， 职 责 上 很 像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的 执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。 ​ TaskManager 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自 己的上游建立 Netty 连接，接收数据并处理。 并行度 一个特定算子的 子任务（subtask）的个数被称之为其并行度（parallelism）。 一般情况下，一个 stream 的并行度，可以认为就是其所有算子中最大的并行度。 TaskManager 和 Slots ​ Flink 中每一个 worker(TaskManager)都是一个 JVM 进程，它可能会在独立的线 程上执行一个或多个 subtask。为了控制一个 worker 能接收多少个 task，worker 通 过 task slot 来进行控制（一个 worker 至少有一个 task slot）。 ​ 每个 task slot 表示 TaskManager 拥有资源的一个固定大小的子集。假如一个 TaskManager 有三个 slot，那么它会将其管理的内存分成三份给各个 slot。资源 slot 化意味着一个 subtask 将不需要跟来自其他 job 的 subtask 竞争被管理的内存，取而 代之的是它将拥有一定数量的内存储备。需要注意的是，这里不会涉及到 CPU 的隔 离，slot 目前仅仅用来隔离 task 的受管理的内存。 ​ ​ 通过调整 task slot 的数量，允许用户定义 subtask 之间如何互相隔离。如果一个 TaskManager 一个 slot，那将意味着每个 task group 运行在独立的 JVM 中（该 JVM 可能是通过一个特定的容器启动的），而一个 TaskManager 多个 slot 意味着更多的 subtask 可以共享同一个 JVM。而在同一个 JVM 进程中的 task 将共享 TCP 连接（基 于多路复用）和心跳消息。它们也可能共享数据集和数据结构，因此这减少了每个 task 的负载。 Flink 中每一个 TaskManager 都是一个JVM进程，它可能会在独立的线程上执 行一个或多个子任务 为了控制一个 TaskManager 能接收多少个 task， TaskManager 通过 task slot 来进行控制（一个 TaskManager 至少有一个 slot） ​ 默认情况下，Flink 允许子任务共享 slot，即使它们是不同任务的子任务（前提 是它们来自同一个 job）。 这样的结果是，一个 slot 可以保存作业的整个管道。 ​ Task Slot 是静态的概念，是指 TaskManager 具有的并发执行能力，可以通过 参数 taskmanager.numberOfTaskSlots 进行配置；而并行度 parallelism 是动态概念， 即 TaskManager 运行程序时实际使用的并发能力，可以通过参数 parallelism.default 进行配置。 ​ 也就是说，假设一共有 3 个 TaskManager，每一个 TaskManager 中的分配 3 个 TaskSlot，也就是每个 TaskManager 可以接收 3 个 task，一共 9 个 TaskSlot，如果我 们设置 parallelism.default=1，即运行程序默认的并行度为 1，9 个 TaskSlot 只用了 1 个，有 8 个空闲，因此，设置合适的并行度才能提高效率。 并行子任务的分配 程序与数据流（DataFlow） 所有的Flink程序都是由三部分组成的： Source 、Transformation 和 Sink。Source 负责读取数据源，Transformation 利用各种算子进行处理加工，Sink负责输出 在运行时，Flink上运行的程序会被映射成“逻辑数据流”（dataflows），它包 含了这三部分 每一个dataflow以一个或多个sources开始以一个或多个sinks结束。dataflow 类似于任意的有向无环图（DAG） 在大部分情况下，程序中的转换运算（transformations）跟dataflow中的算子 （operator）是一一对应的关系 执行图（ExecutionGraph） Flink 中的执行图可以分成四层：StreamGraph -> JobGraph -> ExecutionGraph -> 物理执行图 ➢ StreamGraph：是根据用户通过 Stream API 编写的代码生成的最初的图。用来 表示程序的拓扑结构。 ➢ JobGraph：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点 ➢ ExecutionGraph：JobManager 根据 JobGraph 生成ExecutionGraph。 ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。 ➢ 物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个 TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。 数据传输形式 一个程序中，不同的算子可能具有不同的并行度 算子之间传输数据的形式可以是 one-to-one (forwarding) 的模式也可以是 redistributing 的模式，具体是哪一种形式，取决于算子的种类 ➢ One-to-one：stream维护着分区以及元素的顺序（比如source和map之间）。 这意味着map 算子的子任务看到的元素的个数以及顺序跟 source 算子的子任务 生产的元素的个数、顺序相同。map、fliter、flatMap等算子都是one-to-one 的对应关系。 ➢ Redistributing：stream的分区会发生改变。每一个算子的子任务依据所选择的 transformation发送数据到不同的目标任务。例如，keyBy 基于 hashCode 重 分区、而 broadcast 和 rebalance 会随机重新分区，这些算子都会引起 redistribute过程，而 redistribute 过程就类似于 Spark 中的 shuffle 过程。 任务链（Operator Chains） Flink 采用了一种称为任务链的优化技术，可以在特定条件下减少本地通信的开销。为了满足任务链的要求，必须将两个或多个算子设为相同 的并行度，并通过本地转发（local forward）的方式进行连接 相同并行度的 one-to-one 操作，Flink 这样相连的算子链接在一起形 成一个 task，原来的算子成为里面的 subtask 并行度相同、并且是 one-to-one 操作，两个条件缺一不可 好处 它能减少线 程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量。链接的行为可以在编程 API 中进行指定。 //基于数据流进行转换计算 DataStream> resultStream = inputDataStream.flatMap(new WordCount.MyFlatMapper()) .keyBy(0) .sum(1).setParallelism(2).startNewChain(); // 和前后都不合并任务 // .disableChaining(); // 开始一个新的任务链合并 前面断开后面不断开 // .startNewChain() Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-19 17:36:42 "},"31bigdata/flink/2API.html":{"url":"31bigdata/flink/2API.html","title":"api","keywords":"","body":"API Environment getExecutionEnvironment 这个比较常用 ​ 创建一个执行环境，表示当前执行程序的上下文。 如果程序是独立调用的，则 此方法返回本地执行环境；如果从命令行客户端调用程序以提交到集群，则此方法 返回此集群的执行环境，也就是说，getExecutionEnvironment 会根据查询运行的方 式决定返回什么样的运行环境，是最常用的一种创建执行环境的方式。 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); createLocalEnvironment 返回本地执行环境，需要在调用时指定默认的并行度。 LocalStreamEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(1); createRemoteEnvironment 返回集群执行环境，将 Jar 提交到远程服务器。需要在调用时指定 JobManager 的 IP 和端口号，并指定要在集群中运行的 Jar 包。 StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(\"jobmanage-hostname\", 6123, \"YOURPATH//WordCount.jar\"); source 从集合读取数据 com.matt.apitest.source package com.matt.apitest.source; import com.matt.apitest.beans.SensorReading; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import java.util.Arrays; /** * @author matt * @create 2022-01-16 14:36 */ public class SourceTest1_Collection { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); DataStream dataStream = env.fromCollection(Arrays.asList( new SensorReading(\"sensor_1\", 1547718199L, 35.8), new SensorReading(\"sensor_6\", 1547718201L, 15.4), new SensorReading(\"sensor_7\", 1547718202L, 6.7), new SensorReading(\"sensor_10\", 1547718205L, 38.1) )); dataStream.print(\"collection\"); // job name env.execute(\"my\"); } } 从文件读取数据 package com.matt.apitest.source; import com.matt.apitest.beans.SensorReading; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import java.util.Arrays; /** * @author matt * @create 2022-01-16 14:54 */ public class SourceTest2_File { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //env.setParallelism(1); // /Users/matt/workspace/java/bigdata DataStream dataStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); dataStream.print(\"file\"); // job name env.execute(\"my\"); } } 以 kafka 消息队列的数据作为来源 org.apache.flink flink-connector-kafka-0.11_2.12 1.10.1 package com.matt.apitest.source; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.serialization.SimpleStringSchema; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011; import java.util.Arrays; import java.util.Properties; /** * @author matt * @create 2022-01-16 15:05 */ public class SourceTest3_Kafka { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // Properties properties = new Properties(); properties.setProperty(\"bootstrap.servers\", \"matt05:9092\"); properties.setProperty(\"group.id\", \"consumer-group\"); properties.setProperty(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); properties.setProperty(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); properties.setProperty(\"auto.offset.reset\", \"latest\"); DataStream dataStream = env.addSource( new FlinkKafkaConsumer011(\"sensor\", new SimpleStringSchema(), properties)); dataStream.print(\"kafka\"); // job name env.execute(\"kafak_job\"); } } 自定义 Source 实现SourceFunction接口 package com.matt.apitest.source; import com.matt.apitest.beans.SensorReading; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.source.SourceFunction; import java.util.HashMap; import java.util.Properties; import java.util.Random; /** * @author matt * @create 2022-01-16 15:50 */ public class SourceTest4_UDF { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); DataStream dataStream = env.addSource(new MySensorSouce()); dataStream.print(\"kafka\"); // job name env.execute(\"kafak_job\"); } public static class MySensorSouce implements SourceFunction { // 标志位 控制数据的产生 private boolean running = true; /** * 功能： * * @param ctx * @return void * @author matt * @date 2022/1/16 */ @Override public void run(SourceContext ctx) throws Exception { // 随机数发生器 Random random = new Random(); HashMap sensorTempMap = new HashMap(); for (int i = 0; i Transform map flatmap filter package com.matt.apitest.transform; import org.apache.flink.api.common.functions.FilterFunction; import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.util.Collector; import java.io.File; /** * @author matt * @create 2022-01-16 17:16 */ public class TransFormTest1_Base { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); // 1 map string -> len(string) // 方-》园 DataStream mapStream = inputStream.map(new MapFunction() { @Override public Integer map(String s) throws Exception { return s.length(); } }); // mapStream.print(\"map\"); // flatMap 按逗号切分字端 DataStream flatMapStream = inputStream.flatMap(new FlatMapFunction() { @Override public void flatMap(String s, Collector collector) throws Exception { String[] fields = s.split(\",\"); for (String field : fields) { collector.collect(field); } } }); // 3.filter 过滤 筛选某个数据 DataStream filterStream = inputStream.filter(new FilterFunction() { @Override public boolean filter(String s) throws Exception { // true 要 false 不要这个数据 return s.startsWith(\"sensor_1\"); } }); mapStream.print(\"map\"); flatMapStream.print(\"flatMap\"); filterStream.print(\"filter\"); // job name env.execute(\"trans-form\"); } } keyedBy DataStream → KeyedStream：逻辑地将一个流拆分成不相交的分区，每个分 区包含具有相同 key 的元素，在内部以 hash 的形式实现的。 package com.matt.apitest.transform; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.functions.FilterFunction; import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.KeyedStream; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.util.Collector; /** * @author matt * @create 2022-01-17 22:14 */ public class TransFormTest1_RollingAggregation { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); // SensorReading //DataStream dataStream = inputStream.map(new MapFunction() { // // @Override // public SensorReading map(String value) throws Exception { // String[] fields = value.split(\",\"); // return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); // } //}); DataStream dataStream = inputStream.map( s -> { String[] fields = s.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); //分组 KeyedStream keyedStream = dataStream.keyBy(\"id\"); keyedStream.print(\"keyed\"); // job name env.execute(\"trans-form\"); } } 滚动聚合算子（Rolling Aggregation） 这些算子可以针对 KeyedStream 的每一个支流做聚合。 sum() min() max() minBy() maxBy() max只更新统计的字段 maxBy:非max字段使用最大的值字段那条记录的字段 package com.matt.apitest.transform; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.functions.FilterFunction; import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.KeyedStream; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.util.Collector; /** * @author matt * @create 2022-01-17 22:14 */ public class TransFormTest1_RollingAggregation { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream dataStream = inputStream.map( s -> { String[] fields = s.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); //分组 KeyedStream keyedStream = dataStream.keyBy(\"id\"); // KeyedStream keyedStream1 = dataStream.keyBy(d -> d.getId()); // 滚动聚合 // max 当前的字段 maxBy // maxBy 最大的那条记录 没有她大 非max字段也要改 // max 最大值那条字段 // DataStream resultStream = keyedStream.max(\"temperatrue\"); //resultStream.print(); keyedStream.print(\"keyed\"); // job name env.execute(\"trans-form\"); } } reduce KeyedStream → DataStream：一个分组数据流的聚合操作，合并当前的元素 和上次聚合的结果，产生一个新的值，返回的流中包含每一次聚合的结果，而不是 只返回最后一次聚合的最终结果 package com.matt.apitest.transform; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.functions.ReduceFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.KeyedStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; /** * @author matt * @create 2022-01-17 22:48 */ public class TransFormTest3 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream dataStream = inputStream.map(s -> { String[] fields = s.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); //分组 KeyedStream keyedStream = dataStream.keyBy(\"id\"); DataStream resultStream = keyedStream.reduce(new ReduceFunction() { @Override public SensorReading reduce(SensorReading v1, SensorReading v2) throws Exception { return new SensorReading(v1.getId(), v2.getTimestamp(), Math.max(v1.getTemperatrue(), v2.getTemperatrue())); } }); resultStream.print(); // job name env.execute(\"trans-form\"); } } split select DataStream → SplitStream：根据某些特征把一个 DataStream 拆分成两个或者 多个 DataStream。 SplitStream→DataStream：从一个 SplitStream 中获取一个或者多个 DataStream。 需求：传感器数据按照温度高低（以 30 度为界），拆分成两个流。 拆分 选择 package com.matt.apitest.transform; import com.matt.apitest.beans.SensorReading; import org.apache.commons.math3.geometry.partitioning.SubHyperplane; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.common.functions.ReduceFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.streaming.api.collector.selector.OutputSelector; import org.apache.flink.streaming.api.datastream.*; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.co.CoMapFunction; import java.util.Collections; /** * @author matt * @create 2022-01-17 23:30 */ public class TransFromTest4_MultiplStreams { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream inputStream = env.readTextFile(\"D:\\\\matt\\\\workspace\\\\idea\\\\hadoop\\\\study-flink\\\\src\\\\main\\\\resources\\\\sensor.txt\"); DataStream dataStream = inputStream.map(s -> { String[] fields = s.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); //1. 分流 SplitStream splitStream = dataStream.split(new OutputSelector() { @Override public Iterable select(SensorReading sensorReading) { return (sensorReading.getTemperatrue() > 30) ? Collections.singletonList(\"high\") : Collections.singletonList(\"low\") ; } }); DataStream highTempStream = splitStream.select(\"high\"); DataStream lowTempStream = splitStream.select(\"low\"); highTempStream.print(); System.out.println(\"-------\"); lowTempStream.print(); // job name env.execute(\"trans-form\"); } } Connect 和 CoMap DataStream,DataStream → ConnectedStreams：连接两个保持他们类型的数 据流，两个数据流被 Connect 之后，只是被放在了一个同一个流中，内部依然保持 各自的数据和形式不发生任何变化，两个流相互独立。 ConnectedStreams → DataStream：作用于 ConnectedStreams 上，功能与 map 和 flatMap 一样，对 ConnectedStreams 中的每一个 Stream 分别进行 map 和 flatMap 处理。 connetc 将俩个流放到一个流中 package com.matt.apitest.transform; import com.matt.apitest.beans.SensorReading; import org.apache.commons.math3.geometry.partitioning.SubHyperplane; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.common.functions.ReduceFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.streaming.api.collector.selector.OutputSelector; import org.apache.flink.streaming.api.datastream.*; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.co.CoMapFunction; import java.util.Collections; /** * @author matt * @create 2022-01-17 23:30 */ public class TransFromTest4_MultiplStreams { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream dataStream = inputStream.map(s -> { String[] fields = s.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); //1. 分流 SplitStream splitStream = dataStream.split(new OutputSelector() { @Override public Iterable select(SensorReading sensorReading) { return (sensorReading.getTemperatrue() > 30) ? Collections.singletonList(\"high\") : Collections.singletonList(\"low\") ; } }); DataStream highTempStream = splitStream.select(\"high\"); DataStream lowTempStream = splitStream.select(\"low\"); highTempStream.print(); System.out.println(\"-------\"); lowTempStream.print(); System.out.println(\"河流\"); DataStream> warningStream = highTempStream.map(new MapFunction>() { @Override public Tuple2 map(SensorReading v) throws Exception { return new Tuple2<>(v.getId(), v.getTemperatrue()); } }); ConnectedStreams, SensorReading> connectedStreams = warningStream.connect(lowTempStream); SingleOutputStreamOperator res = connectedStreams.map(new CoMapFunction, SensorReading, Object>() { @Override public Object map1(Tuple2 v) throws Exception { return new Tuple3<>(v.f0, v.f1, \"high temp warning\"); } @Override public Object map2(SensorReading v) throws Exception { return new Tuple2<>(v.getId(), \"normal\"); } }); res.print(); System.out.println(\"union\"); DataStream union = highTempStream.union(lowTempStream); union.print(); // job name env.execute(\"trans-form\"); } } Union DataStream → DataStream：对两个或者两个以上的 DataStream 进行 union 操 作，产生一个包含所有 DataStream 元素的新 DataStream。 Connect 与 Union 区别 Union 之前两个流的类型必须是一样，Connect 可以不一样，在之后的 coMap 中再去调整成为一样的。 Connect 只能操作两个流，Union 可以操作多个。 package com.matt.apitest.transform; import com.matt.apitest.beans.SensorReading; import org.apache.commons.math3.geometry.partitioning.SubHyperplane; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.common.functions.ReduceFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.streaming.api.collector.selector.OutputSelector; import org.apache.flink.streaming.api.datastream.*; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.co.CoMapFunction; import java.util.Collections; /** * @author matt * @create 2022-01-17 23:30 */ public class TransFromTest4_MultiplStreams { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream dataStream = inputStream.map(s -> { String[] fields = s.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); //1. 分流 SplitStream splitStream = dataStream.split(new OutputSelector() { @Override public Iterable select(SensorReading sensorReading) { return (sensorReading.getTemperatrue() > 30) ? Collections.singletonList(\"high\") : Collections.singletonList(\"low\") ; } }); DataStream highTempStream = splitStream.select(\"high\"); DataStream lowTempStream = splitStream.select(\"low\"); highTempStream.print(); System.out.println(\"-------\"); lowTempStream.print(); System.out.println(\"河流\"); DataStream> warningStream = highTempStream.map(new MapFunction>() { @Override public Tuple2 map(SensorReading v) throws Exception { return new Tuple2<>(v.getId(), v.getTemperatrue()); } }); ConnectedStreams, SensorReading> connectedStreams = warningStream.connect(lowTempStream); SingleOutputStreamOperator res = connectedStreams.map(new CoMapFunction, SensorReading, Object>() { @Override public Object map1(Tuple2 v) throws Exception { return new Tuple3<>(v.f0, v.f1, \"high temp warning\"); } @Override public Object map2(SensorReading v) throws Exception { return new Tuple2<>(v.getId(), \"normal\"); } }); res.print(); System.out.println(\"union\"); DataStream union = highTempStream.union(lowTempStream); union.print(); // job name env.execute(\"trans-form\"); } } 数据类型 Flink 支持所有的 Java 和 Scala 基础数据类型，Int, Double, Long, String, … DataStream numberStream = env.fromElements(1, 2, 3, 4); numberStream.map(data -> data * 2); Java 和 Scala 元组（Tuples） DataStream> personStream = env.fromElements( new Tuple2(\"Adam\", 17), new Tuple2(\"Sarah\", 23) ); personStream.filter(p -> p.f1 > 18); Scala 样例类（case classes） case class Person(name: String, age: Int) val persons: DataStream[Person] = env.fromElements( Person(\"Adam\", 17), Person(\"Sarah\", 23) ) persons.filter(p => p.age > 18) Java 简单对象（POJOs） public class Person { public String name; public int age; public Person() {} public Person(String name, int age) { this.name = name; this.age = age; } } DataStream persons = env.fromElements( new Person(\"Alex\", 42), new Person(\"Wendy\", 23)); 其它（Arrays, Lists, Maps, Enums, 等等） Flink 对 Java 和 Scala 中的一些特殊目的的类型也都是支持的，比如 Java 的 ArrayList，HashMap，Enum 等等。 实现 UDF 函数 一个实现接口的类或者是匿名内部类 或者是lambda函数 Rich Functions “富函数”是 DataStream API 提供的一个函数类的接口，所有 Flink 函数类都 有其 Rich 版本。它与常规函数的不同在于，可以获取运行环境的上下文，并拥有一 些生命周期方法，所以可以实现更复杂的功能。 Rich Function 有一个生命周期的概念。典型的生命周期方法有： ⚫ open()方法是 rich function 的初始化方法，当一个算子例如 map 或者 filter 被调用之前 open()会被调用。 ⚫ close()方法是生命周期中的最后一个调用的方法，做一些清理工作。 ⚫ getRuntimeContext()方法提供了函数的 RuntimeContext 的一些信息，例如函数执行的并行度，任务的名字，以及 state 状态 package com.matt.apitest.transform; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.common.functions.RichMapFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.collector.selector.OutputSelector; import org.apache.flink.streaming.api.datastream.ConnectedStreams; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.datastream.SplitStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.co.CoMapFunction; import scala.Enumeration; import java.util.Collections; /** * @author matt * @create 2022-01-24 23:55 */ public class TransfromTest5_RichFunction { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(4); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream dataStream = inputStream.map(s -> { String[] fields = s.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); DataStream> resStream = dataStream.map( new MyMapper() ); resStream.print(); // job name env.execute(\"trans-form\"); } public static class MyMapper extends RichMapFunction> { @Override public Tuple2 map(SensorReading v) throws Exception { return new Tuple2<>(v.getId(), getRuntimeContext().getIndexOfThisSubtask()); } public MyMapper() { super(); } @Override public void open(Configuration parameters) throws Exception { System.out.println(\"init....\"); } @Override public void close() throws Exception { System.out.println(\"clear...\"); } } } sink kafka org.apache.flink flink-connector-kafka-0.11_2.12 1.10.1 package com.matt.apitest.sink; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.serialization.SimpleStringSchema; import org.apache.flink.streaming.api.SimpleTimerService; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011; import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011; /** * @author matt * @create 2022-01-25 23:33 */ public class SinkTest1_kafka { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //env.setParallelism(1); DataStream dataStream = env.readTextFile(\"D:\\\\matt\\\\workspace\\\\idea\\\\hadoop\\\\study-flink\\\\src\\\\main\\\\resources\\\\sensor.txt\"); dataStream.print(\"file\"); DataStream resStream = dataStream.map(line -> { String[] f = line.split(\",\"); return new SensorReading(f[0], new Long(f[1]), new Double(f[2])).toString(); }); resStream.print(); resStream.addSink(new FlinkKafkaProducer011(\"matt05:9092\", \"test\", new SimpleStringSchema())); // job name env.execute(\"my\"); } } redis org.apache.bahir flink-connector-redis_2.11 1.0 package com.matt.apitest.sink; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.serialization.SimpleStringSchema; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011; import org.apache.flink.streaming.connectors.redis.RedisSink; import org.apache.flink.streaming.connectors.redis.common.config.FlinkJedisPoolConfig; import org.apache.flink.streaming.connectors.redis.common.mapper.RedisCommand; import org.apache.flink.streaming.connectors.redis.common.mapper.RedisCommandDescription; import org.apache.flink.streaming.connectors.redis.common.mapper.RedisMapper; /** * @author matt * @create 2022-01-25 23:57 */ public class SinkTest2_Redis { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream dataStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); dataStream.print(\"file\"); DataStream resStream = dataStream.map(line -> { String[] f = line.split(\",\"); return new SensorReading(f[0], new Long(f[1]), new Double(f[2])); }); resStream.print(); FlinkJedisPoolConfig config = new FlinkJedisPoolConfig.Builder() .setHost(\"matt05\") .setPort(6379) .build(); resStream.addSink(new RedisSink<>(config, new MyRedisMapper())); // job name env.execute(\"my\"); } public static class MyRedisMapper implements RedisMapper { // 保存到 redis 的命令，存成哈希表 public RedisCommandDescription getCommandDescription() { return new RedisCommandDescription(RedisCommand.HSET, \"sensor_tempe\"); } // key public String getKeyFromData(SensorReading data) { return data.getId(); } // v public String getValueFromData(SensorReading data) { return data.getTemperatrue().toString(); } } } es org.apache.flink flink-connector-elasticsearch6_2.12 1.10.1 package com.matt.apitest.sink; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.functions.RuntimeContext; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction; import org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer; import org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink; import org.apache.flink.streaming.connectors.redis.RedisSink; import org.apache.flink.streaming.connectors.redis.common.config.FlinkJedisPoolConfig; import org.apache.http.HttpHost; import org.elasticsearch.action.index.IndexRequest; import org.elasticsearch.client.Requests; import java.util.ArrayList; import java.util.HashMap; /** * @author matt * @create 2022-01-26 0:21 */ public class SinkTest3_ES { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream dataStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); dataStream.print(\"file\"); DataStream resStream = dataStream.map(line -> { String[] f = line.split(\",\"); return new SensorReading(f[0], new Long(f[1]), new Double(f[2])); }); resStream.print(); ArrayList httpHosts = new ArrayList<>(); httpHosts.add(new HttpHost(\"localhost\", 9200)); resStream.addSink(new ElasticsearchSink.Builder(httpHosts, new MyEsSinkFunction()).build()); // job name env.execute(\"my\"); } public static class MyEsSinkFunction implements ElasticsearchSinkFunction { @Override public void process(SensorReading element, RuntimeContext ctx, RequestIndexer indexer) { HashMap dataSource = new HashMap<>(); dataSource.put(\"id\", element.getId()); dataSource.put(\"ts\", String.valueOf(element.getTimestamp())); dataSource.put(\"temp\", element.getTemperatrue().toString()); IndexRequest indexRequest = Requests.indexRequest() .index(\"sensor\") .type(\"readingData\") .source(dataSource); indexer.add(indexRequest); } } } jdbc mysql mysql-connector-java 5.1.44 package com.matt.apitest.sink; import com.matt.apitest.beans.SensorReading; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.sink.RichSinkFunction; import java.sql.DriverManager; import java.sql.Connection; import java.sql.PreparedStatement; /** * @author matt * @create 2022-01-26 1:41 */ public class SinkTest4_MySQL { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(16); DataStream dataStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream resStream = dataStream.map(line -> { String[] f = line.split(\",\"); return new SensorReading(f[0], new Long(f[1]), new Double(f[2])); }); resStream.print(); resStream.addSink(new MyJdbcSink()); // job name env.execute(); } public static class MyJdbcSink extends RichSinkFunction { Connection conn = null; PreparedStatement insertStmt = null; PreparedStatement updateStmt = null; // open 主要是创建连接 @Override public void open(Configuration parameters) throws Exception { conn = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/stu_flink\", \"root\", \"root\"); // 创建预编译器，有占位符，可传入参数 insertStmt = conn.prepareStatement(\"INSERT INTO sensor_temp (id, temp) VALUES ( ?, ?)\"); updateStmt = conn.prepareStatement(\"UPDATE sensor_temp SET temp = ? WHERE id = ? \"); } // 调用连接，执行 sql @Override public void invoke(SensorReading value, Context context) throws Exception { // 执行更新语句，注意不要留 super updateStmt.setDouble(1, value.getTemperatrue()); updateStmt.setString(2, value.getId()); updateStmt.execute(); // 如果刚才 update 语句没有更新，那么插入 if (updateStmt.getUpdateCount() == 0) { insertStmt.setString(1, value.getId()); insertStmt.setDouble(2, value.getTemperatrue()); insertStmt.execute(); } } @Override public void close() throws Exception { insertStmt.close(); updateStmt.close(); conn.close(); } } } Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-19 17:36:51 "},"31bigdata/flink/3window.html":{"url":"31bigdata/flink/3window.html","title":"window","keywords":"","body":"window window 概述 window 是一种切割无限数据 为有限块进行处理的手段。 Window 是无限数据流处理的核心，Window 将一个无限的 stream 拆分成有限大小的”buckets”桶，我们可以在这些桶上做计算操作。 类型 CountWindow：按照指定的数据条数生成一个 Window，与时间无关。 TimeWindow：按照时间生成 Window。 对于 TimeWindow，可以根据窗口实现原理的不同分成三类：滚动窗口（Tumbling Window）、滑动窗口（Sliding Window）和会话窗口（Session Window）。 1.滚动窗口（Tumbling Windows）： 将数据依据固定的窗口长度对数据进行切片。 特点：时间对齐，窗口长度固定，没有重叠。 滚动窗口分配器将每个元素分配到一个指定窗口大小的窗口中，滚动窗口有一 个固定的大小，并且不会出现重叠。例如：如果你指定了一个 5 分钟大小的滚动窗 口，窗口的创建如下图所示： 适用场景：适合做 BI 统计等（做每个时间段的聚合计算）。 BI(BusinessIntelligence)即商务智能，它是一套完整的解决方案，用来将企业中现有的数据进行有效的整合，快速准确的提供报表并提出决策依据，帮助企业做出明智的业务经营决策。 2.滑动窗口（Sliding Windows） 滑动窗口是固定窗口的更广义的一种形式，滑动窗口由固定的窗口长度和滑动 间隔组成。 特点：时间对齐，窗口长度固定，可以有重叠。 滑动窗口分配器将元素分配到固定长度的窗口中，与滚动窗口类似，窗口的大 小由窗口大小参数来配置，另一个窗口滑动参数控制滑动窗口开始的频率。因此， 滑动窗口如果滑动参数小于窗口大小的话，窗口是可以重叠的，在这种情况下元素 会被分配到多个窗口中。 例如，你有 10 分钟的窗口和 5 分钟的滑动，那么每个窗口中 5 分钟的窗口里包 含着上个 10 分钟产生的数据，如下图所示： 适用场景：对最近一个时间段内的统计（求某接口最近 5min 的失败率来决定是 否要报警）。 3.会话窗口 由一系列事件组合一个指定时间长度的 timeout 间隙组成，类似于 web 应用的 session，也就是一段时间没有接收到新数据就会生成新的窗口。 特点：时间无对齐。 session 窗口分配器通过 session 活动来对元素进行分组，session 窗口跟滚动窗口和滑动窗口相比，不会有重叠和固定的开始时间和结束时间的情况，相反，当它 在一个固定的时间周期内不再收到元素，即非活动间隔产生，那个这个窗口就会关 闭。一个 session 窗口通过一个 session 间隔来配置，这个 session 间隔定义了非活跃 周期的长度，当这个非活跃周期产生，那么当前的 session 将关闭并且后续的元素将 被分配到新的 session 窗口中去。 window API TimeWindow TimeWindow 是将指定时间范围内的所有数据组成一个 window，一次对一个 window 里面的所有数据进行计算。 1.滚动窗口 Flink 默认的时间窗口根据 Processing Time 进行窗口的划分，将 Flink 获取到的 数据根据进入 Flink 的时间划分到不同的窗口中。 package com.matt.apitest.window; import com.matt.apitest.beans.SensorReading; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; public class Test1 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //env.setParallelism(1); DataStream inputStream = env.socketTextStream(\"localhost\", 777); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); // 开窗测试 // windowAll 都放在一个窗口里面 DataStream t1 = dataStream.keyBy(\"id\") .timeWindow(Time.seconds(10)).minBy(\"temperatrue\"); t1.print(); env.execute(\"my\"); } } minBy():可以是元组的顺序也可以对象的属性 时间间隔可以通过 Time.milliseconds(x)，Time.seconds(x)，Time.minutes(x)等其 中的一个来指定。 2.滑动窗口（SlidingEventTimeWindows） 滑动窗口和滚动窗口的函数名是完全一致的，只是在传参数时需要传入两个参 数，一个是 window_size，一个是 sliding_size。 下面代码中的 sliding_size 设置为了 5s，也就是说，每 5s 就计算输出结果一次， 每一次计算的 window 范围是 15s 内的所有元素。 package com.matt.apitest.window; import com.matt.apitest.beans.SensorReading; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; public class Test1 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //env.setParallelism(1); DataStream inputStream = env.socketTextStream(\"localhost\", 777); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); // 开窗测试 // windowAll 都放在一个窗口里面 DataStream t1 = dataStream.keyBy(\"id\") .timeWindow(Time.seconds(15), Time.seconds(5)).minBy(\"temperatrue\"); t1.print(); env.execute(\"my\"); } } 3. CountWindow CountWindow 根据窗口中相同 key 元素的数量来触发执行，执行时只计算元素 数量达到窗口大小的 key 对应的结果。 注意：CountWindow 的 window_size 指的是相同 Key 的元素的个数，不是输入 的所有元素的总数。 1滚动窗口 默认的 CountWindow 是一个滚动窗口，只需要指定窗口大小即可，当元素数量 达到窗口大小时，就会触发窗口的执行。 package com.matt.apitest.window; import com.matt.apitest.beans.SensorReading; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; public class Test1 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //env.setParallelism(1); DataStream inputStream = env.socketTextStream(\"localhost\", 777); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); // 开窗测试 // windowAll 都放在一个窗口里面 DataStream t1 = dataStream.keyBy(\"id\") .countWindow(2).minBy(\"temperatrue\"); t1.print(); env.execute(\"my\"); } } 2.滑动窗口 滑动窗口和滚动窗口的函数名是完全一致的，只是在传参数时需要传入两个参 数，一个是 window_size，一个是 sliding_size。 下面代码中的 sliding_size 设置为了 2，也就是说，每收到两个相同 key 的数据 就计算一次，每一次计算的 window 范围是 10 个元素。 package com.matt.apitest.window; import com.matt.apitest.beans.SensorReading; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; public class Test1 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //env.setParallelism(1); DataStream inputStream = env.socketTextStream(\"localhost\", 777); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); // 开窗测试 // windowAll 都放在一个窗口里面 DataStream t1 = dataStream.keyBy(\"id\") .countWindow(10,5).minBy(\"temperatrue\"); t1.print(); env.execute(\"my\"); } } window function window function 定义了要对窗口中收集的数据做的计算操作，主要可以分为两 类： 增量聚合函数（incremental aggregation functions） 每条数据到来就进行计算，保持一个简单的状态。典型的增量聚合函数有 ReduceFunction, AggregateFunction。 全窗口函数（full window functions） 先把窗口所有数据收集起来，等到计算的时候会遍历所有数据。 ProcessWindowFunction 就是一个全窗口函数。 package com.matt.apitest.window; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.functions.AggregateFunction; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; public class Test1 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //env.setParallelism(1); DataStream inputStream = env.socketTextStream(\"localhost\", 777); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); // 开窗测试 // windowAll 都放在一个窗口里面 DataStream t1 = dataStream.keyBy(\"id\") .timeWindow(Time.seconds(15)) .aggregate(new AggregateFunction() { @Override public Long createAccumulator() { return 0l; } @Override public Long add(SensorReading sensorReading, Long l1) { return sensorReading.getTimestamp() + l1; } @Override public Long getResult(Long l1) { return l1; } @Override public Long merge(Long aLong, Long acc1) { return null; } }); t1.print(); env.execute(\"my\"); } } package com.matt.apitest.window; import com.matt.apitest.beans.SensorReading; import org.apache.commons.collections.IteratorUtils; import org.apache.flink.api.common.functions.AggregateFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.streaming.api.functions.windowing.WindowFunction; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.windows.TimeWindow; import org.apache.flink.util.Collector; public class Test1 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //env.setParallelism(1); DataStream inputStream = env.socketTextStream(\"localhost\", 777); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); // 开窗测试 // windowAll 都放在一个窗口里面 DataStream> t2 = dataStream.keyBy(\"id\") .timeWindow(Time.seconds(10)) .apply(new WindowFunction, Tuple, TimeWindow>() { @Override public void apply(Tuple tuple, TimeWindow timeWindow, Iterable input, Collector> out) throws Exception { String id = tuple.getField(0); long windowEnd = timeWindow.getEnd(); int count = IteratorUtils.toArray(input.iterator()).length; Tuple3 res = new Tuple3<>(id, windowEnd, count); out.collect(res); } }); t2.print(); env.execute(\"my\"); } } 其他api-后面在看 .trigger() —— 触发器 定义 window 什么时候关闭，触发计算并输出结果 .evitor() —— 移除器 定义移除某些数据的逻辑 .allowedLateness() —— 允许处理迟到的数据 .sideOutputLateData() —— 将迟到的数据放入侧输出流 .getSideOutput() —— 获取侧输出流 Exception in thread \"main\" org.apache.flink.api.common.functions.InvalidTypesException: Could not determine TypeInformation for the OutputTag type. The most common reason is forgetting to make the OutputTag an anonymous inner class. It is also not possible to use generic type variables with OutputTags, such as 'Tuple2'. at org.apache.flink.util.OutputTag.(OutputTag.java:65) at com.matt.apitest.window.WindowTest1_TimeWindow.main(WindowTest1_TimeWindow.java:92) Caused by: org.apache.flink.api.common.functions.InvalidTypesException: The types of the interface org.apache.flink.util.OutputTag could not be inferred. Support for synthetic interfaces, lambdas, and generic or raw types is limited at this point at org.apache.flink.api.java.typeutils.TypeExtractor.getParameterType(TypeExtractor.java:1244) at org.apache.flink.api.java.typeutils.TypeExtractor.privateCreateTypeInfo(TypeExtractor.java:789) at org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfo(TypeExtractor.java:769) at org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfo(TypeExtractor.java:762) at org.apache.flink.util.OutputTag.(OutputTag.java:62) ... 1 more Process finished with exit code 1 https://blog.csdn.net/qq_39598180/article/details/114491282 OutputTag pageTag = new OutputTag<>(\"page\"); 关于为什么会出现这个问题，我们知道在Java中泛型的实现方式是基于Code sharing机制的，也就是对同一个原始类型下的泛型类型只生成同一份目标代码，即字节码，基于此机制JVM会将泛型的类型进行擦除，这与cpp中的泛型实现有本质上的不同，因此也被称为假泛型 解决方案 通过{}重写该类，显式指定该对象的泛型类型即可 OutputTag pageTag = new OutputTag(\"page\"){}; package com.matt.apitest.window; import com.matt.apitest.beans.SensorReading; import org.apache.commons.collections.IteratorUtils; import org.apache.flink.api.common.functions.AggregateFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.functions.windowing.WindowFunction; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.windows.TimeWindow; import org.apache.flink.util.Collector; import org.apache.flink.util.OutputTag; public class Test1 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //env.setParallelism(1); DataStream inputStream = env.socketTextStream(\"localhost\", 777); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); // 开窗测试 // windowAll 都放在一个窗口里面 // 其他api OutputTag outputTag = new OutputTag(\"rate\"){}; SingleOutputStreamOperator> t3 = dataStream.keyBy(\"id\") .timeWindow(Time.seconds(15)) .allowedLateness(Time.minutes(1)) .sideOutputLateData(outputTag) .apply(new WindowFunction, Tuple, TimeWindow>() { @Override public void apply(Tuple tuple, TimeWindow timeWindow, Iterable input, Collector> out) throws Exception { String id = tuple.getField(0); long windowEnd = timeWindow.getEnd(); int count = IteratorUtils.toArray(input.iterator()).length; Tuple3 res = new Tuple3<>(id, windowEnd, count); out.collect(res); } }); // t3.print(); t3.getSideOutput(outputTag).print(\"rate\"); env.execute(\"my\"); } } 默认情况下，如果不指定allowedLateness，其值是0，即对于watermark超过end-of-window之后，还有此window的数据到达时，这些数据被删除掉了。 时间语义与wartermark Flink 中的时间语义 Event Time：是事件创建的时间。它通常由事件中的时间戳描述，例如采集的 日志数据中，每一条日志都会记录自己的生成时间，Flink 通过时间戳分配器访问事 件时间戳。 Ingestion Time：是数据进入 Flink 的时间。 Processing Time：是每一个执行基于时间操作的算子的本地系统时间，与机器 相关，默认的时间属性就是 Processing Time。 EventTime的引入 在 Flink 的流式处理中，绝大部分的业务都会使用 eventTime，一般只在 eventTime 无法使用时，才会被迫使用 ProcessingTime 或者 IngestionTime。 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //env.setParallelism(1); // 默认使用processTime // 从调用时刻开始给 env 创建的每一个 stream 追加时间特征 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); Watermark 概述 事件时间早的数据并不一定先进入flink(数据是乱序的) 那么此时出现一个问题，一旦出现乱序，如果只根据 eventTime 决定 window 的 运行，我们不能明确数据是否全部到位，但又不能无限期的等下去，此时必须要有 个机制来保证一个特定的时间后，必须触发 window 去进行计算了，这个特别的机 制，就是 Watermark。 Watermark 是一种衡量 Event Time 进展的机制。 Watermark 是用于处理乱序事件的，而正确的处理乱序事件，通常用 Watermark 机制结合 window 来实现。 数据流中的 Watermark 用于表示 timestamp 小于 Watermark 的数据，都已经 到达了，因此，window 的执行也是由 Watermark 触发的。 Watermark 可以理解成一个延迟触发机制，我们可以设置 Watermark 的延时 时长 t，每次系统会校验已经到达的数据中最大的 maxEventTime，然后认定 eventTime 小于 maxEventTime - t 的所有数据都已经到达，如果有窗口的停止时间等于 maxEventTime – t，那么这个窗口被触发执行。 窗口长度5， 延迟时间2 7只是会触发窗口1关闭，但是7不会进入窗口1，而是进入窗口2 引入 package com.matt.apitest.window; import com.matt.apitest.beans.SensorReading; import org.apache.commons.collections.IteratorUtils; import org.apache.flink.api.common.functions.AggregateFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.streaming.api.TimeCharacteristic; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor; import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor; import org.apache.flink.streaming.api.functions.windowing.WindowFunction; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.windowing.windows.TimeWindow; import org.apache.flink.util.Collector; import org.apache.flink.util.OutputTag; /** * @author matt * @create 2022-02-08 23:49 */ public class WindowTest3_EventTimeWindow { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //env.setParallelism(1); // 默认使用processTime // 从调用时刻开始给 env 创建的每一个 stream 追加时间特征 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); env.getConfig().setAutoWatermarkInterval(100); DataStream inputStream = env.socketTextStream(\"localhost\", 778); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }) //乱序 // 最大延迟时间2s .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor(Time.seconds(2)) { @Override public long extractTimestamp(SensorReading sensorReading) { // 必须为ms毫秒 return sensorReading.getTimestamp() * 1000L; } }); // 非乱序 //.assignTimestampsAndWatermarks(new AscendingTimestampExtractor() { // @Override // public long extractAscendingTimestamp(SensorReading sensorReading) { // return sensorReading.getTimestamp() * 1000L; // } //}); env.execute(\"my\"); } } Assigner with periodic watermarks 周期性的生成 watermark 周期性的生成 watermark：系统会周期性的将 watermark 插入到流中(水位线也 是一种特殊的事件!)。默认周期是 200 毫秒。可以使用 ExecutionConfig.setAutoWatermarkInterval()方法进行设置。 // 每100ms引入一个woatermark env.getConfig().setAutoWatermarkInterval(100); 产生 watermark 的逻辑：每隔 0.1 秒钟，Flink 会调用 AssignerWithPeriodicWatermarks 的 getCurrentWatermark()方法。如果方法返回一个时间戳大于之前水位的时间戳，新的 watermark 会被插入到流中。这个检查保证了 水位线是单调递增的。如果方法返回的时间戳小于等于之前水位的时间戳，则不会 产生新的 watermark。 // 自定义周期性时间戳分配器 public static class MyPeriodicAssigner implements AssignerWithPeriodicWatermarks{ private Long bound = 60 * 1000L; // 延迟一分钟 private Long maxTs = Long.MIN_VALUE; // 当前最大时间戳 @Nullable @Override public Watermark getCurrentWatermark() { return new Watermark(maxTs - bound); } @Override public long extractTimestamp(SensorReading element, long previousElementTimestamp) { maxTs = Math.max(maxTs, element.getTimestamp()); return element.getTimestamp(); } } Assigner with punctuated watermarks 间断式产生woatermark 间断式地生成 watermark。和周期性生成的方式不同，这种方式不是固定时间的， 而是可以根据需要对每条数据进行筛选和处理。直接上代码来举个例子，我们只给 sensor_1 的传感器的数据流插入 watermark： public static class MyPunctuatedAssigner implements AssignerWithPunctuatedWatermarks{ private Long bound = 60 * 1000L; // 延迟一分钟 @Nullable @Override public Watermark checkAndGetNextWatermark(SensorReading lastElement, long extractedTimestamp) { if(lastElement.getId().equals(\"sensor_1\")) return new Watermark(extractedTimestamp - bound); else return null; } @Override public long extractTimestamp(SensorReading element, long previousElementTimestamp) { return element.getTimestamp(); } } processFunction 我们之前学习的转换算子是无法访问事件的时间戳信息和水位线信息的。而这 在一些应用场景下，极为重要。例如 MapFunction 这样的 map 转换算子就无法访问 时间戳或者当前事件的事件时间。 Flink 提供了 8 个 Process Function： • ProcessFunction • KeyedProcessFunction • CoProcessFunction • ProcessJoinFunction • BroadcastProcessFunction • KeyedBroadcastProcessFunction • ProcessWindowFunction • ProcessAllWindowFunction keyedProcessFunction KeyedProcessFunction 用来操作 KeyedStream。KeyedProcessFunction 会处理流 的每一个元素，输出为 0 个、1 个或者多个元素。所有的 Process Function 都继承自 RichFunction 接口，所以都有 open()、close()和 getRuntimeContext()等方法。而 KeyedProcessFunction还额外提供了两个方法 processElement(I value, Context ctx, Collector out), 流中的每一个元素都 会调用这个方法，调用结果将会放在 Collector 数据类型中输出。Context 可以访问元素的时间戳，元素的 key，以及 TimerService 时间服务。Context 还 可以将结果输出到别的流(side outputs)。 onTimer(long timestamp, OnTimerContext ctx, Collector out) 是一个回调 函数。当之前注册的定时器触发时调用。参数 timestamp 为定时器所设定的 触发的时间戳。Collector 为输出结果的集合。OnTimerContext 和 processElement 的 Context 参数一样，提供了上下文的一些信息，例如定时器 触发的时间信息(事件时间或者处理时间)。 TimerService 和 定时器（Timers） Context 和 OnT™imerContext 所持有的 TimerService 对象拥有以下方法: • long currentProcessingTime() 返回当前处理时间 • long currentWatermark() 返回当前 watermark 的时间戳 • void registerProcessingTimeTimer(long timestamp) 会注册当前 key 的 processing time 的定时器。当 processing time 到达定时时间时，触发 timer。 • void registerEventTimeTimer(long timestamp) 会注册当前 key 的 event time 定 时器。当水位线大于等于定时器注册的时间时，触发定时器执行回调函数。 • void deleteProcessingTimeTimer(long timestamp) 删除之前注册处理时间定时 器。如果没有这个时间戳的定时器，则不执行。 • void deleteEventTimeTimer(long timestamp) 删除之前注册的事件时间定时 器，如果没有此时间戳的定时器，则不执行。 当定时器 timer 触发时，会执行回调函数 onTimer()。注意定时器 timer 只能在 keyed streams 上面使用。 package com.matt.apitest.processfunction; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.state.ValueState; import org.apache.flink.api.common.state.ValueStateDescriptor; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.KeyedProcessFunction; import org.apache.flink.util.Collector; public class Test1_ProcessKeyedProcessFunction { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); DataStream inputStream = env.socketTextStream(\"localhost\", 777); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); // 测试 keyedProcessFunction dataStream.keyBy(\"id\") .process(new MyProcess()).print(); env.execute(\"my\"); } public static class MyProcess extends KeyedProcessFunction { ValueState tsTimerState; @Override public void open(Configuration parameters) throws Exception { tsTimerState = getRuntimeContext().getState(new ValueStateDescriptor(\"ts-timer\", Long.class)); } @Override public void processElement(SensorReading sensorReading, KeyedProcessFunction.Context context, Collector collector) throws Exception { collector.collect(sensorReading.getId().length()); Long timestamp = context.timestamp(); Tuple currentKey = context.getCurrentKey(); // context.output(); context.timerService().currentProcessingTime(); context.timerService().currentWatermark(); context.timerService().registerProcessingTimeTimer(context.timerService().currentProcessingTime() + 1000); tsTimerState.update(context.timerService().currentProcessingTime() + 1000); // ms context.timerService().registerEventTimeTimer((sensorReading.getTimestamp() + 10) * 1000); //context.timerService().deleteEventTimeTimer(tsTimerState.value()); } @Override public void onTimer(long timestamp, KeyedProcessFunction.OnTimerContext ctx, Collector out) throws Exception { System.out.println(timestamp + \"定时器触发\"); } } } 案例：温度上升触发报警 package com.matt.apitest.processfunction; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.state.ValueState; import org.apache.flink.api.common.state.ValueStateDescriptor; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.KeyedProcessFunction; import org.apache.flink.util.Collector; public class Test2_ApplicationTest { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); DataStream inputStream = env.socketTextStream(\"localhost\", 777); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); // 测试 keyedProcessFunction dataStream.keyBy(\"id\") .process(new MyProcess1()).print(); env.execute(\"my\"); } // 检测一段时间温度上升 public static class MyProcess1 extends KeyedProcessFunction { // 时间间隔 private Integer interval = 10; // 上次温度 ValueState lastTempState; // 上次时间 ValueState timerState; @Override public void open(Configuration parameters) throws Exception { lastTempState = getRuntimeContext().getState(new ValueStateDescriptor(\"lastTempState\", Double.class, Double.MIN_VALUE)); timerState = getRuntimeContext().getState(new ValueStateDescriptor(\"timerState\", Long.class)); } @Override public void processElement(SensorReading sensorReading, KeyedProcessFunction.Context context, Collector collector) throws Exception { Double lastTemp = lastTempState.value(); Long lastTimer = timerState.value(); // 状态是否存在 // 温度上升 且上个温度是null 注册定时器 立即触发 if (sensorReading.getTemperatrue() > lastTemp && lastTimer == null) { long ts = context.timerService().currentProcessingTime() + interval * 1000l; // 注册一个时间 context.timerService().registerProcessingTimeTimer(ts); timerState.update(ts); // 温度下降 且上次温度不为null 去除定时器 } else if (sensorReading.getTemperatrue() .OnTimerContext ctx, Collector out) throws Exception { System.out.println(\"持续上升\" + ctx.getCurrentKey().getField(0)); lastTempState.clear(); } } } 侧输出流（SideOutput） 大部分的 DataStream API 的算子的输出是单一输出，也就是某种数据类型的流。 除了 split 算子，可以将一条流分成多条流，这些流的数据类型也都相同。process function 的 side outputs 功能可以产生多条流，并且这些流的数据类型可以不一样。 一个 side output 可以定义为 OutputTag[X]对象，X 是输出流的数据类型。process function 可以通过 Context 对象发射一个事件到一个或者多个 side outputs。 温度小于30度输出到测输出流 package com.matt.apitest.processfunction; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.state.ValueState; import org.apache.flink.api.common.state.ValueStateDescriptor; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.KeyedProcessFunction; import org.apache.flink.streaming.api.functions.ProcessFunction; import org.apache.flink.util.Collector; import org.apache.flink.util.OutputTag; public class Test3_Sideoutputcase { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); DataStream inputStream = env.socketTextStream(\"localhost\", 777); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); // 测试 测输出流 实现分流操作 // 匿名 OutputTag outputTag = new OutputTag(\"lowTemp\"){}; SingleOutputStreamOperator highTempStream = dataStream.process(new ProcessFunction() { @Override public void processElement(SensorReading sensorReading, ProcessFunction.Context context, Collector collector) throws Exception { // if (sensorReading.getTemperatrue() > 30) { collector.collect(sensorReading); } else { // 输出到侧边流 context.output(outputTag, sensorReading); } } }); highTempStream.print(\"high\"); highTempStream.getSideOutput(outputTag).print(\"lowTem\"); env.execute(\"my\"); } // 检测一段时间温度上升 public static class MyProcess1 extends KeyedProcessFunction { // 时间间隔 private Integer interval = 10; ValueState lastTempState; ValueState timerState; @Override public void open(Configuration parameters) throws Exception { lastTempState = getRuntimeContext().getState(new ValueStateDescriptor(\"lastTempState\", Double.class, Double.MIN_VALUE)); timerState = getRuntimeContext().getState(new ValueStateDescriptor(\"timerState\", Long.class)); } @Override public void processElement(SensorReading sensorReading, KeyedProcessFunction.Context context, Collector collector) throws Exception { Double lastTemp = lastTempState.value(); Long lastTimer = timerState.value(); // 状态是否存在 if (sensorReading.getTemperatrue() > lastTemp && lastTimer == null) { long ts = context.timerService().currentProcessingTime() + interval * 1000l; // 注册一个时间 context.timerService().registerProcessingTimeTimer(ts); timerState.update(ts); } else if (sensorReading.getTemperatrue() .OnTimerContext ctx, Collector out) throws Exception { System.out.println(\"持续上升\" + ctx.getCurrentKey().getField(0)); lastTempState.clear(); } } } 迟到数据输到侧输出流 package com.matt.apitest.window; import com.matt.apitest.beans.SensorReading; import org.apache.commons.collections.IteratorUtils; import org.apache.flink.api.common.functions.AggregateFunction; import org.apache.flink.api.java.tuple.Tuple; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.streaming.api.TimeCharacteristic; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor; import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor; import org.apache.flink.streaming.api.functions.windowing.WindowFunction; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.windowing.windows.TimeWindow; import org.apache.flink.util.Collector; import org.apache.flink.util.OutputTag; /** * @author matt * @create 2022-02-08 23:49 */ public class WindowTest3_EventTimeWindow { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //env.setParallelism(1); // 默认使用processTime // 从调用时刻开始给 env 创建的每一个 stream 追加时间特征 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); env.getConfig().setAutoWatermarkInterval(100); DataStream inputStream = env.socketTextStream(\"localhost\", 778); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }) //乱序 .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor(Time.seconds(2)) { @Override public long extractTimestamp(SensorReading sensorReading) { // 必须为ms毫秒 return sensorReading.getTimestamp() * 1000L; } }); // 非乱序 //.assignTimestampsAndWatermarks(new AscendingTimestampExtractor() { // @Override // public long extractAscendingTimestamp(SensorReading sensorReading) { // return sensorReading.getTimestamp() * 1000L; // } //}); OutputTag outputTag = new OutputTag(\"late\"){}; // 开窗测试 // windowAll 都放在一个窗口里面 SingleOutputStreamOperator minTempStream = dataStream.keyBy(\"id\") .timeWindow(Time.seconds(15)) .allowedLateness(Time.minutes(1)) // 1分钟后测输入流 .sideOutputLateData(outputTag) .minBy(\"temperatrue\"); //temperatrue minTempStream.print(\"minTemp\"); minTempStream.getSideOutput(outputTag).print(\"late\"); //OutputTag outputTag = new OutputTag(\"late\") { //}; // //// 基于事件时间的开窗聚合，统计15秒内温度的最小值 //SingleOutputStreamOperator minTempStream = dataStream.keyBy(\"id\") // .timeWindow(Time.seconds(15)) // .allowedLateness(Time.minutes(1)) // .sideOutputLateData(outputTag) // .minBy(\"temperatrue\"); // //minTempStream.print(\"minTemp\"); //minTempStream.getSideOutput(outputTag).print(\"late\"); env.execute(\"my\"); } } Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-19 17:36:58 "},"31bigdata/flink/4状态编程和容错机制.html":{"url":"31bigdata/flink/4状态编程和容错机制.html","title":"状态编程和容错机制","keywords":"","body":"状态编程和容错机制 举例 所有类型的窗口。例如，计算过去一小时的平均温度，就是有状态的计算。 所有用于复杂事件处理的状态机。例例如，若在一分钟内收到两个相差 20 度 以上的温度读数，则发出警告，这是有状态的计算。 流与流之间的所有关联操作，以及流与静态表或动态表之间的关联操作， 都是有状态的计算。 下图展示了无状态流处理和有状态流处理的主要区别。无状态流处理分别接收 每条数据记录(图中的黑条)，然后根据最新输入的数据生成输出数据(白条)。有状态 流处理会维护状态(根据每条输入记录进行更新)，并基于最新输入的记录和当前的状态值生成输出记录(灰条)。 有状态的算子和应用程序 Flink 内置的很多算子，数据源 source，数据存储 sink 都是有状态的，流中的数 据都是 buffer records，会保存一定的元素或者元数据。例如: ProcessWindowFunction 会缓存输入流的数据，ProcessFunction 会保存设置的定时器信息等等。 在 Flink 中，状态始终与特定算子相关联。 总的来说，有两种类型的状态： ⚫ 算子状态（operator state） ⚫ 键控状态（keyed state） 算子状态 算子状态的作用范围限定为算子任务。这意味着由同一并行任务所处理的所有 数据都可以访问到相同的状态，状态对于同一任务而言是共享的。算子状态不能由 相同或不同算子的另一个任务访问。 Flink 为算子状态提供三种基本数据结构： 列表状态（List state） 将状态表示为一组数据的列表。 联合列表状态（Union list state） 也将状态表示为数据的列表。它与常规列表状态的区别在于，在发生故障时，或者从保存点（savepoint）启动应用程序时如何恢复。 广播状态（Broadcast state） 如果一个算子有多项任务，而它的每项任务状态又都相同，那么这种特殊情况最适合应 用广播状态。 键控状态 具有相同 key 的所有数据都会访问相同的状态。 Keyed State 很类似于 一个分布式的 key-value map 数据结构，只能用于 KeyedStream（keyBy 算子处理之后）。 Flink 的 Keyed State 支持以下数据类型 ValueState保存单个的值，值的类型为 T。 get 操作: ValueState.value() set 操作: ValueState.update(T value) ListState保存一个列表，列表里的元素的数据类型为 T。基本操作如下： ListState.add(T value) ListState.addAll(List values) ListState.get()返回 Iterable ListState.update(List values) MapState保存 Key-Value 对 MapState.get(UK key) MapState.put(UK key, UV value) MapState.contains(UK key) MapState.remove(UK key) ReducingState AggregatingState State.clear()是清空操作。 package com.matt.apitest.state; import com.matt.apitest.beans.SensorReading; import org.apache.commons.digester.SetNestedPropertiesRule; import org.apache.flink.api.common.functions.MapFunction; import org.apache.flink.api.common.functions.RichMapFunction; import org.apache.flink.api.common.state.*; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.checkpoint.ListCheckpointed; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.hadoop.util.hash.Hash; import sun.management.Sensor; import java.util.Collections; import java.util.HashMap; import java.util.List; public class StateTest2_KeyedState { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream inputStream = env.socketTextStream(\"localhost\", 777); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); SingleOutputStreamOperator res = dataStream.keyBy(\"id\").map(new MyKeyCountMapper()); res.print(); env.execute(\"my\"); } public static class MyKeyCountMapper extends RichMapFunction{ private ValueState keyCountState; private ListState listState; private MapState mapState; private ReducingState readingReducingState; @Override public void open(Configuration parameters) throws Exception { keyCountState = getRuntimeContext().getState( new ValueStateDescriptor(\"keyedCount\", Integer.class,0) ); // name:不能相同 listState = getRuntimeContext().getListState( new ListStateDescriptor(\"listState\", String.class) ); mapState = getRuntimeContext().getMapState( new MapStateDescriptor(\"mapState\", String.class, String.class) ); //readingReducingState = getRuntimeContext().getReducingState( // new ReducingStateDescriptor(\"reducingState\", new ) //); } @Override public Integer map(SensorReading sensorReading) throws Exception { Integer count = keyCountState.value(); count++; keyCountState.update(count); Iterable strings = listState.get(); for (String string : strings) { System.out.println(string) ; } listState.add(\"hello\"); // mapState // readingReducingState add 方法直接聚合 return count; } } } 案例 检测传感器的温度值，如果连 续的两个温度差值超过 10 度，就输出报警。 package com.matt.apitest.state; import com.matt.apitest.beans.SensorReading; import org.apache.flink.api.common.functions.RichFlatMapFunction; import org.apache.flink.api.common.state.*; import org.apache.flink.api.java.tuple.Tuple3; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.util.Collector; public class StateTest3_keyedstate_app1 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream inputStream = env.socketTextStream(\"localhost\", 777); // 转换成SensorReading类型 DataStream dataStream = inputStream.map(line -> { String[] fields = line.split(\",\"); return new SensorReading(fields[0], new Long(fields[1]), new Double(fields[2])); }); // warn SingleOutputStreamOperator> res = dataStream.keyBy(\"id\") .flatMap(new TempChangeWarning(10.0)); res.print(); env.execute(\"my\"); } public static class TempChangeWarning extends RichFlatMapFunction> { private Double threshold = 10.0; // 状态 上一次温度值 private ValueState lastTempState; public TempChangeWarning(Double threshold) { this.threshold = threshold; } @Override public void open(Configuration parameters) throws Exception { lastTempState = getRuntimeContext().getState( new ValueStateDescriptor(\"last-temp\", Double.class) ); } @Override public void flatMap(SensorReading sensorReading, Collector> collector) throws Exception { Double lastTemp = lastTempState.value(); if (lastTemp != null && Math.abs(sensorReading.getTemperatrue() - lastTemp) > threshold) { collector.collect(new Tuple3<>(\"报警\" + sensorReading.getId(), sensorReading.getTemperatrue(), lastTemp)); } lastTempState.update(sensorReading.getTemperatrue()); } @Override public void close() throws Exception { // 清理状态 lastTempState.clear(); } } } 状态一致性 正确性的级别， 发生故障恢复 和没有发生故障二者的结果 一致性级别 at-most-once: 这其实是没有正确性保障的委婉说法——故障发生之后，计数结果可能丢失。同样的还有 udp。 最多一次 at-least-once: 这表示计数结果可能大于正确值，但绝不会小于正确值。也 就是说，计数程序在发生故障后可能多算，但是绝不会少算。 最少一次 exactly-once: 这指的是系统保证在发生故障后得到的计数结果与正确值一致。 精准一次 端到端一致性 内部保证 —— 依赖 checkpoint source 端 —— 需要外部源可重设数据的读取位置 sink 端 —— 需要保证从故障恢复时，数据不会重复写入外部系统 而对于 sink 端，又有两种具体的实现方式： 幂等（Idempotent）写入和事务性 （Transactional）写入。 所谓幂等操作，是说一个操作，可以重复执行很多次，但只导致一次结果更改， 也就是说，后面再重复执行就不起作用了。 需要构建事务来写入外部系统，构建的事务对应着 checkpoint，等到 checkpoint 真正完成的时候，才把所有对应的结果写入 sink 系统中。 对于事务性写入，具体又有两种实现方式：预写日志（WAL）和两阶段提交 （2PC）。DataStream API 提供了 GenericWriteAheadSink 模板类和 TwoPhaseCommitSinkFunction 接口，可以方便地实现这两种方式的事务性写入。 TODO 预写日志 检查点 Flink 的检查点算法 现有一个算法：任务1：根据第一个元素分组， 任务2：第二个元素求和 检查点之前：b2 c1 b3 检查点之后：a2 a2 c2 当读取输入流的数据源(在本例中与 keyBy 算子内联) 遇到检查点屏障时，它将其在输入流中的位置保存到持久化存储中。如果输入流来 自消息传输系统(Kafka)，这个位置就是偏移量。Flink 的存储机制是插件化的，持久 化存储可以是分布式文件系统，如 HDFS。下图展示了这个过程。 检查点像普通数据记录一样在算子之间流动。当 map 算子处理完前 3 条数据并 收到检查点分界线时，它们会将状态以异步的方式写入持久化存储 当 map 算子的状态备份和检查点分界线的位置备份被确认之后，该检查点操作 就可以被标记为完成 如果检查点操作失败，Flink 可以丢弃该检查点并继续正常执行，因为之后的某 一个检查点可能会成功 Flink+Kafka 如何实现端到端的 exactly-once 语义 内部 —— 利用 checkpoint 机制，把状态存盘，发生故障的时候可以恢复， 保证内部的状态一致性 source —— kafka consumer 作为 source，可以将偏移量保存下来，如果后 续任务出现了故障，恢复的时候可以由连接器重置偏移量，重新消费数据， 保证一致性 sink —— kafka producer 作为 sink，采用两阶段提交 sink，需要实现一个 TwoPhaseCommitSinkFunction 当 checkpoint 启动时，JobManager 会将检查点分界线（barrier）注入数据流； barrier 会在算子间传递下去。 每个算子会对当前的状态做个快照，保存到状态后端。对于 source 任务而言， 就会把当前的 offset 作为状态保存起来。下次从 checkpoint 恢复时，source 任务可 以重新提交偏移量，从上次保存的位置开始重新消费数据。 sink 任务首先把数据写入外部 kafka，这些数据都属于预提交的事务（还不能 被消费）；当遇到 barrier 时，把状态保存到状态后端，并开启新的预提交事务。 当所有算子任务的快照完成，也就是这次的 checkpoint 完成时，JobManager 会 向所有任务发通知，确认这次 checkpoint 完成。 当 sink 任务收到确认通知，就会正式提交之前的事务，kafka 中未确认的数据 就改为“已确认”，数据就真正可以被消费了。 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-27 14:52:01 "},"31bigdata/flink/5sql.html":{"url":"31bigdata/flink/5sql.html","title":"sql","keywords":"","body":"更新： toRetractStream sqlAggr> (true,sensor_1,1) # 删除 撤回 sqlAggr> (false,sensor_1,1) # 插入 sqlAggr> (true,sensor_1,2) sqlAggr> (true,sensor_6,1) sqlAggr> (false,sensor_6,1) sqlAggr> (true,sensor_6,2) sqlAggr> (false,sensor_6,2) sqlAggr> (true,sensor_6,3) sqlAggr> (true,sensor_7,1) sqlAggr> (true,sensor_10,1) table->stream 追加 撤回 s -> t fromDataStream package com.matt.apitest.tableapi; import com.matt.apitest.beans.SensorReading; import org.apache.flink.streaming.api.TimeCharacteristic; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.java.StreamTableEnvironment; import org.apache.flink.types.Row; public class TimeAndWidow5 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 指定事件时间 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream dataStream = inputStream.map(line -> { String[] split = line.split(\",\"); return new SensorReading(split[0], new Long(split[1]), new Double(split[2])); }) // 指定延迟时间 .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor(Time.seconds(2)) { @Override public long extractTimestamp(SensorReading sensorReading) { // 指定事件时间字段 return sensorReading.getTimestamp() * 1000L; } }); // 3.创建表环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 基于流创建一张表 //Table dataTable = tableEnv.fromDataStream(dataStream, \"id, temperatrue, timestamp, pt.proctime\"); // 和类字段保持一致 Table dataTable = tableEnv.fromDataStream(dataStream, \"id, timestamp.rowtime as ts, temperatrue\"); //Table resTable = dataTable.select(\"id, temperatrue, pt\") // .where(\"id = 'sensor_1'\"); // 执行sql tableEnv.toAppendStream(dataTable, Row.class).print(\"事件时间\"); // tableEnv.toAppendStream(resTable, Row.class).print(\"res\"); // job name env.execute(\"my\"); } } 可以追加字段 也可以添加 以一个字段 package com.matt.apitest.tableapi; import com.matt.apitest.beans.SensorReading; import org.apache.flink.streaming.api.TimeCharacteristic; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.table.api.Table; import org.apache.flink.table.api.Tumble; import org.apache.flink.table.api.java.StreamTableEnvironment; import org.apache.flink.types.Row; public class TimeAndWidow5 { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 指定事件时间 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); DataStream inputStream = env.readTextFile(\"/Users/matt/workspace/java/bigdata/study-flink/src/main/resources/sensor.txt\"); DataStream dataStream = inputStream.map(line -> { String[] split = line.split(\",\"); return new SensorReading(split[0], new Long(split[1]), new Double(split[2])); }) // 指定延迟时间 .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor(Time.seconds(2)) { @Override public long extractTimestamp(SensorReading sensorReading) { // 指定事件时间字段 return sensorReading.getTimestamp() * 1000L; } }); // 3.创建表环境 StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); // 基于流创建一张表 //Table dataTable = tableEnv.fromDataStream(dataStream, \"id, temperatrue, timestamp, pt.proctime\"); // 和类字段保持一致 Table dataTable = tableEnv.fromDataStream(dataStream, \"id, timestamp.rowtime as ts, temperatrue\"); //Table resTable = dataTable.select(\"id, temperatrue, pt\") // .where(\"id = 'sensor_1'\"); // 注册table tableEnv.createTemporaryView(\"sensor\", dataTable); // 5 窗口操作 // 滚动窗口 Table resTable = dataTable.window(Tumble.over(\"10.seconds\").on(\"ts\").as(\"tw\")) .groupBy(\"id, tw\") .select(\"id, id.count, temperatrue.avg, tw.end\"); //Table resultTable = dataTable.window(Tumble.over(\"10.seconds\").on(\"ts\").as(\"tw\")) // .groupBy(\"id, tw\") // .select(\"id, id.count, temp.avg, tw.end\"); // tumble_start Table resSQLTable = tableEnv.sqlQuery(\"select id, count(id) as cnt, tumble_end(ts, interval '10' second) from sensor\" + \" group by id, tumble(ts, interval '10' second)\"); // 执行sql //tableEnv.toAppendStream(dataTable, Row.class).print(\"事件时间\"); // tableEnv.toAppendStream(resTable, Row.class).print(\"res\"); tableEnv.toAppendStream(resTable, Row.class).print(\"t1\"); tableEnv.toRetractStream(resSQLTable, Row.class).print(\"t2\"); // job name env.execute(\"my\"); } } t2:9> (true,sensor_6,1,2019-01-17 09:43:30.0) t2:1> (true,sensor_1,1,2019-01-17 09:43:20.0) t2:5> (true,sensor_10,1,2019-01-17 09:43:30.0) t1:5> sensor_10,1,38.1,2019-01-17 09:43:30.0 t1:1> sensor_1,1,990.8,2019-01-17 09:43:20.0 t1:9> sensor_6,1,200.4,2019-01-17 09:43:30.0 t1:9> sensor_6,1,999.4,2019-01-17 09:48:30.0 t2:9> (true,sensor_6,1,2019-01-17 09:48:30.0) t1:1> sensor_7,1,6.7,2019-01-17 09:43:30.0 t2:9> (true,sensor_6,1,2019-01-17 09:55:10.0) t2:1> (true,sensor_7,1,2019-01-17 09:43:30.0) t1:9> sensor_6,1,0.4,2019-01-17 09:55:10.0 t2:1> (true,sensor_1,1,2019-01-17 09:46:50.0) t1:1> sensor_1,1,100.8,2019-01-17 09:46:50.0 如果都是空则返回0 FIELD.sum0 topN 表聚合 返回多行多列数据 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-25 02:15:35 "},"31bigdata/hbase/hbase概述.html":{"url":"31bigdata/hbase/hbase概述.html","title":"概述","keywords":"","body":"1.hbase简介 1.1定义 HBase 是一种分布式、可扩展、支持海量数据存储的 NoSQL（非关系型） 数据库。 1.2数据模型 逻辑上，HBase 的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。 但从 HBase 的底层物理存储结构（K-V）来看，HBase 更像是一个 multi-dimensional map。 1.2.1逻辑结构 store:真正存储的东西 1.2.2物理结构 1.2.3数据模型 1.Name Space 命名空间，类似于关系型数据库的 DatabBase 概念，每个命名空间下有多个表。HBase 有两个自带的命名空间，分别是 hbase 和 default，hbase 中存放的是 HBase 内置的表， default 表是用户默认使用的命名空间。 2.Region 类似于关系型数据库的表概念。不同的是，HBase 定义表时只需要声明列族即可，不需 要声明具体的列。这意味着，往 HBase 写入数据时，字段可以动态、按需指定。因此，和关 系型数据库相比，HBase 能够轻松应对字段变更的场景。 3.Row HBase 表中的每行数据都由一个 RowKey 和多个 Column（列）组成，数据是按照 RowKey 的字典顺序存储的，并且查询数据时只能根据 RowKey 进行检索，所以 RowKey 的设计十分重 要。 4.Column HBase 中的每个列都由 Column Family(列族)和 Column Qualifier（列限定符）进行限 定，例如 info：name，info：age。建表时，只需指明列族，而列限定符无需预先定义。 5.Time Stamp 用于标识数据的不同版本（version），每条数据写入时，如果不指定时间戳，系统会 自动为其加上该字段，其值为写入 HBase 的时间。 6.Cell 由{rowkey, column Family：column Qualifier, time Stamp} 唯一确定的单元。cell 中的数据是没有类型的，全部是字节码形式存贮。 1.3基础架构 架构角色： 1.Region Server Region Server 为 Region 的管理者，其实现类为 HRegionServer，主要作用如下: 对于数据的操作：get, put, delete； 对于 Region 的操作：splitRegion、compactRegion。 2.Master Master 是所有 Region Server 的管理者，其实现类为 HMaster，主要作用如下： 对于表的操作：create, delete, alter 对于 RegionServer 的操作：分配 regions 到每个 RegionServer，监控每个 RegionServer的状态，负载均衡和故障转移。 3.Zookeeper HBase 通过 Zookeeper 来做 Master 的高可用、RegionServer 的监控、元数据的入口以及集群配置的维护等工作。 4.HDFS HDFS 为 HBase 提供最终的底层数据存储服务，同时为 HBase 提供高可用的支持。 2.安装 提前安装zk,hadoop。二者都为集群 2.1安装 1.解压 tar -zxvf hbase-1.3.1-bin.tar.gz -C /opt/module/ 2.2配置 1.进入配置目录 /opt/module/hbase-1.3.1/conf 2.记录 JAVA_HOME 路径 echo $JAVA_HOME /opt/module/jdk1.8.0_212 3.编写配置文件hbase-env.sh vim hbase-env.sh // The java implementation to use. Java 1.7+ required. export JAVA_HOME=/opt/module/jdk1.8.0_212 // hbase 有自带的zk,不要使用自带的 // Tell HBase whether it should manage it's own instance of Zookeeper or not. export HBASE_MANAGES_ZK=false 4.hbase-site.xml vim hbase-site.xml hbase-site.xml=core-site.xml hbase.rootdir=fs.defaultFS fs.defaultFS hdfs://matt05:8020 hbase.rootdir hdfs://matt05:8020/HBase hbase.cluster.distributed true hbase.master.port 16000 hbase.zookeeper.property.dataDir /opt/module/zookeeper-3.5.7/zkData hbase.zookeeper.quorum matt05,matt06,matt07 5.集群文件regionservers vim regionservers matt05 matt06 matt07 6建立hadoop快捷方式 ln -s /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml /opt/module/hbase-1.3.1/conf/core-site.xml ln -s /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml /opt/module/hbase-1.3.1/conf/hdfs-site.xml 7.分发安装 xsync hbase-1.3.1/ 2.3启动 记得开启同步时间 方式一 [matt@matt05 bin]$ ./hbase-daemon.sh start master ./hbase-daemon.sh start regionserver 方式二 bin/start-hbase.sh bin/stop-hbase.sh 2.4访问 web页面 ip:port 192.168.96.135:16010 3.使用 3.1基础 进入hbase shell ./hbase shell quit exit 退出； 帮助 help 查看有哪些表 list 命名相关 // 创建命名空间 create_namespace 'test1' create 'test1:student','info1' // 需要先删除该命名空间下的表 drop_namespace 'test1' list_namespace 表相关ddl hbase(main):023:0> create ERROR: wrong number of arguments (0 for 1) Here is some help for this command: Creates a table. Pass a table name, and a set of column family specifications (at least one), and, optionally, table configuration. Column specification can be a simple string (name), or a dictionary (dictionaries are described below in main help output), necessarily including NAME attribute. Examples: Create a table with namespace=ns1 and table qualifier=t1 hbase> create 'ns1:t1', {NAME => 'f1', VERSIONS => 5} 创建表 create 't2','info' 删除表 disable 'test' drop 'test' 修改列组版本 alter 't1', {NAME => 'info', VERSIONS => 1} 查看表的信息 describe 't1' 查看所有表 list dml crud put 'student','1001','info1:name','matt' scan 'student' // 表名 rowKey get 'student','1001' scan 'student',{COLUMNS => ['info1:name', 'info2:sex'], LIMIT => 10, STARTROW => '1001',STOPROW => '1003'} scan 'student', {RAW => true, VERSIONS => 3} hbase(main):010:0> scan 't2', {RAW => true, VERSIONS => 3} ROW COLUMN+CELL 1001 column=info:, timestamp=1630203659417, type=DeleteColumn 1001 column=info:name, timestamp=1630203905379, type=DeleteColumn 1001 column=info:name, timestamp=1630203448221, value=b 1001 column=info:name, timestamp=1630203437357, value=a 1 row(s) in 0.0240 seconds // update put 'student','1001','info1:name','matt1' // 指定到列组则不会删除 delete 'student','1001','info1:sex' deleteall 'student','1002' truncate 'student' Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-02-13 23:20:17 "},"33mq/0kafka/0概述.html":{"url":"33mq/0kafka/0概述.html","title":"概述","keywords":"","body":"概述 概念 Kafka 是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于 大数据实时处理领域 mq模式 1.发布/订阅（1:1 1:n） 拉取 缺点可能需要长轮训 2.队列主动推 造成消费者阻塞 kafka：拉取 长轮训 设置超时时间 好处 解耦 削峰 架构 1）Producer ：消息生产者，就是向kafka broker 发消息的客户端； 2）Consumer ：消息消费者，向kafka broker 取消息的客户端； 3）Consumer Group （CG）：消费者组，消费者组可以有一个或多个消费者，一个topic的一个分区只能由消费者组内一个消费者消费 4）Broker ：一台kafka 服务器就是一个broker。一个集群由多个broker 组成。 5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic； 6）Partition：一个topic可由多个分区组成 7）Replica：副本，leader follower 8) leader Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-29 01:29:37 "},"33mq/0kafka/1安装.html":{"url":"33mq/0kafka/1安装.html","title":"安装","keywords":"","body":"​ 地址 http://kafka.apache.org/downloads.html 规划 matt05 zk kafka matt06 zk kafka matt07 zk kafka 解压 tar -zxvf kafka_2.11-0.11.0.0.tgz -C /opt/module/ 重命名 mv kafka_2.11-0.11.0.0/ kafka 2.scanla版本 0.11kafka版本 在安装目录下创建data文件夹 默认日志放在logs data:存储数据 mkdir data 修改配置文件 cd config/ vi server.properties # 改 broker 的全局唯一编号，不能重复 broker.id=0 # 改 删除 topic 功能使能 delete.topic.enable=true #处理网络请求的线程数量 num.network.threads=3 #用来处理磁盘 IO 的现成数量 num.io.threads=8 #发送套接字的缓冲区大小 socket.send.buffer.bytes=102400 #接收套接字的缓冲区大小 socket.receive.buffer.bytes=102400 # 请求套接字的缓冲区大小 socket.request.max.bytes=104857600 # 改 kafka 运行日志存放的路径 日志目录 log.dirs=/opt/module/kafka/data #topic 在当前 broker 上的分区个数 num.partitions=1 #用来恢复和清理data 下数据的线程数量 num.recovery.threads.per.data.dir=1 #segment 文件保留的最长时间，超时将被删除 log.retention.hours=168 # 改 配置连接 Zookeeper 集群地址 zookeeper.connect=zookeeper.connect=192.168.96.135:2181,192.168.96.136:2181,192.168.96.137:2181 分发安装包 xsync ./kafka/ 记得修改机器的broker.id 因为他是惟一的 启动,安装目录bin目录下， 分别进入三台机器 ./kafka-server-start.sh -daemon ../config/server.properties 关闭,安装目录bin目录下 ./kafka-server-stop.sh stop 使用时可能会无法连接kafka，在server.properties进行如下配置 advertised.listeners=PLAINTEXT://192.168.96.128:9092 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-29 01:29:42 "},"33mq/0kafka/2终端操作.html":{"url":"33mq/0kafka/2终端操作.html","title":"终端操作.md","keywords":"","body":"终端操作kafka 如果启动失败 查看logs/server.log 查看所有topic ./kafka-topics.sh --zookeeper matt05:2181 --list 创建topic ./kafka-topics.sh --zookeeper matt05:2181 --create --replication-factor 2 --partitions 2 --topic first 选项 说明： --topic 定义 topic名 --replication-factor 定义副本数 --partitions 定义分区数 副本数量 不能超过机器数 分区可以大于机器数量 删除topic ./kafka-topics.sh --zookeeper matt05:2181 --delete --topic first 需要server.properties中设置 delete.topic.enable=true否则只是标记删除。 查看topic详情 ./kafka-topics.sh --zookeeper matt05:2181 --describe --topic first 发送消息 ./kafka-console-producer.sh --broker-list matt05:9092 --topic first 消费消息 可以连接zookeeper ./kafka-console-consumer.sh --zookeeper matt05:2181 --topic first 会有警告 ./kafka-console-consumer.sh --bootstrap-server matt05:9092 --topic first 从最开始的位置消费 ./kafka-console-consumer.sh --bootstrap-server matt05:9092 --from-beginning --topic first 修改分区数 ./kafka-topics.sh --zookeeper matt05:2181 --alter --topic first --partitions 3 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-29 01:29:47 "},"33mq/0kafka/3原理.html":{"url":"33mq/0kafka/3原理.html","title":"原理.md","keywords":"","body":"整体架构 kafka工作流程 kafka中消息按照topic进行分类 topic是逻辑上的，partition是物理的 每个partition对应一个log文件，该log文件存储producer生产的数据，producer生产的数据会追加到该文件的末端，且每条数据都有自己offset，记录消费者消费数据的位置 由于生产者生产的消息会不断追加到 log 文件末尾，为防止 log 文件过大导致数据定位 效率低下，Kafka 采取了分片和索引机制，将每个 partition 分为多个 segment。每个 segment 对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名 规则为：topic 名称+分区序号。例如，first 这个 topic 有三个分区，则其对应的文件夹为 first-0,first-1,first-2。 index 和 log 文件以当前 segment 的第一条消息的 offset 命名。下图为 index 文件和 log 文件的结构示意图。 “.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元 数据指向对应数据文件中 message 的物理偏移地址。 默认会存7天 168h 生产者 分区策略 分区原因 方便在集群扩展：可以修改分区数量以适应它所在的机器 高并发：因为可以以 Partition 为单位读写了。 如何分区 （1）指明 partition 的情况下，直接将指明的值直接作为 partiton 值； （2）没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值； （3）既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后 面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。 数据可靠性 为保证 producer 发送的数据，能可靠的发送到指定的 topic，topic 的每个 partition 收到 producer 发送的数据后，都需要向 producer 发送 ack（acknowledgement 确认收到），如果 producer 收到 ack，就会进行下一轮的发送，否则重新发送数据。 副本同步策略 方案 优点 缺点 半数以上完成同步，就发 送 ack 延迟低 选举新的 leader 时，容忍 n 台 节点的故障，需要 2n+1 个副 本 全部完成同步，才发送 ack 选举新的 leader 时，容忍 n 台 节点的故障，需要 n+1 个副 本 延迟高 Kafka 选择了第二种方案，原因如下： 1.同样为了容忍 n 台节点的故障，第一种方案需要 2n+1 个副本，而第二种方案只需要 n+1 个副本， 而 Kafka 的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。 2.虽然第二种方案的网络延迟会比较高，但网络延迟对 Kafka 的影响较小 副本数 3 1 个 leader 2 个 follower ISR 问题：如果采用方案二同步，因为某种故障，leader不能和follower同步，那么leader就会一直不能给生产者发送ack Leader 维护了一个动态的 in-sync replica set (ISR)，意为和 leader 保持同步的 follower 集 合。当 ISR 中的 follower 完成数据的同步之后，leader 就会给 follower 发送 ack。如果 follower 长时间 未 向 leader 同 步 数 据 ， 则 该 follower 将 被 踢 出 ISR ， 该 时 间 阈 值 由 replica.lag.time.max.ms 参数设定。Leader 发生故障之后，就会从 ISR 中选举新的 leader。 ack 应答机制 acks： 0：producer 不等待 broker 的 ack，这一操作提供了一个最低的延迟，broker 一接收到还 没有写入磁盘就已经返回，当 broker 故障时有可能丢失数据； 1：producer 等待 broker 的 ack，partition 的 leader 落盘成功后返回 ack，如果在 follower 同步成功之前 leader 故障，那么将会丢失数据； -1（all）：producer 等待 broker 的 ack，partition 的 leader 和 follower 全部落盘成功后才 返回 ack。但是如果在 follower 同步完成后，broker 发送 ack 之前，leader 发生故障，那么会 造成数据重复。 故障处理 1.follower故障 follower 发生故障后会被临时踢出 ISR，待该 follower 恢复后，follower 会读取本地磁盘 记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。 等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重 新加入 ISR 了。 2.leader故障 leader 发生故障之后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的数据一致性，其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader 同步数据。 注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。 Exactly Once 将服务器的 ACK 级别设置为-1，可以保证 Producer 到 Server 之间不会丢失数据，即 At Least Once 语义。相对的，将服务器 ACK 级别设置为 0，可以保证生产者每条消息只会被 发送一次，即 At Most Once 语义。 At Least Once:不丢失，可能会重复 At Most Once：不重复，可能会丢失 At Least Once + 幂等性 = Exactly Once 要启用幂等性，只需要将 Producer 的参数中 enable.idompotence 设置为 true 即可。Kafka 的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的 Producer 在 初始化的时候会被分配一个 PID，发往同一 Partition 的消息会附带 Sequence Number。而 Broker 端会对做缓存，当具有相同主键的消息提交时，Broker 只 会持久化一条。 但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨 分区跨会话的 Exactly Once。 消费者 消费方式 consumer 采用 pull（拉）模式从 broker 中读取数据。 push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。消费者会阻塞 pull 模式不足之处是，如果 kafka 没有数据，消费者会一直轮询。针对这一点，Kafka 的消费者在消费数据时会传入一个时长参数 timeout，如果当前没有 数据可供消费，consumer 会等待一段时间之后再返回，这段时长即为 timeout。 分区分配策略 参考 分区分配 一个topic会有多个分区。一个消费者组会有多个消费者 RoundRobin 1.消费者按照字典序排序 c0 c1 c2... 2.topic按照字典序排序，并得到每个topic所有的分区，从而得到分区集合 3.遍历分区集合，同时轮询消费者 4.如果轮询的消费者没有订阅当前分区所在的topic则跳过当前消费者,g给当前分区给下一个消费者否则分配给当前消费者 同时遍历分区 3个Topic：T0（3个分区0, 1, 2）, T1（两个分区0, 1）, T2（4个分区0, 1, 2, 3）； 3个consumer: C0订阅了[T0, T1]， C1订阅了[T1, T2]， C2订阅了[T2, T0]； roundrobin结果分配结果如下： T0-P0分配给C0，T0-P1分配给C2，T0-P2分配给C0， T1-P0分配给C1，T1-P1分配给C0， T2-P0分配给C1，T2-P1分配给C2，T2-P2分配给C1，T0-P3分配给C2； range 1.获取topic下的所有分区 p0 p1 p2 2.消费者按照字典序排序 c0 c1 c2 3.分区数除消费者数，得到n 4.分区数对消费者数取余，得到m 5.消费者集合中，前m个消费者有n+1分区，剩余的消费者分配n个分区 注意 当消费者组内消费者发生变化时 分区重新分配 offset 维护 记录消费者消费到哪条消息 Kafka 0.9 版本之前，consumer 默认将 offset 保存在 Zookeeper 中，从 0.9 版本开始， consumer 默认将 offset 保存在 Kafka 一个内置的 topic 中，该 topic 为__consumer_offsets。 offset 消费者组 + 主题 + 分区 consumer.properties exclude.internal.topics=false 1.zk ./kafka-console-consumer.sh --topic __consumer_offsets --zookeeper matt05:2181 --formatter \"kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter\" --consumer.config ../config/consumer.properties --from-beginning 2、mq [console-consumer-39623,bigdata,0]::[OffsetMetadata[4,NO_METADATA],CommitTime 1637488435475,ExpirationTime 1637574835475] [console-consumer-39623,bigdata,1]::[OffsetMetadata[4,NO_METADATA],CommitTime 1637488435475,ExpirationTime 1637574835475] 验证一个分区只可以被一个消费者消费 matt05 matt06 vi consumer.properties matt05 matt06 group.id=matt 5 6 启动消费者 ./kafka-console-consumer.sh --zookeeper matt05:2181 --topic first --consumer.config ../config/consumer.properties 7生产者 ./kafka-console-producer.sh --broker-list matt05:9092 --topic first kafka 1）顺序写磁盘 Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端， 为顺序写。官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这 与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。 2）零拷贝 zk Kafka 集群中有一个 broker 会被选举为 Controller，负责管理集群 broker 的上下线，所 有 topic 的分区副本分配和 leader 选举等工作。 Controller 的管理工作都是依赖于 Zookeeper 的 zk:监听节点 事务 Kafka 从 0.11 版本开始引入了事务支持。事务可以保证 Kafka 在 Exactly Once 语义的基 础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。 producer Transaction ID，并将 Producer 获得的PID 和Transaction ID 绑定。并写入内部topic。 TransactionL：用户给定的 consumer 消费者 事务较弱 用户可以修改offset Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-29 01:32:03 "},"33mq/0kafka/4api.html":{"url":"33mq/0kafka/4api.html","title":"api","keywords":"","body":"Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-29 01:29:56 "},"34mw/zookeeper/zookeeper学习.html":{"url":"34mw/zookeeper/zookeeper学习.html","title":"zookeeper笔记","keywords":"","body":"1.概述 1.1概述 Zookeeper 是一个开源的分布式的，为分布式框架提供协调服务的 Apache 项目。 Zookeeper从设计模式角度来理解:是一个基于观察者模式设计的分布式服务管理框架，它负责 存储和管理大家都关心的数据，然后接受观察者的 注册，一旦这些数据的状态发生变化，Zookeeper 就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应。 存储少量数据，一般是配置信息。 1.2特点 1.Zookeeper:一个领导者(Leader)，多个跟随者(Follower)组成的集群。 2.集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。所以Zookeeper适合安装奇数台服务器。 3.全局数据一致:每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的。 4.更新请求顺序执行，来自同一个Client的更新请求按其发送顺序依次执行。 5.数据更新原子性，一次数据更新要么成功，要么失败。 6.实时性，在一定时间范围内，Client能读到最新数据。 1.3数据结构 ZooKeeper 数据模型的结构与 Unix 文件系统很类似，整体上可以看作是一棵树，每个 节点称做一个 ZNode。每一个 ZNode 默认能够存储 1MB 的数据，每个 ZNode 都可以通过 其路径唯一标识。 1.4应用场景 提供的服务包括:统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下 线、软负载均衡等。 统一命名服务 在分布式环境下，经常需要对应用/服 务进行统一命名，便于识别。 例如:IP不容易记住，而域名容易记住。 统一配置管理 1.分布式环境下，配置文件同步非常常见。 1.1一般要求一个集群中，所有节点的配置信息是一致的，比如 Kafka 集群。 1.2对配置文件修改后，希望能够快速同步到各个节点上。 2配置管理可交由ZooKeeper实现。 2.1可将配置信息写入ZooKeeper上的一个Znode。 2.2各个客户端服务器监听这个Znode。 2.3一旦Znode中的数据被修改，ZooKeeper将通知 各个客户端服务器。 统一集群管理 1分布式环境中，实时掌握每个节点的状态是必要的。 1.1可根据节点实时状态做出一些调整。 2.ZooKeeper可以实现实时监控节点状态变化 2.1可将节点信息写入ZooKeeper上的一个ZNode。 2.2监听这个ZNode可获取它的实时状态变化。 服务器动态上下线 软负载均衡 1.5下载地址 3.5.7 https://archive.apache.org/dist/zookeeper/zookeeper-3.5.7/ 2.安装 2.1前置条件 需要安装jdk,参考linux 目录下的常用软件安装即可看到jdk的安装。 2.2本地安装 安装 1.将tar包发送到服务器 scp -r apache-zookeeper-3.5.7-bin.tar.gz root@123.56.135.43:/opt/software 2.解压 tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/ 3.改名 mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7/ 配置修改 /tmp存储临时数据，一定周期就会删除 1.在 zookeeper 安装目录下创建数据目录 mkdir zkData pwd 2.配置文件目录下的zoo_sample.cfg改为zoo.cfg cd conf mv zoo_sample.cfg zoo.cfg 3.修改配置文件 vim zoo.cfg dataDir=/opt/module/zookeeper-3.5.7/zkData 2.3集群安装 安装 1.将tar包发送到服务器 scp -r apache-zookeeper-3.5.7-bin.tar.gz root@123.56.135.43:/opt/software 2.解压 tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/ 3.改名 mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7/ 配置 1.在zookeeper安装目录下创建zkData目录 mkdir zkData 2.在zkData目录下创建一个 myid 的文件 touch myid 3.编写myid文件，文件中写该服务器唯一标识符 1 4.将该zookeeper安装目录分发到其他服务器 xsync zookeeper-3.5.7 5.更改其他服务中zkData中的myid文件中的唯一标识 6.配置文件目录下的zoo_sample.cfg改为zoo.cfg cd conf mv zoo_sample.cfg zoo.cfg 7.修改配置文件 vim zoo.cfg 8.修改数据存储路径配置 dataDir=/opt/module/zookeeper-3.5.7/zkData 9.添加集群信息 #######################cluster########################## server.2=matt05:2888:3888 server.3=matt06:2888:3888 server.4=matt07:2888:3888 配置文件解读 server.A=B:C:D A 是一个数字，表示这个是第几号服务器;集群模式下配置一个文件 myid，这个文件在 dataDir 目录下，这个文件里面有一个数据 就是 A 的值，Zookeeper 启动时读取此文件，拿到里面的数据zoo.cfg 里面的配置信息比 较从而判断到底是哪个 server。 B 是这个服务器的地址; C 是这个服务器 Follower 与集群中的 Leader 服务器交换信息的端口; D 是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而个端口就是用来执行选举时服务器相互通信的端口。 10.同步配置文件，发送到其他服务器 xsync zoo.cfg 启动集群脚本 cd /home/matt/bin vim zk.sh #!/bin/bash case $1 in \"start\"){ for i in matt05 matt06 matt07 do echo ---------- zookeeper $i 启动 ------------ ssh $i \"/opt/module/zookeeper-3.5.7/bin/zkServer.sh start\" done };; \"stop\"){ for i in matt05 matt06 matt07 do echo ---------- zookeeper $i 停止 ------------ ssh $i \"/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop\" done };; \"status\"){ for i in matt05 matt06 matt07 do echo ---------- zookeeper $i 状态 ------------ ssh $i \"/opt/module/zookeeper-3.5.7/bin/zkServer.sh status\" done };; esac chmod u+x zk.sh zk.sh start zk.sh stop 写数据：半数以上同意就可以给客户端发送ack。 3.客户端操作 启动 服务端启动 ./zkServer.sh start 查看服务端状态 ./zkServer.sh status 服务端关闭 ./zkServer.sh stop 客户端启动 // 不指定服务端 ./zkCli.sh // 指定服务端 ./zkCli.sh -server 192.168.96.132:2181 全格式查看jps jps -l 查看所有命令 help 创建节点 create /matt \"matt\" -e:短暂，客户端关闭再次开启该节点就会删除 -s:带序号 create -e -s /matt \"1\" create -s /matt \"2\" 删除节点 delete /matt // 递归删除节点 deleteall /matt 修改节点 set /matt \"matt\" 查看子节点 // 查看当前节点包含哪些子节点 ls /matt // 更加详细 查看当前节点包含哪些子节点 ls -s /matt 查看当前节点的值 get /matt // 详细 get -s /matt // 查看节点的状态，就是不显示当前的值 stat /matt 1.czxid:创建节点的事务 zxid 每次修改ZooKeeper状态都会产生一个ZooKeeper事务ID。事务ID是ZooKeeper中所 有修改总的次序。每次修改都有唯一的 zxid，如果 zxid1 小于 zxid2，那么 zxid1 在 zxid2 之 前发生。 2.ctime:znode 被创建的毫秒数(从 1970 年开始) 3.mzxid:znode 最后更新的事务 zxid 4.mtime:znode 最后修改的毫秒数(从 1970 年开始) 5.pZxid:znode 最后更新的子节点 zxid 6.cversion:znode 子节点变化号，znode 子节点修改次数 7.dataversion:znode 数据变化号 8.aclVersion:znode 访问控制列表的变化号 9.ephemeralOwner:如果是临时节点，这个是 znode 拥有者的 session id。如果不是 临时节点则是 0。 10.dataLength:znode 的数据长度 11.numChildren:znode 子节点数量 监听 监听器：使用一次就会失效 监听节点的值发生变化 get -w /matt 监听节点的子节点发生变化 ls -w /matt 错误 org.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Address 2021-08-14 11:44:23,004 [myid:] - ERROR [main:QuorumPeerMain@89] - Invalid config, exiting abnormally org.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Address unresolved: 192.168.96.135:3887 在集群配置有空格，删除即可 待做 软件包管理 2.2本地安装配置文件解读 3.1.2选举机制 监听器原理 Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-02-13 23:20:17 "},"temp.html":{"url":"temp.html","title":"temp","keywords":"","body":"临时记录 插件 https://jiangminggithub.github.io/gitbook/chapter-plugins/8-code.html this is good Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-04-05 11:59:26 "},"留言墙.html":{"url":"留言墙.html","title":"留言墙","keywords":"","body":"留言墙 var gitalk = new Gitalk({ \"clientID\": \"8ab96766a9f6ec805ff4\", \"clientSecret\": \"df4b41cc098c4e9e2949fb5b72f98cefadb4f29f\", \"repo\": \"iweiwan.github.io\", \"owner\": \"iweiwan\", \"admin\": [\"iweiwan\"], \"id\": decodeURI(location.pathname), \"distractionFreeMode\": false }); gitalk.render(\"gitalk-container\"); Copyright © iweiwan.github.io 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-03-30 08:31:04 "}}